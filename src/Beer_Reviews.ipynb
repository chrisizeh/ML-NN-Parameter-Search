{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08eed9e0-4130-4a9d-bfa1-3c0b489fb8a8",
   "metadata": {},
   "source": [
    "# Preprocessing + NN Playing for Beer Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23a3508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: liac-arff in /Users/chrisizeh/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages (2.5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install liac-arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be2e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/chrisizeh/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /Users/chrisizeh/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/chrisizeh/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/chrisizeh/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/chrisizeh/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/chrisizeh/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/chrisizeh/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/chrisizeh/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/chrisizeh/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c931a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import arff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8c879",
   "metadata": {},
   "source": [
    "## Load Dataset and create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "168151d4-3d0d-4ffd-a0bd-95d65371ada6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['brewery_id', 'brewery_name', 'review_time', 'review_overall',\n",
       "       'review_aroma', 'review_appearance', 'review_profilename', 'beer_style',\n",
       "       'review_palate', 'review_taste', 'beer_name', 'beer_abv',\n",
       "       'beer_beerid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = arff.load(open('../data/beer_reviews.arff', 'r'))\n",
    "attr = np.array(data['attributes'])\n",
    "numericals = [i[0] for i in attr if i[1] == 'INTEGER' or i[1] == 'REAL']\n",
    "df = pd.DataFrame(data['data'], columns=attr[:, 0])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef52e7b2",
   "metadata": {},
   "source": [
    "Use only 10% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54b405bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, df = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91c54ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158662"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "367a7f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brewery_id</th>\n",
       "      <th>brewery_name</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review_overall</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_profilename</th>\n",
       "      <th>beer_style</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>beer_name</th>\n",
       "      <th>beer_abv</th>\n",
       "      <th>beer_beerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>859964</th>\n",
       "      <td>13307</td>\n",
       "      <td>Mikkeller ApS</td>\n",
       "      <td>2011-10-03 01:48:53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Ghenna</td>\n",
       "      <td>American Barleywine</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Big Worst</td>\n",
       "      <td>18.5</td>\n",
       "      <td>56831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581443</th>\n",
       "      <td>126</td>\n",
       "      <td>Pete's Brewing Company</td>\n",
       "      <td>2007-11-17 06:34:54</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>hwwty4</td>\n",
       "      <td>Cream Ale</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Pete's Wicked Wanderlust Cream Ale</td>\n",
       "      <td>5.6</td>\n",
       "      <td>19950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270456</th>\n",
       "      <td>203</td>\n",
       "      <td>Greene King / Morland Brewery</td>\n",
       "      <td>2008-10-15 22:32:23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>dsa7783</td>\n",
       "      <td>English Pale Ale</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Abbot Ale</td>\n",
       "      <td>5.0</td>\n",
       "      <td>910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477415</th>\n",
       "      <td>167</td>\n",
       "      <td>Brewery Lobkowicz</td>\n",
       "      <td>2003-12-03 22:07:06</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>TastyTaste</td>\n",
       "      <td>Czech Pilsener</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Lobkowicz Knight</td>\n",
       "      <td>4.8</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408999</th>\n",
       "      <td>664</td>\n",
       "      <td>Wells &amp; Young's Ltd</td>\n",
       "      <td>2007-02-23 05:32:26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>BadRockBeer</td>\n",
       "      <td>Milk / Sweet Stout</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Young's Double Chocolate Stout</td>\n",
       "      <td>5.2</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         brewery_id                   brewery_name         review_time  \\\n",
       "859964        13307                  Mikkeller ApS 2011-10-03 01:48:53   \n",
       "581443          126         Pete's Brewing Company 2007-11-17 06:34:54   \n",
       "1270456         203  Greene King / Morland Brewery 2008-10-15 22:32:23   \n",
       "477415          167              Brewery Lobkowicz 2003-12-03 22:07:06   \n",
       "1408999         664            Wells & Young's Ltd 2007-02-23 05:32:26   \n",
       "\n",
       "         review_overall  review_aroma  review_appearance review_profilename  \\\n",
       "859964              4.0           4.5                4.5             Ghenna   \n",
       "581443              2.5           3.0                3.5             hwwty4   \n",
       "1270456             4.0           3.5                4.0            dsa7783   \n",
       "477415              4.5           4.0                3.5         TastyTaste   \n",
       "1408999             5.0           4.0                5.0        BadRockBeer   \n",
       "\n",
       "                  beer_style  review_palate  review_taste  \\\n",
       "859964   American Barleywine            3.5           3.5   \n",
       "581443             Cream Ale            3.0           2.5   \n",
       "1270456     English Pale Ale            3.5           3.5   \n",
       "477415        Czech Pilsener            4.0           4.5   \n",
       "1408999   Milk / Sweet Stout            5.0           5.0   \n",
       "\n",
       "                                  beer_name  beer_abv  beer_beerid  \n",
       "859964                            Big Worst      18.5        56831  \n",
       "581443   Pete's Wicked Wanderlust Cream Ale       5.6        19950  \n",
       "1270456                           Abbot Ale       5.0          910  \n",
       "477415                     Lobkowicz Knight       4.8          444  \n",
       "1408999      Young's Double Chocolate Stout       5.2           73  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.columns\n",
    "\n",
    "df['review_time'] = df['review_time'].apply(lambda sec: pd.Timestamp(sec, unit='s'))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0077eea",
   "metadata": {},
   "source": [
    "## Fix Missing Values\n",
    "\n",
    "The rows with missing brewery name for id 1193 are found through a quick google search and added. For the ones with brewery id 27 where the beers already exist with the correct brewery, so I add it based on the dataset. For the others I google with the provided data.\n",
    "\n",
    "The missing review profilenames are set to anonynoums, but otherwise kept, because the review is still done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d37a856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brewery_id</th>\n",
       "      <th>brewery_name</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review_overall</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_profilename</th>\n",
       "      <th>beer_style</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>beer_name</th>\n",
       "      <th>beer_abv</th>\n",
       "      <th>beer_beerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>990101</th>\n",
       "      <td>4338</td>\n",
       "      <td>C.B. &amp; Potts Restaurant &amp; Brewery</td>\n",
       "      <td>2008-03-21 22:47:02</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>rowew</td>\n",
       "      <td>American Amber / Red Ale</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Big Horn Buttface Amber Ale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274155</th>\n",
       "      <td>1874</td>\n",
       "      <td>Pizzeria Uno Chicago Grill &amp; Brewery</td>\n",
       "      <td>2003-09-02 18:40:29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Dogbrick</td>\n",
       "      <td>Hefeweizen</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Hefeweizen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545596</th>\n",
       "      <td>2854</td>\n",
       "      <td>Flensburger Brauerei GmbH Und Co. KG</td>\n",
       "      <td>2007-02-04 03:28:46</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Cadyfatcat</td>\n",
       "      <td>Hefeweizen</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Flensburger Weizen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723440</th>\n",
       "      <td>341</td>\n",
       "      <td>Columbus Brewing Company</td>\n",
       "      <td>2006-05-12 22:32:21</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Ahhdball</td>\n",
       "      <td>American Pale Ale (APA)</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Columbus Pale Ale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277262</th>\n",
       "      <td>1916</td>\n",
       "      <td>Kettle House Brewing Co.</td>\n",
       "      <td>2002-08-20 17:32:23</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>beernut7</td>\n",
       "      <td>American Pale Ale (APA)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Ginseng Pale Ale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810185</th>\n",
       "      <td>684</td>\n",
       "      <td>Fish Brewing Company / Fishbowl Brewpub</td>\n",
       "      <td>2009-11-18 05:02:55</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>RedDiamond</td>\n",
       "      <td>German Pilsener</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Reister Pilsner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495915</th>\n",
       "      <td>1111</td>\n",
       "      <td>Martha's Vineyard Ales</td>\n",
       "      <td>2003-01-13 20:47:10</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>TheLongBeachBum</td>\n",
       "      <td>Foreign / Export Stout</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Martha's Vineyard Extra Stout</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429549</th>\n",
       "      <td>5408</td>\n",
       "      <td>Herkimer Pub &amp; Brewery</td>\n",
       "      <td>2008-11-24 22:56:31</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>tempest</td>\n",
       "      <td>Altbier</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Alt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188478</th>\n",
       "      <td>7776</td>\n",
       "      <td>Brauerei Westheim GmbH</td>\n",
       "      <td>2005-05-24 01:48:32</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>allengarvin</td>\n",
       "      <td>Märzen / Oktoberfest</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Westheimer Oktoberfest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342280</th>\n",
       "      <td>303</td>\n",
       "      <td>Bud&amp;#283;jovický M&amp;#283;&amp;#357;anský Pivovar</td>\n",
       "      <td>2005-03-26 02:11:11</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>RoyalT</td>\n",
       "      <td>Euro Dark Lager</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Crystal Diplomat Dark Beer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6896 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         brewery_id                                  brewery_name  \\\n",
       "990101         4338             C.B. & Potts Restaurant & Brewery   \n",
       "1274155        1874          Pizzeria Uno Chicago Grill & Brewery   \n",
       "545596         2854          Flensburger Brauerei GmbH Und Co. KG   \n",
       "723440          341                      Columbus Brewing Company   \n",
       "277262         1916                      Kettle House Brewing Co.   \n",
       "...             ...                                           ...   \n",
       "810185          684       Fish Brewing Company / Fishbowl Brewpub   \n",
       "1495915        1111                        Martha's Vineyard Ales   \n",
       "1429549        5408                        Herkimer Pub & Brewery   \n",
       "188478         7776                        Brauerei Westheim GmbH   \n",
       "1342280         303  Bud&#283;jovický M&#283;&#357;anský Pivovar   \n",
       "\n",
       "                review_time  review_overall  review_aroma  review_appearance  \\\n",
       "990101  2008-03-21 22:47:02             2.5           3.0                3.0   \n",
       "1274155 2003-09-02 18:40:29             3.5           3.0                3.0   \n",
       "545596  2007-02-04 03:28:46             4.0           4.5                4.0   \n",
       "723440  2006-05-12 22:32:21             4.0           3.5                4.0   \n",
       "277262  2002-08-20 17:32:23             4.5           4.0                4.0   \n",
       "...                     ...             ...           ...                ...   \n",
       "810185  2009-11-18 05:02:55             3.0           3.0                3.5   \n",
       "1495915 2003-01-13 20:47:10             4.5           4.5                4.5   \n",
       "1429549 2008-11-24 22:56:31             3.5           3.0                4.0   \n",
       "188478  2005-05-24 01:48:32             3.0           3.0                3.5   \n",
       "1342280 2005-03-26 02:11:11             3.5           4.0                4.0   \n",
       "\n",
       "        review_profilename                beer_style  review_palate  \\\n",
       "990101               rowew  American Amber / Red Ale            3.0   \n",
       "1274155           Dogbrick                Hefeweizen            3.0   \n",
       "545596          Cadyfatcat                Hefeweizen            3.5   \n",
       "723440            Ahhdball   American Pale Ale (APA)            3.5   \n",
       "277262            beernut7   American Pale Ale (APA)            4.0   \n",
       "...                    ...                       ...            ...   \n",
       "810185          RedDiamond           German Pilsener            2.5   \n",
       "1495915    TheLongBeachBum    Foreign / Export Stout            5.0   \n",
       "1429549            tempest                   Altbier            3.5   \n",
       "188478         allengarvin      Märzen / Oktoberfest            3.5   \n",
       "1342280             RoyalT           Euro Dark Lager            3.5   \n",
       "\n",
       "         review_taste                      beer_name  beer_abv  beer_beerid  \n",
       "990101            3.0    Big Horn Buttface Amber Ale       NaN        25912  \n",
       "1274155           2.5                     Hefeweizen       NaN        12429  \n",
       "545596            3.5             Flensburger Weizen       NaN         6712  \n",
       "723440            3.5              Columbus Pale Ale       NaN         3700  \n",
       "277262            4.5               Ginseng Pale Ale       NaN         5589  \n",
       "...               ...                            ...       ...          ...  \n",
       "810185            3.0                Reister Pilsner       NaN        54202  \n",
       "1495915           5.0  Martha's Vineyard Extra Stout       NaN         3266  \n",
       "1429549           3.5                            Alt       NaN        21231  \n",
       "188478            3.5         Westheimer Oktoberfest       NaN        20547  \n",
       "1342280           3.5     Crystal Diplomat Dark Beer       NaN         3122  \n",
       "\n",
       "[6896 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brewery_id</th>\n",
       "      <th>brewery_name</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review_overall</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_profilename</th>\n",
       "      <th>beer_style</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>beer_name</th>\n",
       "      <th>beer_abv</th>\n",
       "      <th>beer_beerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>659305</th>\n",
       "      <td>1193</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-11-09 19:12:42</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>dqrull</td>\n",
       "      <td>Keller Bier / Zwickel Bier</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Engel Keller Dunkel  WRONG BREWERY SEE CRAILSH...</td>\n",
       "      <td>5.3</td>\n",
       "      <td>63324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391053</th>\n",
       "      <td>27</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-11-11 07:10:08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Docer</td>\n",
       "      <td>American Stout</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Caboose Oatmeal Stout</td>\n",
       "      <td>7.0</td>\n",
       "      <td>75137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         brewery_id brewery_name         review_time  review_overall  \\\n",
       "659305         1193         None 2010-11-09 19:12:42             3.5   \n",
       "1391053          27         None 2011-11-11 07:10:08             5.0   \n",
       "\n",
       "         review_aroma  review_appearance review_profilename  \\\n",
       "659305            3.5                4.0             dqrull   \n",
       "1391053           4.5                4.0              Docer   \n",
       "\n",
       "                         beer_style  review_palate  review_taste  \\\n",
       "659305   Keller Bier / Zwickel Bier            4.0           4.0   \n",
       "1391053              American Stout            4.5           4.5   \n",
       "\n",
       "                                                 beer_name  beer_abv  \\\n",
       "659305   Engel Keller Dunkel  WRONG BREWERY SEE CRAILSH...       5.3   \n",
       "1391053                              Caboose Oatmeal Stout       7.0   \n",
       "\n",
       "         beer_beerid  \n",
       "659305         63324  \n",
       "1391053        75137  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brewery_id</th>\n",
       "      <th>brewery_name</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review_overall</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_profilename</th>\n",
       "      <th>beer_style</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>beer_name</th>\n",
       "      <th>beer_abv</th>\n",
       "      <th>beer_beerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>659305</th>\n",
       "      <td>1193</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-11-09 19:12:42</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>dqrull</td>\n",
       "      <td>Keller Bier / Zwickel Bier</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Engel Keller Dunkel  WRONG BREWERY SEE CRAILSH...</td>\n",
       "      <td>5.3</td>\n",
       "      <td>63324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        brewery_id brewery_name         review_time  review_overall  \\\n",
       "659305        1193         None 2010-11-09 19:12:42             3.5   \n",
       "\n",
       "        review_aroma  review_appearance review_profilename  \\\n",
       "659305           3.5                4.0             dqrull   \n",
       "\n",
       "                        beer_style  review_palate  review_taste  \\\n",
       "659305  Keller Bier / Zwickel Bier            4.0           4.0   \n",
       "\n",
       "                                                beer_name  beer_abv  \\\n",
       "659305  Engel Keller Dunkel  WRONG BREWERY SEE CRAILSH...       5.3   \n",
       "\n",
       "        beer_beerid  \n",
       "659305        63324  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brewery_id</th>\n",
       "      <th>brewery_name</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review_overall</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_profilename</th>\n",
       "      <th>beer_style</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>beer_name</th>\n",
       "      <th>beer_abv</th>\n",
       "      <th>beer_beerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>659305</th>\n",
       "      <td>1193</td>\n",
       "      <td>Crailsheimer Engel-Bräu</td>\n",
       "      <td>2010-11-09 19:12:42</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>dqrull</td>\n",
       "      <td>Keller Bier / Zwickel Bier</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Engel Keller Dunkel</td>\n",
       "      <td>5.3</td>\n",
       "      <td>63324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        brewery_id             brewery_name         review_time  \\\n",
       "659305        1193  Crailsheimer Engel-Bräu 2010-11-09 19:12:42   \n",
       "\n",
       "        review_overall  review_aroma  review_appearance review_profilename  \\\n",
       "659305             3.5           3.5                4.0             dqrull   \n",
       "\n",
       "                        beer_style  review_palate  review_taste  \\\n",
       "659305  Keller Bier / Zwickel Bier            4.0           4.0   \n",
       "\n",
       "                   beer_name  beer_abv  beer_beerid  \n",
       "659305  Engel Keller Dunkel        5.3        63324  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df[df.isna().any(axis=1)])\n",
    "display(df[df['brewery_name'].isna()])\n",
    "\n",
    "display(df[df['brewery_id'] == 1193])\n",
    "df.loc[df['brewery_id'] == 1193, 'brewery_name'] = 'Crailsheimer Engel-Bräu'\n",
    "df.loc[df['brewery_id'] == 1193, 'beer_name'] = df.loc[df['brewery_id'] == 1193, 'beer_name'].apply(lambda name: name.split(' WRONG')[0])\n",
    "display(df[df['brewery_id'] == 1193])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b925c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brewery_id</th>\n",
       "      <th>brewery_name</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review_overall</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_profilename</th>\n",
       "      <th>beer_style</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>beer_name</th>\n",
       "      <th>beer_abv</th>\n",
       "      <th>beer_beerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1391053</th>\n",
       "      <td>27</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-11-11 07:10:08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Docer</td>\n",
       "      <td>American Stout</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Caboose Oatmeal Stout</td>\n",
       "      <td>7.0</td>\n",
       "      <td>75137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         brewery_id brewery_name         review_time  review_overall  \\\n",
       "1391053          27         None 2011-11-11 07:10:08             5.0   \n",
       "\n",
       "         review_aroma  review_appearance review_profilename      beer_style  \\\n",
       "1391053           4.5                4.0              Docer  American Stout   \n",
       "\n",
       "         review_palate  review_taste              beer_name  beer_abv  \\\n",
       "1391053            4.5           4.5  Caboose Oatmeal Stout       7.0   \n",
       "\n",
       "         beer_beerid  \n",
       "1391053        75137  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df[df['brewery_id'] == 27])\n",
    "\n",
    "if (1391053 in df.index):\n",
    "    df.loc[1391053, 'brewery_id'] = 24831\n",
    "    df.loc[1391053, 'brewery_name'] = 'American Brewing Company'\n",
    "\n",
    "if (1391051 in df.index):\n",
    "    df.loc[1391051, 'brewery_id'] = 24831\n",
    "    df.loc[1391051, 'brewery_name'] = 'American Brewing Company'\n",
    "\n",
    "if (1391052 in df.index):  \n",
    "    df.loc[1391052, 'brewery_id'] = 24831\n",
    "    df.loc[1391052, 'brewery_name'] = 'American Brewing Company'\n",
    "\n",
    "if (1391049 in df.index):  \n",
    "    df.loc[1391049, 'brewery_id'] = 782\n",
    "    df.loc[1391049, 'brewery_name'] = 'City Brewing Company, LLC'\n",
    "    df.loc[1391049, 'beer_name'] = 'Side Pocket High Gravity Ale'\n",
    "\n",
    "if (1391050 in df.index):  \n",
    "    df.loc[1391050, 'brewery_id'] = 782\n",
    "    df.loc[1391050, 'brewery_name'] = 'City Brewing Company, LLC'\n",
    "    df.loc[1391050, 'beer_name'] = 'Side Pocket High Gravity Ale'\n",
    "\n",
    "if (1391050 in df.index):  \n",
    "    df.drop(1391050, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "300b743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['review_profilename'].isna(), 'review_profilename'] = 'Anonymous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c5547a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['beer_style'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c2745c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3850"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'American Barleywine': 10.7, 'Cream Ale': 5.2, 'English Pale Ale': 5.0, 'Czech Pilsener': 5.0, 'Milk / Sweet Stout': 5.8, 'Quadrupel (Quad)': 10.5, 'Fruit / Vegetable Beer': 5.8, 'Munich Dunkel Lager': 5.1, 'Pumpkin Ale': 6.5, 'California Common / Steam Beer': 5.3, 'American Pale Wheat Ale': 5.2, 'German Pilsener': 5.1, 'American Amber / Red Ale': 6.0, 'Winter Warmer': 6.6, 'Scotch Ale / Wee Heavy': 8.2, 'Belgian Pale Ale': 6.1, 'English Barleywine': 10.9, 'American IPA': 6.6, 'Belgian Strong Dark Ale': 9.4, 'Smoked Beer': 7.1, 'Belgian Dark Ale': 6.3, 'Irish Dry Stout': 4.9, 'Hefeweizen': 5.3, 'American Amber / Red Lager': 4.9, 'Weizenbock': 8.1, 'American Strong Ale': 9.7, 'American Double / Imperial Stout': 10.6, 'Russian Imperial Stout': 10.0, 'Lambic - Fruit': 5.0, 'Altbier': 5.8, 'American Double / Imperial IPA': 9.4, 'Keller Bier / Zwickel Bier': 5.2, 'American Brown Ale': 6.5, 'Dubbel': 7.6, 'Tripel': 8.9, 'Witbier': 5.5, 'English India Pale Ale (IPA)': 6.1, 'American Adjunct Lager': 4.9, 'Maibock / Helles Bock': 6.9, 'Belgian Strong Pale Ale': 8.7, 'Euro Pale Lager': 5.1, 'Flanders Red Ale': 6.1, 'American Porter': 6.2, 'Extra Special / Strong Bitter (ESB)': 5.8, 'English Bitter': 4.3, 'American Pale Ale (APA)': 5.5, 'American Stout': 6.3, 'Berliner Weissbier': 4.2, 'Wheatwine': 10.6, 'Märzen / Oktoberfest': 5.9, 'Flanders Oud Bruin': 7.4, 'Scottish Ale': 5.8, 'American Pale Lager': 5.0, 'American Wild Ale': 7.7, 'Chile Beer': 6.4, 'Schwarzbier': 5.2, 'Saison / Farmhouse Ale': 7.0, 'American Double / Imperial Pilsner': 8.2, 'English Brown Ale': 5.3, 'Light Lager': 4.0, 'Kölsch': 5.0, 'Munich Helles Lager': 5.0, 'Belgian IPA': 8.4, 'American Black Ale': 8.0, 'Euro Dark Lager': 5.3, 'Oatmeal Stout': 5.8, 'Doppelbock': 8.3, 'Bock': 5.9, 'Old Ale': 9.5, 'Japanese Rice Lager': 4.9, 'American Malt Liquor': 7.4, 'English Porter': 5.7, 'Baltic Porter': 8.2, 'Herbed / Spiced Beer': 6.6, 'Gueuze': 5.6, 'Bière de Garde': 7.3, 'American Blonde Ale': 5.0, 'Foreign / Export Stout': 7.7, 'Eisbock': 11.5, 'Dunkelweizen': 5.4, 'Rye Beer': 6.7, 'English Dark Mild Ale': 4.1, 'Scottish Gruit / Ancient Herbed Ale': 6.0, 'Gose': 4.7, 'Irish Red Ale': 5.3, 'Kvass': 1.5, 'Vienna Lager': 5.0, 'Lambic - Unblended': 5.3, 'Low Alcohol Beer': 0.5, 'Rauchbier': 5.7, 'English Strong Ale': 7.2, 'Dortmunder / Export Lager': 5.5, 'American Dark Wheat Ale': 5.6, 'Black & Tan': 5.3, 'Kristalweizen': 5.1, 'English Pale Mild Ale': 4.3, 'Euro Strong Lager': 8.1, 'Bière de Champagne / Bière Brut': 10.6, 'Roggenbier': 5.5, 'English Stout': 5.3, 'Braggot': 9.3, 'Faro': 4.8, 'Sahti': 8.6, 'Happoshu': 5.4}\n"
     ]
    }
   ],
   "source": [
    "display(len(df.loc[df['beer_abv'].isna(), 'beer_name'].unique()))\n",
    "\n",
    "def create_mean(df):\n",
    "    means = {}\n",
    "    for style in df['beer_style'].unique():\n",
    "        mean_abv = df.loc[df['beer_style'] == style, 'beer_abv'].mean()\n",
    "        means[style] = round(mean_abv, 1)\n",
    "\n",
    "    return means\n",
    "\n",
    "def fill_mean(means, row):\n",
    "    return \n",
    "\n",
    "means = create_mean(df)\n",
    "print(means)\n",
    "df.loc[df['beer_abv'].isna(), 'beer_abv'] = df.loc[df['beer_abv'].isna()].apply(lambda row: means[row['beer_style']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b50239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brewery_id            0\n",
      "brewery_name          0\n",
      "review_time           0\n",
      "review_overall        0\n",
      "review_aroma          0\n",
      "review_appearance     0\n",
      "review_profilename    0\n",
      "beer_style            0\n",
      "review_palate         0\n",
      "review_taste          0\n",
      "beer_name             0\n",
      "beer_abv              0\n",
      "beer_beerid           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[df.isna().any(axis=1)].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5cb73c",
   "metadata": {},
   "source": [
    "## Bag of Word for Brewery and Beer Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abc8e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import sklearn\n",
    "sklearn.set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fbe2e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_TO_REMOVE = punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a26b9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "859964                               mikkeller aps big worst\n",
       "581443     petes brewing company petes wicked wanderlust ...\n",
       "1270456               greene king  morland brewery abbot ale\n",
       "477415                    brewery lobkowicz lobkowicz knight\n",
       "1408999      wells  youngs ltd youngs double chocolate stout\n",
       "                                 ...                        \n",
       "1540908                     victory brewing company festbier\n",
       "912301     bierbrouwerij sint christoffel bv christoffel ...\n",
       "1192883                  dogfish head brewery 120 minute ipa\n",
       "209014         new belgium brewing lips of faith  le terroir\n",
       "212102     gasthausbrauerei braustelle freigeist deutsche...\n",
       "Name: text, Length: 158662, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"brewery_name\"] = df[\"brewery_name\"].str.lower()\n",
    "df[\"brewery_name\"] = df[\"brewery_name\"].apply(lambda text: remove_punctuation(text))\n",
    "\n",
    "df[\"beer_name\"] = df[\"beer_name\"].str.lower()\n",
    "df[\"beer_name\"] = df[\"beer_name\"].apply(lambda text: remove_punctuation(text))\n",
    "\n",
    "df[\"text\"] = df[\"brewery_name\"] + \" \" + df[\"beer_name\"]\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "439060fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16221\n",
      "{'lager', 'old', 'company', 'samuel', 'co', 'porter', 'ipa', 'beer', 'de', 'sierra', 'brewery', 'imperial', 'brouwerij', 'stout', 'adams', 'pale', 'brewing', 'ale', 'the', 'stone'}\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter()\n",
    "for text in df[\"text\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(20)])\n",
    "\n",
    "counts = list(cnt.values())\n",
    "print(len(counts))\n",
    "print(FREQWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6147e2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15034\n"
     ]
    }
   ],
   "source": [
    "n_rare = sum([i <= 100 for i in counts])\n",
    "print(n_rare)\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare-1:-1]])\n",
    "\n",
    "def remove_rarewords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_rarewords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f6197c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>10th</th>\n",
       "      <th>12</th>\n",
       "      <th>120</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>1554</th>\n",
       "      <th>15th</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>yakima</th>\n",
       "      <th>yards</th>\n",
       "      <th>yeti</th>\n",
       "      <th>youngs</th>\n",
       "      <th>yuengling</th>\n",
       "      <th>yulesmith</th>\n",
       "      <th>zywiec</th>\n",
       "      <th>éphémère</th>\n",
       "      <th>ølfabrikken</th>\n",
       "      <th>über</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>859964</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581443</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270456</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477415</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540908</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912301</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192883</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209014</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158662 rows × 1170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         10  100  10th  12  120  13  15  1554  15th  16  ...  yakima  yards  \\\n",
       "859964    0    0     0   0    0   0   0     0     0   0  ...       0      0   \n",
       "581443    0    0     0   0    0   0   0     0     0   0  ...       0      0   \n",
       "1270456   0    0     0   0    0   0   0     0     0   0  ...       0      0   \n",
       "477415    0    0     0   0    0   0   0     0     0   0  ...       0      0   \n",
       "1408999   0    0     0   0    0   0   0     0     0   0  ...       0      0   \n",
       "...      ..  ...   ...  ..  ...  ..  ..   ...   ...  ..  ...     ...    ...   \n",
       "1540908   0    0     0   0    0   0   0     0     0   0  ...       0      0   \n",
       "912301    0    0     0   0    0   0   0     0     0   0  ...       0      0   \n",
       "1192883   0    0     0   0    1   0   0     0     0   0  ...       0      0   \n",
       "209014    0    0     0   0    0   0   0     0     0   0  ...       0      0   \n",
       "212102    0    0     0   0    0   0   0     0     0   0  ...       0      0   \n",
       "\n",
       "         yeti  youngs  yuengling  yulesmith  zywiec  éphémère  ølfabrikken  \\\n",
       "859964      0       0          0          0       0         0            0   \n",
       "581443      0       0          0          0       0         0            0   \n",
       "1270456     0       0          0          0       0         0            0   \n",
       "477415      0       0          0          0       0         0            0   \n",
       "1408999     0       2          0          0       0         0            0   \n",
       "...       ...     ...        ...        ...     ...       ...          ...   \n",
       "1540908     0       0          0          0       0         0            0   \n",
       "912301      0       0          0          0       0         0            0   \n",
       "1192883     0       0          0          0       0         0            0   \n",
       "209014      0       0          0          0       0         0            0   \n",
       "212102      0       0          0          0       0         0            0   \n",
       "\n",
       "         über  \n",
       "859964      0  \n",
       "581443      0  \n",
       "1270456     0  \n",
       "477415      0  \n",
       "1408999     0  \n",
       "...       ...  \n",
       "1540908     0  \n",
       "912301      0  \n",
       "1192883     0  \n",
       "209014      0  \n",
       "212102      0  \n",
       "\n",
       "[158662 rows x 1170 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "words = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "words.set_index(df.index, inplace=True)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e42f573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bag = df.merge(words, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ea8dee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.index.equals(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185a4d56",
   "metadata": {},
   "source": [
    "## Splitting Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11ebe8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4993f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df['beer_style'])\n",
    "df['class'] = le.transform(df['beer_style'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8f157d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# le = LabelEncoder()\n",
    "# le.fit(df['review_profilename'])\n",
    "# df['review_profilecode'] = le.transform(df['review_profilename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b1db7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_overall</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>beer_abv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>859964</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581443</th>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270456</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477415</th>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408999</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540908</th>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912301</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192883</th>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209014</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212102</th>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158662 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_overall  review_aroma  review_appearance  review_palate  \\\n",
       "859964              4.0           4.5                4.5            3.5   \n",
       "581443              2.5           3.0                3.5            3.0   \n",
       "1270456             4.0           3.5                4.0            3.5   \n",
       "477415              4.5           4.0                3.5            4.0   \n",
       "1408999             5.0           4.0                5.0            5.0   \n",
       "...                 ...           ...                ...            ...   \n",
       "1540908             4.5           3.5                3.0            4.0   \n",
       "912301              4.0           2.5                3.5            3.5   \n",
       "1192883             2.5           3.5                4.5            3.5   \n",
       "209014              4.0           4.5                4.0            4.0   \n",
       "212102              2.5           3.0                3.5            3.5   \n",
       "\n",
       "         review_taste  beer_abv  \n",
       "859964            3.5      18.5  \n",
       "581443            2.5       5.6  \n",
       "1270456           3.5       5.0  \n",
       "477415            4.5       4.8  \n",
       "1408999           5.0       5.2  \n",
       "...               ...       ...  \n",
       "1540908           4.0       5.6  \n",
       "912301            3.5       6.4  \n",
       "1192883           3.5      18.0  \n",
       "209014            4.5       7.5  \n",
       "212102            3.0       8.0  \n",
       "\n",
       "[158662 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.copy()\n",
    "X.drop('brewery_name', axis=1, inplace=True)\n",
    "X.drop('brewery_id', axis=1, inplace=True)\n",
    "X.drop('beer_name', axis=1, inplace=True)\n",
    "X.drop('beer_beerid', axis=1, inplace=True)\n",
    "X.drop('beer_style', axis=1, inplace=True)\n",
    "X.drop('review_profilename', axis=1, inplace=True)\n",
    "X.drop('review_time', axis=1, inplace=True)\n",
    "X.drop('text', axis=1, inplace=True)\n",
    "\n",
    "y = df['class']\n",
    "X.drop('class', axis=1, inplace=True)\n",
    "# X = normalize(X, norm='l2')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cfd1e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.index.equals(words.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fe9226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, words_train, words_valid, y_train, y_valid = train_test_split(X, words, y, test_size=0.33, random_state=42)\n",
    "# words_train, words_test, y_train, y_test = train_test_split(words, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f7520b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106303\n",
      "106303\n",
      "106303\n",
      "True\n",
      "True\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(words_train))\n",
    "print(len(y_train))\n",
    "print(X_train.index.equals(y_train.index))\n",
    "print(X_train.index.equals(words_train.index))\n",
    "print(y.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db5a44",
   "metadata": {},
   "source": [
    "## Scaling, Feature Selection, Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17097edd",
   "metadata": {},
   "source": [
    "## Check for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad0a2130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'review_overall'}>,\n",
       "        <Axes: title={'center': 'review_aroma'}>],\n",
       "       [<Axes: title={'center': 'review_appearance'}>,\n",
       "        <Axes: title={'center': 'review_palate'}>],\n",
       "       [<Axes: title={'center': 'review_taste'}>,\n",
       "        <Axes: title={'center': 'beer_abv'}>]], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train.hist(bins=30, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45e3bc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_overall</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>beer_abv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review_overall, review_aroma, review_appearance, review_palate, review_taste, beer_abv]\n",
       "Index: []"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[X_train['beer_abv'] > 31])\n",
    "len(X_train[X_train['beer_abv'] > 50])\n",
    "X_train[X_train['beer_abv'] > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58043892",
   "metadata": {},
   "source": [
    "Looks like there exist really that strong beers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1be3c1",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c24af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2fb888b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mRandomForestClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m SelectFromModel(clf, prefit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m words_new \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(words_train)\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    448\u001b[0m ]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/sklearn/tree/_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/sklearn/tree/_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    434\u001b[0m         splitter,\n\u001b[1;32m    435\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=20, max_features=100).fit(words_train, y_train)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "words_new = model.transform(words_train)\n",
    "words_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd85fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_new = words_train[words_train.columns[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2890386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_train.index.equals(X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bdea90c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m f_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mclf\u001b[49m\u001b[38;5;241m.\u001b[39mfeature_names_in_,clf\u001b[38;5;241m.\u001b[39mfeature_importances_))\n\u001b[1;32m      2\u001b[0m f_i\u001b[38;5;241m.\u001b[39msort(key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x : x[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      3\u001b[0m f_i \u001b[38;5;241m=\u001b[39m f_i[:\u001b[38;5;241m20\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "f_i = list(zip(clf.feature_names_in_,clf.feature_importances_))\n",
    "f_i.sort(key = lambda x : x[1])\n",
    "f_i = f_i[:20]\n",
    "plt.barh([x[0] for x in f_i],[x[1] for x in f_i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a72b99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bag = X_train.merge(words_new, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2003acde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_overall</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>beer_abv</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>10th</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>autumn</th>\n",
       "      <th>aventinus</th>\n",
       "      <th>avery</th>\n",
       "      <th>aying</th>\n",
       "      <th>ayinger</th>\n",
       "      <th>back</th>\n",
       "      <th>bad</th>\n",
       "      <th>ballast</th>\n",
       "      <th>baltic</th>\n",
       "      <th>baltika</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>461298</th>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567650</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075743</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771813</th>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319046</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551606</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210112</th>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829163</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170468</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425167</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106303 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_overall  review_aroma  review_appearance  review_palate  \\\n",
       "461298              3.5           4.0                3.5            3.5   \n",
       "1567650             4.0           3.5                3.0            4.0   \n",
       "1075743             4.0           3.5                3.5            3.5   \n",
       "771813              4.5           4.0                4.0            4.0   \n",
       "319046              3.0           3.5                2.5            2.5   \n",
       "...                 ...           ...                ...            ...   \n",
       "551606              4.5           2.5                4.5            3.5   \n",
       "1210112             3.5           4.0                4.5            4.0   \n",
       "829163              4.0           4.0                3.5            3.0   \n",
       "1170468             4.0           4.5                3.5            4.0   \n",
       "1425167             5.0           4.5                5.0            4.0   \n",
       "\n",
       "         review_taste  beer_abv  10  100  10th  12  ...  autumn  aventinus  \\\n",
       "461298            3.5       3.9   0    0     0   0  ...       0          0   \n",
       "1567650           4.0       7.5   0    0     0   0  ...       0          0   \n",
       "1075743           3.5       6.6   0    0     0   0  ...       0          0   \n",
       "771813            4.5       6.5   0    0     0   0  ...       0          0   \n",
       "319046            2.5       5.5   0    0     0   0  ...       0          0   \n",
       "...               ...       ...  ..  ...   ...  ..  ...     ...        ...   \n",
       "551606            4.0       5.4   0    0     0   0  ...       0          0   \n",
       "1210112           3.5       7.5   0    0     0   0  ...       0          0   \n",
       "829163            4.5       9.3   0    0     0   0  ...       0          0   \n",
       "1170468           4.5       6.0   0    0     0   0  ...       0          0   \n",
       "1425167           4.5       5.2   0    0     0   0  ...       0          0   \n",
       "\n",
       "         avery  aying  ayinger  back  bad  ballast  baltic  baltika  \n",
       "461298       0      0        0     0    0        0       0        0  \n",
       "1567650      0      0        0     0    0        0       0        0  \n",
       "1075743      0      0        0     0    0        0       0        0  \n",
       "771813       0      0        0     0    0        0       0        0  \n",
       "319046       0      0        0     0    0        0       0        0  \n",
       "...        ...    ...      ...   ...  ...      ...     ...      ...  \n",
       "551606       0      0        0     0    0        0       0        0  \n",
       "1210112      0      0        0     0    0        0       0        0  \n",
       "829163       0      0        0     0    0        0       0        0  \n",
       "1170468      0      0        0     0    0        0       0        0  \n",
       "1425167      0      0        0     0    0        0       0        0  \n",
       "\n",
       "[106303 rows x 106 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e8bf931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_overall</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>beer_abv</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>10th</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>autumn</th>\n",
       "      <th>aventinus</th>\n",
       "      <th>avery</th>\n",
       "      <th>aying</th>\n",
       "      <th>ayinger</th>\n",
       "      <th>back</th>\n",
       "      <th>bad</th>\n",
       "      <th>ballast</th>\n",
       "      <th>baltic</th>\n",
       "      <th>baltika</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1519525</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662262</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961338</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728521</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050775</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563841</th>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884138</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550271</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895352</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206435</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52359 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_overall  review_aroma  review_appearance  review_palate  \\\n",
       "1519525             4.0           4.0                4.0            4.0   \n",
       "662262              4.0           4.0                4.5            4.0   \n",
       "961338              4.0           4.0                4.5            4.5   \n",
       "728521              4.0           4.0                3.0            3.0   \n",
       "1050775             3.0           2.5                3.0            3.0   \n",
       "...                 ...           ...                ...            ...   \n",
       "1563841             4.5           4.0                4.5            4.0   \n",
       "884138              4.0           4.5                4.0            4.0   \n",
       "550271              3.0           3.0                3.5            3.0   \n",
       "895352              4.0           4.0                4.5            4.0   \n",
       "206435              4.0           4.0                4.5            4.0   \n",
       "\n",
       "         review_taste  beer_abv  10  100  10th  12  ...  autumn  aventinus  \\\n",
       "1519525           4.0       5.2   0    0     0   0  ...       0          0   \n",
       "662262            4.0       7.0   0    0     0   0  ...       0          0   \n",
       "961338            4.5       9.8   0    0     0   0  ...       0          0   \n",
       "728521            4.0       6.4   0    0     0   0  ...       0          0   \n",
       "1050775           3.5       4.2   0    0     0   0  ...       0          0   \n",
       "...               ...       ...  ..  ...   ...  ..  ...     ...        ...   \n",
       "1563841           4.5       4.5   0    0     0   0  ...       0          0   \n",
       "884138            4.0       5.9   0    0     0   0  ...       0          0   \n",
       "550271            3.5       4.2   0    0     0   0  ...       0          0   \n",
       "895352            4.0       8.5   0    0     0   0  ...       0          0   \n",
       "206435            4.0       6.5   0    0     0   0  ...       0          0   \n",
       "\n",
       "         avery  aying  ayinger  back  bad  ballast  baltic  baltika  \n",
       "1519525      0      0        0     0    0        0       0        0  \n",
       "662262       0      0        0     0    0        0       0        0  \n",
       "961338       0      0        0     0    0        0       0        0  \n",
       "728521       0      0        0     0    0        0       0        0  \n",
       "1050775      0      0        0     0    0        0       0        0  \n",
       "...        ...    ...      ...   ...  ...      ...     ...      ...  \n",
       "1563841      0      0        0     0    0        0       0        0  \n",
       "884138       0      0        0     0    0        0       0        0  \n",
       "550271       0      0        0     0    0        0       0        0  \n",
       "895352       0      0        0     0    0        0       0        0  \n",
       "206435       0      0        0     0    0        0       0        0  \n",
       "\n",
       "[52359 rows x 106 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_bag = X_valid.merge(words_valid[words_new.columns], how='inner', left_index=True, right_index=True)\n",
    "X_valid_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2757d3",
   "metadata": {},
   "source": [
    "# Find Solution of NN + THEORIE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4afda021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7ff094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ttrain, X_test, y_ttrain, y_test = train_test_split(X_train_bag.values, y_train.values, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65e8e5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.5 2.5 4.  ... 0.  0.  0. ]\n",
      " [3.  3.  3.5 ... 0.  0.  0. ]\n",
      " [4.  3.5 3.  ... 0.  0.  0. ]\n",
      " ...\n",
      " [3.  3.  3.  ... 0.  0.  0. ]\n",
      " [4.  4.  4.5 ... 0.  0.  0. ]\n",
      " [2.5 2.  2.  ... 0.  0.  0. ]]\n",
      "[49 90 73 ... 93 89  1]\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "print(X_ttrain)\n",
    "print(y_ttrain)\n",
    "print(y_ttrain.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e8573",
   "metadata": {},
   "source": [
    "## Build torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a580683",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.any(np.isnan(X_ttrain))\n",
    "assert not np.any(np.isnan(y_ttrain))\n",
    "assert not np.any(np.isnan(X_test))\n",
    "assert not np.any(np.isnan(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb1b970c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# convert a df to tensor to be used in pytorch\n",
    "def X_to_tensor(df):\n",
    "    return torch.from_numpy(df).float().to(device)\n",
    "\n",
    "def y_to_tensor(df):\n",
    "    return torch.from_numpy(df).long().to(device)\n",
    "\n",
    "X_train_tensor = X_to_tensor(X_ttrain)\n",
    "y_train_tensor = y_to_tensor(y_ttrain)\n",
    "\n",
    "X_test_tensor = X_to_tensor(X_test)\n",
    "y_test_tensor = y_to_tensor(y_test)\n",
    "\n",
    "X_valid_tensor = X_to_tensor(X_valid_bag.values)\n",
    "y_valid_tensor = y_to_tensor(y_valid.values)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "valid_ds = TensorDataset(X_valid_tensor, y_valid_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc9573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 282])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(valid_ds, batch_size=batch_size)\n",
    "\n",
    "for XX, yy in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {XX.shape}\")\n",
    "    print(f\"Shape of y: {yy.shape} {yy.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16463da9",
   "metadata": {},
   "source": [
    "## Creating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0c38c825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=282, out_features=250, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=250, out_features=164, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=164, out_features=164, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=164, out_features=104, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # self.linear_relu_stack = nn.Sequential(\n",
    "        #     nn.Linear(292, 612),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(612, 1024),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(1024, 1024),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(1024, 104)\n",
    "        # )\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(282, 250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 164),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(164, 164),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(164, 104)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5300a4",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95387a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (XX, yy) in enumerate(dataloader):\n",
    "        XX, yy = XX.to(device), yy.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(XX)\n",
    "        loss = loss_fn(pred, yy)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(XX)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a613680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for XX, yy in dataloader:\n",
    "            XX, yy = XX.to(device), yy.to(device)\n",
    "            pred = model(XX)\n",
    "            test_loss += loss_fn(pred, yy).item()\n",
    "            correct += (pred.argmax(1) == yy).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a02e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4.648266  [   64/74412]\n",
      "loss: 4.647295  [ 6464/74412]\n",
      "loss: 4.642521  [12864/74412]\n",
      "loss: 4.636545  [19264/74412]\n",
      "loss: 4.638532  [25664/74412]\n",
      "loss: 4.634614  [32064/74412]\n",
      "loss: 4.619991  [38464/74412]\n",
      "loss: 4.618537  [44864/74412]\n",
      "loss: 4.622634  [51264/74412]\n",
      "loss: 4.616058  [57664/74412]\n",
      "loss: 4.619352  [64064/74412]\n",
      "loss: 4.619661  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 3.2%, Avg loss: 4.609188 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 4.613788  [   64/74412]\n",
      "loss: 4.617097  [ 6464/74412]\n",
      "loss: 4.607101  [12864/74412]\n",
      "loss: 4.577701  [19264/74412]\n",
      "loss: 4.603384  [25664/74412]\n",
      "loss: 4.586442  [32064/74412]\n",
      "loss: 4.568108  [38464/74412]\n",
      "loss: 4.561793  [44864/74412]\n",
      "loss: 4.564485  [51264/74412]\n",
      "loss: 4.540073  [57664/74412]\n",
      "loss: 4.551323  [64064/74412]\n",
      "loss: 4.569292  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 4.533602 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 4.549047  [   64/74412]\n",
      "loss: 4.556447  [ 6464/74412]\n",
      "loss: 4.522321  [12864/74412]\n",
      "loss: 4.429866  [19264/74412]\n",
      "loss: 4.513262  [25664/74412]\n",
      "loss: 4.441970  [32064/74412]\n",
      "loss: 4.398906  [38464/74412]\n",
      "loss: 4.367568  [44864/74412]\n",
      "loss: 4.352928  [51264/74412]\n",
      "loss: 4.248955  [57664/74412]\n",
      "loss: 4.298827  [64064/74412]\n",
      "loss: 4.397597  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 4.278351 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 4.349995  [   64/74412]\n",
      "loss: 4.394661  [ 6464/74412]\n",
      "loss: 4.291330  [12864/74412]\n",
      "loss: 4.069799  [19264/74412]\n",
      "loss: 4.298592  [25664/74412]\n",
      "loss: 4.170842  [32064/74412]\n",
      "loss: 4.151895  [38464/74412]\n",
      "loss: 4.107512  [44864/74412]\n",
      "loss: 4.151182  [51264/74412]\n",
      "loss: 4.070485  [57664/74412]\n",
      "loss: 4.096772  [64064/74412]\n",
      "loss: 4.294338  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 4.169942 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 4.270308  [   64/74412]\n",
      "loss: 4.297612  [ 6464/74412]\n",
      "loss: 4.180315  [12864/74412]\n",
      "loss: 4.012699  [19264/74412]\n",
      "loss: 4.190736  [25664/74412]\n",
      "loss: 4.075197  [32064/74412]\n",
      "loss: 4.112967  [38464/74412]\n",
      "loss: 4.012957  [44864/74412]\n",
      "loss: 4.104687  [51264/74412]\n",
      "loss: 4.043536  [57664/74412]\n",
      "loss: 4.049987  [64064/74412]\n",
      "loss: 4.255142  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 4.135684 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 4.258389  [   64/74412]\n",
      "loss: 4.247161  [ 6464/74412]\n",
      "loss: 4.140123  [12864/74412]\n",
      "loss: 4.025161  [19264/74412]\n",
      "loss: 4.163880  [25664/74412]\n",
      "loss: 4.041085  [32064/74412]\n",
      "loss: 4.110186  [38464/74412]\n",
      "loss: 3.982398  [44864/74412]\n",
      "loss: 4.086725  [51264/74412]\n",
      "loss: 4.033508  [57664/74412]\n",
      "loss: 4.036610  [64064/74412]\n",
      "loss: 4.234446  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 4.122159 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 4.254796  [   64/74412]\n",
      "loss: 4.224704  [ 6464/74412]\n",
      "loss: 4.124321  [12864/74412]\n",
      "loss: 4.036629  [19264/74412]\n",
      "loss: 4.154778  [25664/74412]\n",
      "loss: 4.023268  [32064/74412]\n",
      "loss: 4.107849  [38464/74412]\n",
      "loss: 3.972514  [44864/74412]\n",
      "loss: 4.078280  [51264/74412]\n",
      "loss: 4.026907  [57664/74412]\n",
      "loss: 4.030851  [64064/74412]\n",
      "loss: 4.222432  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 4.114928 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 4.248932  [   64/74412]\n",
      "loss: 4.214550  [ 6464/74412]\n",
      "loss: 4.116935  [12864/74412]\n",
      "loss: 4.041568  [19264/74412]\n",
      "loss: 4.149887  [25664/74412]\n",
      "loss: 4.011220  [32064/74412]\n",
      "loss: 4.104731  [38464/74412]\n",
      "loss: 3.967480  [44864/74412]\n",
      "loss: 4.074041  [51264/74412]\n",
      "loss: 4.021944  [57664/74412]\n",
      "loss: 4.027676  [64064/74412]\n",
      "loss: 4.214870  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 7.6%, Avg loss: 4.109759 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 4.242568  [   64/74412]\n",
      "loss: 4.209241  [ 6464/74412]\n",
      "loss: 4.112717  [12864/74412]\n",
      "loss: 4.042909  [19264/74412]\n",
      "loss: 4.145825  [25664/74412]\n",
      "loss: 4.002422  [32064/74412]\n",
      "loss: 4.101305  [38464/74412]\n",
      "loss: 3.963334  [44864/74412]\n",
      "loss: 4.071257  [51264/74412]\n",
      "loss: 4.017717  [57664/74412]\n",
      "loss: 4.025496  [64064/74412]\n",
      "loss: 4.209294  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 7.6%, Avg loss: 4.105201 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 4.236784  [   64/74412]\n",
      "loss: 4.205567  [ 6464/74412]\n",
      "loss: 4.109524  [12864/74412]\n",
      "loss: 4.041964  [19264/74412]\n",
      "loss: 4.141675  [25664/74412]\n",
      "loss: 3.994905  [32064/74412]\n",
      "loss: 4.097607  [38464/74412]\n",
      "loss: 3.959067  [44864/74412]\n",
      "loss: 4.068554  [51264/74412]\n",
      "loss: 4.013622  [57664/74412]\n",
      "loss: 4.023436  [64064/74412]\n",
      "loss: 4.204360  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 7.7%, Avg loss: 4.100601 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 4.231462  [   64/74412]\n",
      "loss: 4.202186  [ 6464/74412]\n",
      "loss: 4.106414  [12864/74412]\n",
      "loss: 4.039081  [19264/74412]\n",
      "loss: 4.137103  [25664/74412]\n",
      "loss: 3.987568  [32064/74412]\n",
      "loss: 4.093568  [38464/74412]\n",
      "loss: 3.954137  [44864/74412]\n",
      "loss: 4.065303  [51264/74412]\n",
      "loss: 4.008987  [57664/74412]\n",
      "loss: 4.020745  [64064/74412]\n",
      "loss: 4.199406  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 7.9%, Avg loss: 4.095398 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 4.226001  [   64/74412]\n",
      "loss: 4.198057  [ 6464/74412]\n",
      "loss: 4.102729  [12864/74412]\n",
      "loss: 4.033846  [19264/74412]\n",
      "loss: 4.131544  [25664/74412]\n",
      "loss: 3.979276  [32064/74412]\n",
      "loss: 4.088502  [38464/74412]\n",
      "loss: 3.947797  [44864/74412]\n",
      "loss: 4.060961  [51264/74412]\n",
      "loss: 4.003394  [57664/74412]\n",
      "loss: 4.017428  [64064/74412]\n",
      "loss: 4.193595  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 8.4%, Avg loss: 4.089417 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 4.220035  [   64/74412]\n",
      "loss: 4.193607  [ 6464/74412]\n",
      "loss: 4.099034  [12864/74412]\n",
      "loss: 4.026882  [19264/74412]\n",
      "loss: 4.124938  [25664/74412]\n",
      "loss: 3.969952  [32064/74412]\n",
      "loss: 4.082653  [38464/74412]\n",
      "loss: 3.940390  [44864/74412]\n",
      "loss: 4.055445  [51264/74412]\n",
      "loss: 3.997072  [57664/74412]\n",
      "loss: 4.012945  [64064/74412]\n",
      "loss: 4.186565  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 9.2%, Avg loss: 4.082092 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 4.212817  [   64/74412]\n",
      "loss: 4.188034  [ 6464/74412]\n",
      "loss: 4.094686  [12864/74412]\n",
      "loss: 4.017443  [19264/74412]\n",
      "loss: 4.116685  [25664/74412]\n",
      "loss: 3.958859  [32064/74412]\n",
      "loss: 4.075248  [38464/74412]\n",
      "loss: 3.930752  [44864/74412]\n",
      "loss: 4.047856  [51264/74412]\n",
      "loss: 3.989100  [57664/74412]\n",
      "loss: 4.006386  [64064/74412]\n",
      "loss: 4.177665  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 9.8%, Avg loss: 4.072538 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 4.203295  [   64/74412]\n",
      "loss: 4.180696  [ 6464/74412]\n",
      "loss: 4.089021  [12864/74412]\n",
      "loss: 4.004761  [19264/74412]\n",
      "loss: 4.106194  [25664/74412]\n",
      "loss: 3.944495  [32064/74412]\n",
      "loss: 4.065035  [38464/74412]\n",
      "loss: 3.918040  [44864/74412]\n",
      "loss: 4.037469  [51264/74412]\n",
      "loss: 3.978555  [57664/74412]\n",
      "loss: 3.997330  [64064/74412]\n",
      "loss: 4.166443  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 4.060140 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 4.191302  [   64/74412]\n",
      "loss: 4.171819  [ 6464/74412]\n",
      "loss: 4.081439  [12864/74412]\n",
      "loss: 3.987574  [19264/74412]\n",
      "loss: 4.091777  [25664/74412]\n",
      "loss: 3.926679  [32064/74412]\n",
      "loss: 4.051459  [38464/74412]\n",
      "loss: 3.901349  [44864/74412]\n",
      "loss: 4.023490  [51264/74412]\n",
      "loss: 3.964829  [57664/74412]\n",
      "loss: 3.984700  [64064/74412]\n",
      "loss: 4.151574  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 10.5%, Avg loss: 4.043521 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 4.174603  [   64/74412]\n",
      "loss: 4.160586  [ 6464/74412]\n",
      "loss: 4.071472  [12864/74412]\n",
      "loss: 3.964697  [19264/74412]\n",
      "loss: 4.072155  [25664/74412]\n",
      "loss: 3.903931  [32064/74412]\n",
      "loss: 4.032981  [38464/74412]\n",
      "loss: 3.878713  [44864/74412]\n",
      "loss: 4.004427  [51264/74412]\n",
      "loss: 3.946984  [57664/74412]\n",
      "loss: 3.967026  [64064/74412]\n",
      "loss: 4.132498  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 10.7%, Avg loss: 4.021293 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 4.151979  [   64/74412]\n",
      "loss: 4.145910  [ 6464/74412]\n",
      "loss: 4.057990  [12864/74412]\n",
      "loss: 3.934537  [19264/74412]\n",
      "loss: 4.045988  [25664/74412]\n",
      "loss: 3.875518  [32064/74412]\n",
      "loss: 4.008313  [38464/74412]\n",
      "loss: 3.848427  [44864/74412]\n",
      "loss: 3.979327  [51264/74412]\n",
      "loss: 3.924736  [57664/74412]\n",
      "loss: 3.942938  [64064/74412]\n",
      "loss: 4.108532  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 10.8%, Avg loss: 3.992671 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 4.122497  [   64/74412]\n",
      "loss: 4.127295  [ 6464/74412]\n",
      "loss: 4.040522  [12864/74412]\n",
      "loss: 3.896815  [19264/74412]\n",
      "loss: 4.013074  [25664/74412]\n",
      "loss: 3.841427  [32064/74412]\n",
      "loss: 3.977636  [38464/74412]\n",
      "loss: 3.811427  [44864/74412]\n",
      "loss: 3.949469  [51264/74412]\n",
      "loss: 3.900711  [57664/74412]\n",
      "loss: 3.912932  [64064/74412]\n",
      "loss: 4.082301  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 10.9%, Avg loss: 3.959817 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 4.087973  [   64/74412]\n",
      "loss: 4.102943  [ 6464/74412]\n",
      "loss: 4.020034  [12864/74412]\n",
      "loss: 3.854452  [19264/74412]\n",
      "loss: 3.976282  [25664/74412]\n",
      "loss: 3.805478  [32064/74412]\n",
      "loss: 3.944940  [38464/74412]\n",
      "loss: 3.771852  [44864/74412]\n",
      "loss: 3.917761  [51264/74412]\n",
      "loss: 3.878674  [57664/74412]\n",
      "loss: 3.880465  [64064/74412]\n",
      "loss: 4.055507  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 11.1%, Avg loss: 3.925630 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 4.051575  [   64/74412]\n",
      "loss: 4.072987  [ 6464/74412]\n",
      "loss: 3.998437  [12864/74412]\n",
      "loss: 3.811907  [19264/74412]\n",
      "loss: 3.938893  [25664/74412]\n",
      "loss: 3.772753  [32064/74412]\n",
      "loss: 3.914390  [38464/74412]\n",
      "loss: 3.734012  [44864/74412]\n",
      "loss: 3.887450  [51264/74412]\n",
      "loss: 3.860595  [57664/74412]\n",
      "loss: 3.849432  [64064/74412]\n",
      "loss: 4.031472  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 11.2%, Avg loss: 3.893311 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 4.015750  [   64/74412]\n",
      "loss: 4.040719  [ 6464/74412]\n",
      "loss: 3.977195  [12864/74412]\n",
      "loss: 3.775337  [19264/74412]\n",
      "loss: 3.903389  [25664/74412]\n",
      "loss: 3.746043  [32064/74412]\n",
      "loss: 3.887750  [38464/74412]\n",
      "loss: 3.700852  [44864/74412]\n",
      "loss: 3.860363  [51264/74412]\n",
      "loss: 3.842874  [57664/74412]\n",
      "loss: 3.820702  [64064/74412]\n",
      "loss: 4.011446  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 3.863849 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 3.982278  [   64/74412]\n",
      "loss: 4.006556  [ 6464/74412]\n",
      "loss: 3.955866  [12864/74412]\n",
      "loss: 3.744640  [19264/74412]\n",
      "loss: 3.870667  [25664/74412]\n",
      "loss: 3.725129  [32064/74412]\n",
      "loss: 3.864791  [38464/74412]\n",
      "loss: 3.672452  [44864/74412]\n",
      "loss: 3.836636  [51264/74412]\n",
      "loss: 3.824338  [57664/74412]\n",
      "loss: 3.795335  [64064/74412]\n",
      "loss: 3.993968  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 11.4%, Avg loss: 3.837512 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 3.951558  [   64/74412]\n",
      "loss: 3.973266  [ 6464/74412]\n",
      "loss: 3.933736  [12864/74412]\n",
      "loss: 3.718679  [19264/74412]\n",
      "loss: 3.842155  [25664/74412]\n",
      "loss: 3.709031  [32064/74412]\n",
      "loss: 3.844609  [38464/74412]\n",
      "loss: 3.648977  [44864/74412]\n",
      "loss: 3.816032  [51264/74412]\n",
      "loss: 3.807128  [57664/74412]\n",
      "loss: 3.775112  [64064/74412]\n",
      "loss: 3.977166  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 11.5%, Avg loss: 3.814303 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 3.923248  [   64/74412]\n",
      "loss: 3.944680  [ 6464/74412]\n",
      "loss: 3.913097  [12864/74412]\n",
      "loss: 3.698062  [19264/74412]\n",
      "loss: 3.817738  [25664/74412]\n",
      "loss: 3.696428  [32064/74412]\n",
      "loss: 3.827085  [38464/74412]\n",
      "loss: 3.628582  [44864/74412]\n",
      "loss: 3.799256  [51264/74412]\n",
      "loss: 3.789922  [57664/74412]\n",
      "loss: 3.759083  [64064/74412]\n",
      "loss: 3.961997  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 11.6%, Avg loss: 3.793681 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 3.897850  [   64/74412]\n",
      "loss: 3.919015  [ 6464/74412]\n",
      "loss: 3.894443  [12864/74412]\n",
      "loss: 3.681892  [19264/74412]\n",
      "loss: 3.796805  [25664/74412]\n",
      "loss: 3.686732  [32064/74412]\n",
      "loss: 3.812670  [38464/74412]\n",
      "loss: 3.610254  [44864/74412]\n",
      "loss: 3.785501  [51264/74412]\n",
      "loss: 3.772272  [57664/74412]\n",
      "loss: 3.745696  [64064/74412]\n",
      "loss: 3.947244  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 12.4%, Avg loss: 3.775438 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 3.875145  [   64/74412]\n",
      "loss: 3.896879  [ 6464/74412]\n",
      "loss: 3.876936  [12864/74412]\n",
      "loss: 3.668861  [19264/74412]\n",
      "loss: 3.779142  [25664/74412]\n",
      "loss: 3.678384  [32064/74412]\n",
      "loss: 3.799988  [38464/74412]\n",
      "loss: 3.594609  [44864/74412]\n",
      "loss: 3.774866  [51264/74412]\n",
      "loss: 3.754106  [57664/74412]\n",
      "loss: 3.734247  [64064/74412]\n",
      "loss: 3.933232  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 12.7%, Avg loss: 3.759177 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 3.854284  [   64/74412]\n",
      "loss: 3.877734  [ 6464/74412]\n",
      "loss: 3.860699  [12864/74412]\n",
      "loss: 3.657750  [19264/74412]\n",
      "loss: 3.763855  [25664/74412]\n",
      "loss: 3.672199  [32064/74412]\n",
      "loss: 3.788483  [38464/74412]\n",
      "loss: 3.580435  [44864/74412]\n",
      "loss: 3.766185  [51264/74412]\n",
      "loss: 3.735577  [57664/74412]\n",
      "loss: 3.723976  [64064/74412]\n",
      "loss: 3.920388  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 12.9%, Avg loss: 3.744503 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 3.835586  [   64/74412]\n",
      "loss: 3.860673  [ 6464/74412]\n",
      "loss: 3.845486  [12864/74412]\n",
      "loss: 3.648634  [19264/74412]\n",
      "loss: 3.750116  [25664/74412]\n",
      "loss: 3.667329  [32064/74412]\n",
      "loss: 3.777672  [38464/74412]\n",
      "loss: 3.567766  [44864/74412]\n",
      "loss: 3.758580  [51264/74412]\n",
      "loss: 3.717699  [57664/74412]\n",
      "loss: 3.714229  [64064/74412]\n",
      "loss: 3.908253  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 13.1%, Avg loss: 3.731255 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 3.819727  [   64/74412]\n",
      "loss: 3.845831  [ 6464/74412]\n",
      "loss: 3.831326  [12864/74412]\n",
      "loss: 3.640461  [19264/74412]\n",
      "loss: 3.738229  [25664/74412]\n",
      "loss: 3.663165  [32064/74412]\n",
      "loss: 3.767375  [38464/74412]\n",
      "loss: 3.556679  [44864/74412]\n",
      "loss: 3.751614  [51264/74412]\n",
      "loss: 3.700973  [57664/74412]\n",
      "loss: 3.705233  [64064/74412]\n",
      "loss: 3.896855  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 13.3%, Avg loss: 3.719160 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 3.805756  [   64/74412]\n",
      "loss: 3.832390  [ 6464/74412]\n",
      "loss: 3.818185  [12864/74412]\n",
      "loss: 3.633338  [19264/74412]\n",
      "loss: 3.727533  [25664/74412]\n",
      "loss: 3.659448  [32064/74412]\n",
      "loss: 3.757486  [38464/74412]\n",
      "loss: 3.546478  [44864/74412]\n",
      "loss: 3.744742  [51264/74412]\n",
      "loss: 3.685081  [57664/74412]\n",
      "loss: 3.696276  [64064/74412]\n",
      "loss: 3.886870  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 13.5%, Avg loss: 3.707943 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 3.793190  [   64/74412]\n",
      "loss: 3.820511  [ 6464/74412]\n",
      "loss: 3.806170  [12864/74412]\n",
      "loss: 3.626372  [19264/74412]\n",
      "loss: 3.718089  [25664/74412]\n",
      "loss: 3.655907  [32064/74412]\n",
      "loss: 3.747773  [38464/74412]\n",
      "loss: 3.536992  [44864/74412]\n",
      "loss: 3.738029  [51264/74412]\n",
      "loss: 3.669451  [57664/74412]\n",
      "loss: 3.687143  [64064/74412]\n",
      "loss: 3.877769  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 13.6%, Avg loss: 3.697360 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 3.781801  [   64/74412]\n",
      "loss: 3.809878  [ 6464/74412]\n",
      "loss: 3.794899  [12864/74412]\n",
      "loss: 3.619857  [19264/74412]\n",
      "loss: 3.708940  [25664/74412]\n",
      "loss: 3.652659  [32064/74412]\n",
      "loss: 3.738396  [38464/74412]\n",
      "loss: 3.527752  [44864/74412]\n",
      "loss: 3.731061  [51264/74412]\n",
      "loss: 3.653986  [57664/74412]\n",
      "loss: 3.677812  [64064/74412]\n",
      "loss: 3.869373  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 13.7%, Avg loss: 3.687216 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 3.771172  [   64/74412]\n",
      "loss: 3.800198  [ 6464/74412]\n",
      "loss: 3.784372  [12864/74412]\n",
      "loss: 3.613264  [19264/74412]\n",
      "loss: 3.700169  [25664/74412]\n",
      "loss: 3.649313  [32064/74412]\n",
      "loss: 3.729406  [38464/74412]\n",
      "loss: 3.518606  [44864/74412]\n",
      "loss: 3.723732  [51264/74412]\n",
      "loss: 3.638594  [57664/74412]\n",
      "loss: 3.668223  [64064/74412]\n",
      "loss: 3.861366  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 13.9%, Avg loss: 3.677363 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 3.760941  [   64/74412]\n",
      "loss: 3.791213  [ 6464/74412]\n",
      "loss: 3.774185  [12864/74412]\n",
      "loss: 3.606617  [19264/74412]\n",
      "loss: 3.691951  [25664/74412]\n",
      "loss: 3.645754  [32064/74412]\n",
      "loss: 3.720653  [38464/74412]\n",
      "loss: 3.509762  [44864/74412]\n",
      "loss: 3.715588  [51264/74412]\n",
      "loss: 3.623179  [57664/74412]\n",
      "loss: 3.658290  [64064/74412]\n",
      "loss: 3.853375  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 14.0%, Avg loss: 3.667633 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 3.751328  [   64/74412]\n",
      "loss: 3.782432  [ 6464/74412]\n",
      "loss: 3.764238  [12864/74412]\n",
      "loss: 3.599720  [19264/74412]\n",
      "loss: 3.684058  [25664/74412]\n",
      "loss: 3.641710  [32064/74412]\n",
      "loss: 3.711925  [38464/74412]\n",
      "loss: 3.500966  [44864/74412]\n",
      "loss: 3.706718  [51264/74412]\n",
      "loss: 3.607504  [57664/74412]\n",
      "loss: 3.648034  [64064/74412]\n",
      "loss: 3.845488  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 14.1%, Avg loss: 3.657863 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 3.741832  [   64/74412]\n",
      "loss: 3.773595  [ 6464/74412]\n",
      "loss: 3.754362  [12864/74412]\n",
      "loss: 3.592315  [19264/74412]\n",
      "loss: 3.676155  [25664/74412]\n",
      "loss: 3.636945  [32064/74412]\n",
      "loss: 3.703229  [38464/74412]\n",
      "loss: 3.492075  [44864/74412]\n",
      "loss: 3.697045  [51264/74412]\n",
      "loss: 3.591215  [57664/74412]\n",
      "loss: 3.637257  [64064/74412]\n",
      "loss: 3.837409  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 14.2%, Avg loss: 3.647870 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 3.732334  [   64/74412]\n",
      "loss: 3.764569  [ 6464/74412]\n",
      "loss: 3.744277  [12864/74412]\n",
      "loss: 3.584401  [19264/74412]\n",
      "loss: 3.668324  [25664/74412]\n",
      "loss: 3.631527  [32064/74412]\n",
      "loss: 3.694227  [38464/74412]\n",
      "loss: 3.482877  [44864/74412]\n",
      "loss: 3.686421  [51264/74412]\n",
      "loss: 3.574029  [57664/74412]\n",
      "loss: 3.626108  [64064/74412]\n",
      "loss: 3.829011  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 3.637429 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 3.722605  [   64/74412]\n",
      "loss: 3.754918  [ 6464/74412]\n",
      "loss: 3.733687  [12864/74412]\n",
      "loss: 3.575684  [19264/74412]\n",
      "loss: 3.660069  [25664/74412]\n",
      "loss: 3.624953  [32064/74412]\n",
      "loss: 3.685012  [38464/74412]\n",
      "loss: 3.473238  [44864/74412]\n",
      "loss: 3.674742  [51264/74412]\n",
      "loss: 3.555949  [57664/74412]\n",
      "loss: 3.614222  [64064/74412]\n",
      "loss: 3.819821  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 14.5%, Avg loss: 3.626385 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 3.712559  [   64/74412]\n",
      "loss: 3.744466  [ 6464/74412]\n",
      "loss: 3.722607  [12864/74412]\n",
      "loss: 3.566355  [19264/74412]\n",
      "loss: 3.651764  [25664/74412]\n",
      "loss: 3.616850  [32064/74412]\n",
      "loss: 3.675277  [38464/74412]\n",
      "loss: 3.462809  [44864/74412]\n",
      "loss: 3.661760  [51264/74412]\n",
      "loss: 3.536935  [57664/74412]\n",
      "loss: 3.601530  [64064/74412]\n",
      "loss: 3.809290  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 14.6%, Avg loss: 3.614478 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 3.702009  [   64/74412]\n",
      "loss: 3.733475  [ 6464/74412]\n",
      "loss: 3.710851  [12864/74412]\n",
      "loss: 3.556061  [19264/74412]\n",
      "loss: 3.642666  [25664/74412]\n",
      "loss: 3.607301  [32064/74412]\n",
      "loss: 3.664564  [38464/74412]\n",
      "loss: 3.451502  [44864/74412]\n",
      "loss: 3.646842  [51264/74412]\n",
      "loss: 3.516685  [57664/74412]\n",
      "loss: 3.588087  [64064/74412]\n",
      "loss: 3.797767  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 14.8%, Avg loss: 3.601485 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 3.690596  [   64/74412]\n",
      "loss: 3.721597  [ 6464/74412]\n",
      "loss: 3.698480  [12864/74412]\n",
      "loss: 3.544502  [19264/74412]\n",
      "loss: 3.632692  [25664/74412]\n",
      "loss: 3.596139  [32064/74412]\n",
      "loss: 3.652777  [38464/74412]\n",
      "loss: 3.439135  [44864/74412]\n",
      "loss: 3.629604  [51264/74412]\n",
      "loss: 3.495231  [57664/74412]\n",
      "loss: 3.573480  [64064/74412]\n",
      "loss: 3.785077  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 15.0%, Avg loss: 3.587006 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 3.678468  [   64/74412]\n",
      "loss: 3.708049  [ 6464/74412]\n",
      "loss: 3.685297  [12864/74412]\n",
      "loss: 3.531518  [19264/74412]\n",
      "loss: 3.621736  [25664/74412]\n",
      "loss: 3.582107  [32064/74412]\n",
      "loss: 3.640004  [38464/74412]\n",
      "loss: 3.425459  [44864/74412]\n",
      "loss: 3.609771  [51264/74412]\n",
      "loss: 3.472172  [57664/74412]\n",
      "loss: 3.557368  [64064/74412]\n",
      "loss: 3.771177  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 15.3%, Avg loss: 3.570961 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 3.665404  [   64/74412]\n",
      "loss: 3.692995  [ 6464/74412]\n",
      "loss: 3.671288  [12864/74412]\n",
      "loss: 3.517045  [19264/74412]\n",
      "loss: 3.609936  [25664/74412]\n",
      "loss: 3.565760  [32064/74412]\n",
      "loss: 3.625921  [38464/74412]\n",
      "loss: 3.410383  [44864/74412]\n",
      "loss: 3.586950  [51264/74412]\n",
      "loss: 3.447575  [57664/74412]\n",
      "loss: 3.539577  [64064/74412]\n",
      "loss: 3.754469  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 15.5%, Avg loss: 3.552987 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 3.650192  [   64/74412]\n",
      "loss: 3.676865  [ 6464/74412]\n",
      "loss: 3.655518  [12864/74412]\n",
      "loss: 3.500299  [19264/74412]\n",
      "loss: 3.596312  [25664/74412]\n",
      "loss: 3.546278  [32064/74412]\n",
      "loss: 3.609882  [38464/74412]\n",
      "loss: 3.393389  [44864/74412]\n",
      "loss: 3.561375  [51264/74412]\n",
      "loss: 3.422314  [57664/74412]\n",
      "loss: 3.519422  [64064/74412]\n",
      "loss: 3.737062  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 3.533501 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 3.634336  [   64/74412]\n",
      "loss: 3.659616  [ 6464/74412]\n",
      "loss: 3.640249  [12864/74412]\n",
      "loss: 3.482184  [19264/74412]\n",
      "loss: 3.582015  [25664/74412]\n",
      "loss: 3.523893  [32064/74412]\n",
      "loss: 3.592932  [38464/74412]\n",
      "loss: 3.375138  [44864/74412]\n",
      "loss: 3.532645  [51264/74412]\n",
      "loss: 3.396405  [57664/74412]\n",
      "loss: 3.497443  [64064/74412]\n",
      "loss: 3.717853  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 16.1%, Avg loss: 3.512511 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 3.616647  [   64/74412]\n",
      "loss: 3.641819  [ 6464/74412]\n",
      "loss: 3.624599  [12864/74412]\n",
      "loss: 3.461858  [19264/74412]\n",
      "loss: 3.566519  [25664/74412]\n",
      "loss: 3.499222  [32064/74412]\n",
      "loss: 3.574761  [38464/74412]\n",
      "loss: 3.355635  [44864/74412]\n",
      "loss: 3.501867  [51264/74412]\n",
      "loss: 3.370950  [57664/74412]\n",
      "loss: 3.474223  [64064/74412]\n",
      "loss: 3.696675  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 16.3%, Avg loss: 3.490643 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 3.597628  [   64/74412]\n",
      "loss: 3.623955  [ 6464/74412]\n",
      "loss: 3.608740  [12864/74412]\n",
      "loss: 3.439496  [19264/74412]\n",
      "loss: 3.549860  [25664/74412]\n",
      "loss: 3.472724  [32064/74412]\n",
      "loss: 3.555518  [38464/74412]\n",
      "loss: 3.335212  [44864/74412]\n",
      "loss: 3.470311  [51264/74412]\n",
      "loss: 3.346731  [57664/74412]\n",
      "loss: 3.450597  [64064/74412]\n",
      "loss: 3.674341  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 3.468208 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 3.577935  [   64/74412]\n",
      "loss: 3.606460  [ 6464/74412]\n",
      "loss: 3.592800  [12864/74412]\n",
      "loss: 3.416121  [19264/74412]\n",
      "loss: 3.533665  [25664/74412]\n",
      "loss: 3.445844  [32064/74412]\n",
      "loss: 3.535265  [38464/74412]\n",
      "loss: 3.314072  [44864/74412]\n",
      "loss: 3.438816  [51264/74412]\n",
      "loss: 3.324051  [57664/74412]\n",
      "loss: 3.426123  [64064/74412]\n",
      "loss: 3.651278  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 17.2%, Avg loss: 3.445910 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 3.558039  [   64/74412]\n",
      "loss: 3.588652  [ 6464/74412]\n",
      "loss: 3.576491  [12864/74412]\n",
      "loss: 3.391747  [19264/74412]\n",
      "loss: 3.517459  [25664/74412]\n",
      "loss: 3.419728  [32064/74412]\n",
      "loss: 3.514058  [38464/74412]\n",
      "loss: 3.292595  [44864/74412]\n",
      "loss: 3.408352  [51264/74412]\n",
      "loss: 3.302763  [57664/74412]\n",
      "loss: 3.401536  [64064/74412]\n",
      "loss: 3.628529  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 17.7%, Avg loss: 3.424116 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 3.539377  [   64/74412]\n",
      "loss: 3.571770  [ 6464/74412]\n",
      "loss: 3.559479  [12864/74412]\n",
      "loss: 3.367565  [19264/74412]\n",
      "loss: 3.501004  [25664/74412]\n",
      "loss: 3.395664  [32064/74412]\n",
      "loss: 3.492373  [38464/74412]\n",
      "loss: 3.271138  [44864/74412]\n",
      "loss: 3.379774  [51264/74412]\n",
      "loss: 3.283146  [57664/74412]\n",
      "loss: 3.376790  [64064/74412]\n",
      "loss: 3.606603  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 17.9%, Avg loss: 3.403129 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 3.522237  [   64/74412]\n",
      "loss: 3.555615  [ 6464/74412]\n",
      "loss: 3.541740  [12864/74412]\n",
      "loss: 3.343716  [19264/74412]\n",
      "loss: 3.485089  [25664/74412]\n",
      "loss: 3.373543  [32064/74412]\n",
      "loss: 3.470415  [38464/74412]\n",
      "loss: 3.250307  [44864/74412]\n",
      "loss: 3.353042  [51264/74412]\n",
      "loss: 3.264491  [57664/74412]\n",
      "loss: 3.352112  [64064/74412]\n",
      "loss: 3.585598  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 18.3%, Avg loss: 3.382803 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 3.506767  [   64/74412]\n",
      "loss: 3.539723  [ 6464/74412]\n",
      "loss: 3.523411  [12864/74412]\n",
      "loss: 3.320235  [19264/74412]\n",
      "loss: 3.469438  [25664/74412]\n",
      "loss: 3.353309  [32064/74412]\n",
      "loss: 3.447778  [38464/74412]\n",
      "loss: 3.229951  [44864/74412]\n",
      "loss: 3.328227  [51264/74412]\n",
      "loss: 3.246243  [57664/74412]\n",
      "loss: 3.327115  [64064/74412]\n",
      "loss: 3.566459  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 18.6%, Avg loss: 3.363383 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 3.493284  [   64/74412]\n",
      "loss: 3.523892  [ 6464/74412]\n",
      "loss: 3.504443  [12864/74412]\n",
      "loss: 3.297812  [19264/74412]\n",
      "loss: 3.453332  [25664/74412]\n",
      "loss: 3.335002  [32064/74412]\n",
      "loss: 3.425192  [38464/74412]\n",
      "loss: 3.210394  [44864/74412]\n",
      "loss: 3.304921  [51264/74412]\n",
      "loss: 3.228051  [57664/74412]\n",
      "loss: 3.301484  [64064/74412]\n",
      "loss: 3.548656  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 18.8%, Avg loss: 3.344350 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 3.481255  [   64/74412]\n",
      "loss: 3.506713  [ 6464/74412]\n",
      "loss: 3.483575  [12864/74412]\n",
      "loss: 3.275435  [19264/74412]\n",
      "loss: 3.436367  [25664/74412]\n",
      "loss: 3.317685  [32064/74412]\n",
      "loss: 3.402622  [38464/74412]\n",
      "loss: 3.191067  [44864/74412]\n",
      "loss: 3.282043  [51264/74412]\n",
      "loss: 3.209054  [57664/74412]\n",
      "loss: 3.274911  [64064/74412]\n",
      "loss: 3.531141  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 19.1%, Avg loss: 3.325329 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 3.470573  [   64/74412]\n",
      "loss: 3.490007  [ 6464/74412]\n",
      "loss: 3.462247  [12864/74412]\n",
      "loss: 3.253572  [19264/74412]\n",
      "loss: 3.418545  [25664/74412]\n",
      "loss: 3.301717  [32064/74412]\n",
      "loss: 3.379449  [38464/74412]\n",
      "loss: 3.172196  [44864/74412]\n",
      "loss: 3.260290  [51264/74412]\n",
      "loss: 3.189181  [57664/74412]\n",
      "loss: 3.247911  [64064/74412]\n",
      "loss: 3.513836  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 19.4%, Avg loss: 3.306539 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 3.461617  [   64/74412]\n",
      "loss: 3.472480  [ 6464/74412]\n",
      "loss: 3.439237  [12864/74412]\n",
      "loss: 3.231705  [19264/74412]\n",
      "loss: 3.400048  [25664/74412]\n",
      "loss: 3.286888  [32064/74412]\n",
      "loss: 3.356000  [38464/74412]\n",
      "loss: 3.153322  [44864/74412]\n",
      "loss: 3.239002  [51264/74412]\n",
      "loss: 3.168163  [57664/74412]\n",
      "loss: 3.219795  [64064/74412]\n",
      "loss: 3.496962  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 19.6%, Avg loss: 3.287444 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 3.453089  [   64/74412]\n",
      "loss: 3.454434  [ 6464/74412]\n",
      "loss: 3.415679  [12864/74412]\n",
      "loss: 3.209651  [19264/74412]\n",
      "loss: 3.380263  [25664/74412]\n",
      "loss: 3.272179  [32064/74412]\n",
      "loss: 3.332071  [38464/74412]\n",
      "loss: 3.133502  [44864/74412]\n",
      "loss: 3.217583  [51264/74412]\n",
      "loss: 3.145789  [57664/74412]\n",
      "loss: 3.190167  [64064/74412]\n",
      "loss: 3.480051  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 20.0%, Avg loss: 3.268076 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 3.445222  [   64/74412]\n",
      "loss: 3.436278  [ 6464/74412]\n",
      "loss: 3.391029  [12864/74412]\n",
      "loss: 3.187466  [19264/74412]\n",
      "loss: 3.359969  [25664/74412]\n",
      "loss: 3.257041  [32064/74412]\n",
      "loss: 3.308014  [38464/74412]\n",
      "loss: 3.113178  [44864/74412]\n",
      "loss: 3.195907  [51264/74412]\n",
      "loss: 3.122413  [57664/74412]\n",
      "loss: 3.158840  [64064/74412]\n",
      "loss: 3.462925  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 20.6%, Avg loss: 3.248518 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 3.437652  [   64/74412]\n",
      "loss: 3.417519  [ 6464/74412]\n",
      "loss: 3.365505  [12864/74412]\n",
      "loss: 3.165643  [19264/74412]\n",
      "loss: 3.338669  [25664/74412]\n",
      "loss: 3.241960  [32064/74412]\n",
      "loss: 3.283098  [38464/74412]\n",
      "loss: 3.092093  [44864/74412]\n",
      "loss: 3.173023  [51264/74412]\n",
      "loss: 3.097818  [57664/74412]\n",
      "loss: 3.126473  [64064/74412]\n",
      "loss: 3.445669  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 21.2%, Avg loss: 3.228404 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 3.430008  [   64/74412]\n",
      "loss: 3.398344  [ 6464/74412]\n",
      "loss: 3.339392  [12864/74412]\n",
      "loss: 3.143661  [19264/74412]\n",
      "loss: 3.316742  [25664/74412]\n",
      "loss: 3.225906  [32064/74412]\n",
      "loss: 3.258071  [38464/74412]\n",
      "loss: 3.070728  [44864/74412]\n",
      "loss: 3.150040  [51264/74412]\n",
      "loss: 3.072417  [57664/74412]\n",
      "loss: 3.092980  [64064/74412]\n",
      "loss: 3.427666  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 21.5%, Avg loss: 3.208310 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 3.422524  [   64/74412]\n",
      "loss: 3.378015  [ 6464/74412]\n",
      "loss: 3.313214  [12864/74412]\n",
      "loss: 3.122072  [19264/74412]\n",
      "loss: 3.294559  [25664/74412]\n",
      "loss: 3.209394  [32064/74412]\n",
      "loss: 3.233703  [38464/74412]\n",
      "loss: 3.049700  [44864/74412]\n",
      "loss: 3.126826  [51264/74412]\n",
      "loss: 3.046705  [57664/74412]\n",
      "loss: 3.059547  [64064/74412]\n",
      "loss: 3.409145  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 21.7%, Avg loss: 3.187688 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 3.414367  [   64/74412]\n",
      "loss: 3.356355  [ 6464/74412]\n",
      "loss: 3.287027  [12864/74412]\n",
      "loss: 3.101058  [19264/74412]\n",
      "loss: 3.271858  [25664/74412]\n",
      "loss: 3.192647  [32064/74412]\n",
      "loss: 3.210132  [38464/74412]\n",
      "loss: 3.029000  [44864/74412]\n",
      "loss: 3.103257  [51264/74412]\n",
      "loss: 3.020904  [57664/74412]\n",
      "loss: 3.026479  [64064/74412]\n",
      "loss: 3.389899  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 22.0%, Avg loss: 3.167202 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 3.406052  [   64/74412]\n",
      "loss: 3.333905  [ 6464/74412]\n",
      "loss: 3.261468  [12864/74412]\n",
      "loss: 3.080740  [19264/74412]\n",
      "loss: 3.249130  [25664/74412]\n",
      "loss: 3.176704  [32064/74412]\n",
      "loss: 3.187706  [38464/74412]\n",
      "loss: 3.008363  [44864/74412]\n",
      "loss: 3.080439  [51264/74412]\n",
      "loss: 2.995483  [57664/74412]\n",
      "loss: 2.994903  [64064/74412]\n",
      "loss: 3.369835  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 22.2%, Avg loss: 3.146914 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 3.396842  [   64/74412]\n",
      "loss: 3.310492  [ 6464/74412]\n",
      "loss: 3.236812  [12864/74412]\n",
      "loss: 3.060827  [19264/74412]\n",
      "loss: 3.226831  [25664/74412]\n",
      "loss: 3.160244  [32064/74412]\n",
      "loss: 3.166009  [38464/74412]\n",
      "loss: 2.988859  [44864/74412]\n",
      "loss: 3.058382  [51264/74412]\n",
      "loss: 2.971275  [57664/74412]\n",
      "loss: 2.965843  [64064/74412]\n",
      "loss: 3.349311  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 22.4%, Avg loss: 3.126811 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 3.386414  [   64/74412]\n",
      "loss: 3.286738  [ 6464/74412]\n",
      "loss: 3.212974  [12864/74412]\n",
      "loss: 3.041715  [19264/74412]\n",
      "loss: 3.205043  [25664/74412]\n",
      "loss: 3.143927  [32064/74412]\n",
      "loss: 3.145378  [38464/74412]\n",
      "loss: 2.970289  [44864/74412]\n",
      "loss: 3.036813  [51264/74412]\n",
      "loss: 2.947932  [57664/74412]\n",
      "loss: 2.939377  [64064/74412]\n",
      "loss: 3.328850  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 22.5%, Avg loss: 3.107872 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 3.376121  [   64/74412]\n",
      "loss: 3.263199  [ 6464/74412]\n",
      "loss: 3.190103  [12864/74412]\n",
      "loss: 3.023482  [19264/74412]\n",
      "loss: 3.184186  [25664/74412]\n",
      "loss: 3.128706  [32064/74412]\n",
      "loss: 3.126190  [38464/74412]\n",
      "loss: 2.953271  [44864/74412]\n",
      "loss: 3.017531  [51264/74412]\n",
      "loss: 2.925771  [57664/74412]\n",
      "loss: 2.916390  [64064/74412]\n",
      "loss: 3.308238  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 22.8%, Avg loss: 3.089636 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 3.365321  [   64/74412]\n",
      "loss: 3.240405  [ 6464/74412]\n",
      "loss: 3.168586  [12864/74412]\n",
      "loss: 3.005638  [19264/74412]\n",
      "loss: 3.163901  [25664/74412]\n",
      "loss: 3.113497  [32064/74412]\n",
      "loss: 3.108012  [38464/74412]\n",
      "loss: 2.937303  [44864/74412]\n",
      "loss: 2.998907  [51264/74412]\n",
      "loss: 2.904934  [57664/74412]\n",
      "loss: 2.896473  [64064/74412]\n",
      "loss: 3.287891  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 23.0%, Avg loss: 3.072257 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 3.354243  [   64/74412]\n",
      "loss: 3.217602  [ 6464/74412]\n",
      "loss: 3.147596  [12864/74412]\n",
      "loss: 2.988333  [19264/74412]\n",
      "loss: 3.144281  [25664/74412]\n",
      "loss: 3.098654  [32064/74412]\n",
      "loss: 3.090739  [38464/74412]\n",
      "loss: 2.921905  [44864/74412]\n",
      "loss: 2.981493  [51264/74412]\n",
      "loss: 2.885182  [57664/74412]\n",
      "loss: 2.878305  [64064/74412]\n",
      "loss: 3.267594  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 23.3%, Avg loss: 3.055645 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 3.342476  [   64/74412]\n",
      "loss: 3.195243  [ 6464/74412]\n",
      "loss: 3.127669  [12864/74412]\n",
      "loss: 2.971571  [19264/74412]\n",
      "loss: 3.124815  [25664/74412]\n",
      "loss: 3.084766  [32064/74412]\n",
      "loss: 3.074364  [38464/74412]\n",
      "loss: 2.906735  [44864/74412]\n",
      "loss: 2.965451  [51264/74412]\n",
      "loss: 2.866661  [57664/74412]\n",
      "loss: 2.861810  [64064/74412]\n",
      "loss: 3.248451  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 23.4%, Avg loss: 3.039517 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 3.329891  [   64/74412]\n",
      "loss: 3.173751  [ 6464/74412]\n",
      "loss: 3.107775  [12864/74412]\n",
      "loss: 2.954548  [19264/74412]\n",
      "loss: 3.105343  [25664/74412]\n",
      "loss: 3.070908  [32064/74412]\n",
      "loss: 3.059009  [38464/74412]\n",
      "loss: 2.892105  [44864/74412]\n",
      "loss: 2.949930  [51264/74412]\n",
      "loss: 2.848483  [57664/74412]\n",
      "loss: 2.846543  [64064/74412]\n",
      "loss: 3.228458  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 23.7%, Avg loss: 3.024535 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 3.317597  [   64/74412]\n",
      "loss: 3.152879  [ 6464/74412]\n",
      "loss: 3.088900  [12864/74412]\n",
      "loss: 2.938099  [19264/74412]\n",
      "loss: 3.086145  [25664/74412]\n",
      "loss: 3.057434  [32064/74412]\n",
      "loss: 3.043376  [38464/74412]\n",
      "loss: 2.877824  [44864/74412]\n",
      "loss: 2.935058  [51264/74412]\n",
      "loss: 2.830713  [57664/74412]\n",
      "loss: 2.832441  [64064/74412]\n",
      "loss: 3.208920  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 24.0%, Avg loss: 3.009546 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 3.305122  [   64/74412]\n",
      "loss: 3.132808  [ 6464/74412]\n",
      "loss: 3.070124  [12864/74412]\n",
      "loss: 2.921294  [19264/74412]\n",
      "loss: 3.066830  [25664/74412]\n",
      "loss: 3.043543  [32064/74412]\n",
      "loss: 3.028403  [38464/74412]\n",
      "loss: 2.863454  [44864/74412]\n",
      "loss: 2.920545  [51264/74412]\n",
      "loss: 2.812297  [57664/74412]\n",
      "loss: 2.819211  [64064/74412]\n",
      "loss: 3.189912  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 24.2%, Avg loss: 2.995327 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 3.292587  [   64/74412]\n",
      "loss: 3.113276  [ 6464/74412]\n",
      "loss: 3.051398  [12864/74412]\n",
      "loss: 2.904743  [19264/74412]\n",
      "loss: 3.047743  [25664/74412]\n",
      "loss: 3.029468  [32064/74412]\n",
      "loss: 3.013462  [38464/74412]\n",
      "loss: 2.848550  [44864/74412]\n",
      "loss: 2.907294  [51264/74412]\n",
      "loss: 2.794722  [57664/74412]\n",
      "loss: 2.806999  [64064/74412]\n",
      "loss: 3.170820  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 24.5%, Avg loss: 2.981842 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 3.279830  [   64/74412]\n",
      "loss: 3.093772  [ 6464/74412]\n",
      "loss: 3.032815  [12864/74412]\n",
      "loss: 2.888190  [19264/74412]\n",
      "loss: 3.028272  [25664/74412]\n",
      "loss: 3.015797  [32064/74412]\n",
      "loss: 2.998749  [38464/74412]\n",
      "loss: 2.833385  [44864/74412]\n",
      "loss: 2.894562  [51264/74412]\n",
      "loss: 2.776735  [57664/74412]\n",
      "loss: 2.795575  [64064/74412]\n",
      "loss: 3.151636  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 24.7%, Avg loss: 2.968026 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 3.266382  [   64/74412]\n",
      "loss: 3.074779  [ 6464/74412]\n",
      "loss: 3.014285  [12864/74412]\n",
      "loss: 2.872180  [19264/74412]\n",
      "loss: 3.008941  [25664/74412]\n",
      "loss: 3.002376  [32064/74412]\n",
      "loss: 2.984185  [38464/74412]\n",
      "loss: 2.818388  [44864/74412]\n",
      "loss: 2.881867  [51264/74412]\n",
      "loss: 2.758282  [57664/74412]\n",
      "loss: 2.784451  [64064/74412]\n",
      "loss: 3.132377  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 25.0%, Avg loss: 2.954080 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 3.252104  [   64/74412]\n",
      "loss: 3.056175  [ 6464/74412]\n",
      "loss: 2.995895  [12864/74412]\n",
      "loss: 2.856239  [19264/74412]\n",
      "loss: 2.989654  [25664/74412]\n",
      "loss: 2.988852  [32064/74412]\n",
      "loss: 2.969811  [38464/74412]\n",
      "loss: 2.803177  [44864/74412]\n",
      "loss: 2.869537  [51264/74412]\n",
      "loss: 2.740122  [57664/74412]\n",
      "loss: 2.773475  [64064/74412]\n",
      "loss: 3.113057  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 25.3%, Avg loss: 2.940690 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 3.237717  [   64/74412]\n",
      "loss: 3.037611  [ 6464/74412]\n",
      "loss: 2.977625  [12864/74412]\n",
      "loss: 2.840099  [19264/74412]\n",
      "loss: 2.970326  [25664/74412]\n",
      "loss: 2.975200  [32064/74412]\n",
      "loss: 2.955742  [38464/74412]\n",
      "loss: 2.787531  [44864/74412]\n",
      "loss: 2.857733  [51264/74412]\n",
      "loss: 2.721039  [57664/74412]\n",
      "loss: 2.762658  [64064/74412]\n",
      "loss: 3.093303  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 25.6%, Avg loss: 2.927134 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 3.223049  [   64/74412]\n",
      "loss: 3.020111  [ 6464/74412]\n",
      "loss: 2.959545  [12864/74412]\n",
      "loss: 2.824363  [19264/74412]\n",
      "loss: 2.951066  [25664/74412]\n",
      "loss: 2.961949  [32064/74412]\n",
      "loss: 2.941545  [38464/74412]\n",
      "loss: 2.771563  [44864/74412]\n",
      "loss: 2.845870  [51264/74412]\n",
      "loss: 2.701346  [57664/74412]\n",
      "loss: 2.752153  [64064/74412]\n",
      "loss: 3.073054  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 26.0%, Avg loss: 2.913334 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 3.207475  [   64/74412]\n",
      "loss: 3.002810  [ 6464/74412]\n",
      "loss: 2.941401  [12864/74412]\n",
      "loss: 2.808415  [19264/74412]\n",
      "loss: 2.932087  [25664/74412]\n",
      "loss: 2.948089  [32064/74412]\n",
      "loss: 2.927691  [38464/74412]\n",
      "loss: 2.754976  [44864/74412]\n",
      "loss: 2.834036  [51264/74412]\n",
      "loss: 2.681511  [57664/74412]\n",
      "loss: 2.741956  [64064/74412]\n",
      "loss: 3.052456  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 26.3%, Avg loss: 2.899883 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 3.192159  [   64/74412]\n",
      "loss: 2.985746  [ 6464/74412]\n",
      "loss: 2.923568  [12864/74412]\n",
      "loss: 2.792715  [19264/74412]\n",
      "loss: 2.913434  [25664/74412]\n",
      "loss: 2.934640  [32064/74412]\n",
      "loss: 2.914207  [38464/74412]\n",
      "loss: 2.738414  [44864/74412]\n",
      "loss: 2.823145  [51264/74412]\n",
      "loss: 2.660933  [57664/74412]\n",
      "loss: 2.731924  [64064/74412]\n",
      "loss: 3.031645  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 26.7%, Avg loss: 2.886734 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 3.177131  [   64/74412]\n",
      "loss: 2.968992  [ 6464/74412]\n",
      "loss: 2.905601  [12864/74412]\n",
      "loss: 2.776939  [19264/74412]\n",
      "loss: 2.894976  [25664/74412]\n",
      "loss: 2.921896  [32064/74412]\n",
      "loss: 2.900816  [38464/74412]\n",
      "loss: 2.721342  [44864/74412]\n",
      "loss: 2.812301  [51264/74412]\n",
      "loss: 2.640635  [57664/74412]\n",
      "loss: 2.721914  [64064/74412]\n",
      "loss: 3.010866  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 26.9%, Avg loss: 2.873201 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 3.161416  [   64/74412]\n",
      "loss: 2.953068  [ 6464/74412]\n",
      "loss: 2.887837  [12864/74412]\n",
      "loss: 2.761274  [19264/74412]\n",
      "loss: 2.876920  [25664/74412]\n",
      "loss: 2.908844  [32064/74412]\n",
      "loss: 2.887654  [38464/74412]\n",
      "loss: 2.703889  [44864/74412]\n",
      "loss: 2.801552  [51264/74412]\n",
      "loss: 2.620223  [57664/74412]\n",
      "loss: 2.711635  [64064/74412]\n",
      "loss: 2.988964  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 27.2%, Avg loss: 2.859744 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 3.145702  [   64/74412]\n",
      "loss: 2.937789  [ 6464/74412]\n",
      "loss: 2.869463  [12864/74412]\n",
      "loss: 2.745564  [19264/74412]\n",
      "loss: 2.859456  [25664/74412]\n",
      "loss: 2.896420  [32064/74412]\n",
      "loss: 2.874593  [38464/74412]\n",
      "loss: 2.686451  [44864/74412]\n",
      "loss: 2.790694  [51264/74412]\n",
      "loss: 2.599712  [57664/74412]\n",
      "loss: 2.701452  [64064/74412]\n",
      "loss: 2.967212  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 27.5%, Avg loss: 2.846485 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 3.130057  [   64/74412]\n",
      "loss: 2.922918  [ 6464/74412]\n",
      "loss: 2.851513  [12864/74412]\n",
      "loss: 2.729752  [19264/74412]\n",
      "loss: 2.842342  [25664/74412]\n",
      "loss: 2.884753  [32064/74412]\n",
      "loss: 2.861812  [38464/74412]\n",
      "loss: 2.668754  [44864/74412]\n",
      "loss: 2.779361  [51264/74412]\n",
      "loss: 2.578874  [57664/74412]\n",
      "loss: 2.691058  [64064/74412]\n",
      "loss: 2.945723  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 27.8%, Avg loss: 2.833272 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 3.113919  [   64/74412]\n",
      "loss: 2.908723  [ 6464/74412]\n",
      "loss: 2.833315  [12864/74412]\n",
      "loss: 2.713685  [19264/74412]\n",
      "loss: 2.825559  [25664/74412]\n",
      "loss: 2.872965  [32064/74412]\n",
      "loss: 2.848456  [38464/74412]\n",
      "loss: 2.650905  [44864/74412]\n",
      "loss: 2.768598  [51264/74412]\n",
      "loss: 2.558136  [57664/74412]\n",
      "loss: 2.680283  [64064/74412]\n",
      "loss: 2.923578  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 28.1%, Avg loss: 2.819715 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 3.097310  [   64/74412]\n",
      "loss: 2.894317  [ 6464/74412]\n",
      "loss: 2.814990  [12864/74412]\n",
      "loss: 2.697356  [19264/74412]\n",
      "loss: 2.809238  [25664/74412]\n",
      "loss: 2.861533  [32064/74412]\n",
      "loss: 2.835157  [38464/74412]\n",
      "loss: 2.633233  [44864/74412]\n",
      "loss: 2.757712  [51264/74412]\n",
      "loss: 2.537004  [57664/74412]\n",
      "loss: 2.669232  [64064/74412]\n",
      "loss: 2.900939  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 28.3%, Avg loss: 2.806250 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 3.080913  [   64/74412]\n",
      "loss: 2.880218  [ 6464/74412]\n",
      "loss: 2.796067  [12864/74412]\n",
      "loss: 2.680756  [19264/74412]\n",
      "loss: 2.793605  [25664/74412]\n",
      "loss: 2.849919  [32064/74412]\n",
      "loss: 2.821908  [38464/74412]\n",
      "loss: 2.615268  [44864/74412]\n",
      "loss: 2.746324  [51264/74412]\n",
      "loss: 2.516029  [57664/74412]\n",
      "loss: 2.657857  [64064/74412]\n",
      "loss: 2.877545  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 2.792527 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 3.063944  [   64/74412]\n",
      "loss: 2.866084  [ 6464/74412]\n",
      "loss: 2.777064  [12864/74412]\n",
      "loss: 2.664219  [19264/74412]\n",
      "loss: 2.778400  [25664/74412]\n",
      "loss: 2.838758  [32064/74412]\n",
      "loss: 2.808676  [38464/74412]\n",
      "loss: 2.597501  [44864/74412]\n",
      "loss: 2.734424  [51264/74412]\n",
      "loss: 2.495101  [57664/74412]\n",
      "loss: 2.645810  [64064/74412]\n",
      "loss: 2.855280  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 28.9%, Avg loss: 2.778572 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 3.046645  [   64/74412]\n",
      "loss: 2.852381  [ 6464/74412]\n",
      "loss: 2.757550  [12864/74412]\n",
      "loss: 2.647204  [19264/74412]\n",
      "loss: 2.763855  [25664/74412]\n",
      "loss: 2.828035  [32064/74412]\n",
      "loss: 2.795410  [38464/74412]\n",
      "loss: 2.579290  [44864/74412]\n",
      "loss: 2.722244  [51264/74412]\n",
      "loss: 2.473630  [57664/74412]\n",
      "loss: 2.634038  [64064/74412]\n",
      "loss: 2.832758  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 29.2%, Avg loss: 2.765036 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 3.029634  [   64/74412]\n",
      "loss: 2.839160  [ 6464/74412]\n",
      "loss: 2.737765  [12864/74412]\n",
      "loss: 2.630264  [19264/74412]\n",
      "loss: 2.749843  [25664/74412]\n",
      "loss: 2.817591  [32064/74412]\n",
      "loss: 2.782177  [38464/74412]\n",
      "loss: 2.561656  [44864/74412]\n",
      "loss: 2.710908  [51264/74412]\n",
      "loss: 2.452500  [57664/74412]\n",
      "loss: 2.621475  [64064/74412]\n",
      "loss: 2.810620  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 29.5%, Avg loss: 2.751644 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 3.012717  [   64/74412]\n",
      "loss: 2.826108  [ 6464/74412]\n",
      "loss: 2.718022  [12864/74412]\n",
      "loss: 2.613589  [19264/74412]\n",
      "loss: 2.736366  [25664/74412]\n",
      "loss: 2.806874  [32064/74412]\n",
      "loss: 2.768636  [38464/74412]\n",
      "loss: 2.543758  [44864/74412]\n",
      "loss: 2.698204  [51264/74412]\n",
      "loss: 2.431325  [57664/74412]\n",
      "loss: 2.608637  [64064/74412]\n",
      "loss: 2.788237  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 29.8%, Avg loss: 2.738009 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 2.995920  [   64/74412]\n",
      "loss: 2.813537  [ 6464/74412]\n",
      "loss: 2.698009  [12864/74412]\n",
      "loss: 2.597047  [19264/74412]\n",
      "loss: 2.723344  [25664/74412]\n",
      "loss: 2.796840  [32064/74412]\n",
      "loss: 2.755514  [38464/74412]\n",
      "loss: 2.526850  [44864/74412]\n",
      "loss: 2.685831  [51264/74412]\n",
      "loss: 2.411043  [57664/74412]\n",
      "loss: 2.596022  [64064/74412]\n",
      "loss: 2.767066  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 30.0%, Avg loss: 2.724413 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 2.979263  [   64/74412]\n",
      "loss: 2.801359  [ 6464/74412]\n",
      "loss: 2.678159  [12864/74412]\n",
      "loss: 2.580687  [19264/74412]\n",
      "loss: 2.710843  [25664/74412]\n",
      "loss: 2.786511  [32064/74412]\n",
      "loss: 2.743033  [38464/74412]\n",
      "loss: 2.510619  [44864/74412]\n",
      "loss: 2.673318  [51264/74412]\n",
      "loss: 2.391038  [57664/74412]\n",
      "loss: 2.583619  [64064/74412]\n",
      "loss: 2.746559  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.711072 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 2.962812  [   64/74412]\n",
      "loss: 2.789630  [ 6464/74412]\n",
      "loss: 2.658395  [12864/74412]\n",
      "loss: 2.565583  [19264/74412]\n",
      "loss: 2.699084  [25664/74412]\n",
      "loss: 2.776095  [32064/74412]\n",
      "loss: 2.731221  [38464/74412]\n",
      "loss: 2.495353  [44864/74412]\n",
      "loss: 2.660654  [51264/74412]\n",
      "loss: 2.372440  [57664/74412]\n",
      "loss: 2.571989  [64064/74412]\n",
      "loss: 2.726839  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 30.5%, Avg loss: 2.698479 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 2.947591  [   64/74412]\n",
      "loss: 2.778362  [ 6464/74412]\n",
      "loss: 2.639030  [12864/74412]\n",
      "loss: 2.550934  [19264/74412]\n",
      "loss: 2.688174  [25664/74412]\n",
      "loss: 2.765326  [32064/74412]\n",
      "loss: 2.719182  [38464/74412]\n",
      "loss: 2.480852  [44864/74412]\n",
      "loss: 2.648363  [51264/74412]\n",
      "loss: 2.354494  [57664/74412]\n",
      "loss: 2.560796  [64064/74412]\n",
      "loss: 2.708315  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 30.7%, Avg loss: 2.686734 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 2.933169  [   64/74412]\n",
      "loss: 2.767291  [ 6464/74412]\n",
      "loss: 2.620386  [12864/74412]\n",
      "loss: 2.537576  [19264/74412]\n",
      "loss: 2.677820  [25664/74412]\n",
      "loss: 2.755129  [32064/74412]\n",
      "loss: 2.708218  [38464/74412]\n",
      "loss: 2.467579  [44864/74412]\n",
      "loss: 2.636814  [51264/74412]\n",
      "loss: 2.338411  [57664/74412]\n",
      "loss: 2.550087  [64064/74412]\n",
      "loss: 2.690678  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 30.9%, Avg loss: 2.675301 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 2.919674  [   64/74412]\n",
      "loss: 2.756360  [ 6464/74412]\n",
      "loss: 2.602543  [12864/74412]\n",
      "loss: 2.524718  [19264/74412]\n",
      "loss: 2.668048  [25664/74412]\n",
      "loss: 2.744379  [32064/74412]\n",
      "loss: 2.697474  [38464/74412]\n",
      "loss: 2.455392  [44864/74412]\n",
      "loss: 2.625177  [51264/74412]\n",
      "loss: 2.323360  [57664/74412]\n",
      "loss: 2.540059  [64064/74412]\n",
      "loss: 2.674692  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 31.1%, Avg loss: 2.664837 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 2.907352  [   64/74412]\n",
      "loss: 2.745144  [ 6464/74412]\n",
      "loss: 2.585055  [12864/74412]\n",
      "loss: 2.512408  [19264/74412]\n",
      "loss: 2.658704  [25664/74412]\n",
      "loss: 2.733472  [32064/74412]\n",
      "loss: 2.687648  [38464/74412]\n",
      "loss: 2.444309  [44864/74412]\n",
      "loss: 2.614570  [51264/74412]\n",
      "loss: 2.309403  [57664/74412]\n",
      "loss: 2.530225  [64064/74412]\n",
      "loss: 2.659508  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 31.2%, Avg loss: 2.654552 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 2.895853  [   64/74412]\n",
      "loss: 2.734127  [ 6464/74412]\n",
      "loss: 2.568735  [12864/74412]\n",
      "loss: 2.501149  [19264/74412]\n",
      "loss: 2.649668  [25664/74412]\n",
      "loss: 2.722193  [32064/74412]\n",
      "loss: 2.678258  [38464/74412]\n",
      "loss: 2.433581  [44864/74412]\n",
      "loss: 2.604241  [51264/74412]\n",
      "loss: 2.296375  [57664/74412]\n",
      "loss: 2.520938  [64064/74412]\n",
      "loss: 2.645956  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 31.3%, Avg loss: 2.645167 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 2.884134  [   64/74412]\n",
      "loss: 2.723228  [ 6464/74412]\n",
      "loss: 2.552947  [12864/74412]\n",
      "loss: 2.490553  [19264/74412]\n",
      "loss: 2.640893  [25664/74412]\n",
      "loss: 2.710436  [32064/74412]\n",
      "loss: 2.668371  [38464/74412]\n",
      "loss: 2.423132  [44864/74412]\n",
      "loss: 2.593904  [51264/74412]\n",
      "loss: 2.284042  [57664/74412]\n",
      "loss: 2.512057  [64064/74412]\n",
      "loss: 2.632921  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 31.5%, Avg loss: 2.635518 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 2.872234  [   64/74412]\n",
      "loss: 2.712076  [ 6464/74412]\n",
      "loss: 2.537921  [12864/74412]\n",
      "loss: 2.480231  [19264/74412]\n",
      "loss: 2.632694  [25664/74412]\n",
      "loss: 2.698232  [32064/74412]\n",
      "loss: 2.659432  [38464/74412]\n",
      "loss: 2.413818  [44864/74412]\n",
      "loss: 2.584137  [51264/74412]\n",
      "loss: 2.272667  [57664/74412]\n",
      "loss: 2.503271  [64064/74412]\n",
      "loss: 2.620638  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 31.7%, Avg loss: 2.626093 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 2.860680  [   64/74412]\n",
      "loss: 2.701014  [ 6464/74412]\n",
      "loss: 2.523876  [12864/74412]\n",
      "loss: 2.470091  [19264/74412]\n",
      "loss: 2.624372  [25664/74412]\n",
      "loss: 2.685819  [32064/74412]\n",
      "loss: 2.650201  [38464/74412]\n",
      "loss: 2.404885  [44864/74412]\n",
      "loss: 2.574711  [51264/74412]\n",
      "loss: 2.261484  [57664/74412]\n",
      "loss: 2.494956  [64064/74412]\n",
      "loss: 2.608687  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.617501 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 2.850667  [   64/74412]\n",
      "loss: 2.690470  [ 6464/74412]\n",
      "loss: 2.509625  [12864/74412]\n",
      "loss: 2.460249  [19264/74412]\n",
      "loss: 2.616139  [25664/74412]\n",
      "loss: 2.673313  [32064/74412]\n",
      "loss: 2.641173  [38464/74412]\n",
      "loss: 2.396143  [44864/74412]\n",
      "loss: 2.564740  [51264/74412]\n",
      "loss: 2.251035  [57664/74412]\n",
      "loss: 2.486881  [64064/74412]\n",
      "loss: 2.596937  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.608824 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 2.840178  [   64/74412]\n",
      "loss: 2.679890  [ 6464/74412]\n",
      "loss: 2.495995  [12864/74412]\n",
      "loss: 2.450930  [19264/74412]\n",
      "loss: 2.608202  [25664/74412]\n",
      "loss: 2.661052  [32064/74412]\n",
      "loss: 2.632541  [38464/74412]\n",
      "loss: 2.388032  [44864/74412]\n",
      "loss: 2.555605  [51264/74412]\n",
      "loss: 2.241199  [57664/74412]\n",
      "loss: 2.478893  [64064/74412]\n",
      "loss: 2.585953  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 2.600378 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 2.830059  [   64/74412]\n",
      "loss: 2.668979  [ 6464/74412]\n",
      "loss: 2.482917  [12864/74412]\n",
      "loss: 2.441863  [19264/74412]\n",
      "loss: 2.600329  [25664/74412]\n",
      "loss: 2.648723  [32064/74412]\n",
      "loss: 2.623702  [38464/74412]\n",
      "loss: 2.379625  [44864/74412]\n",
      "loss: 2.546504  [51264/74412]\n",
      "loss: 2.232116  [57664/74412]\n",
      "loss: 2.470857  [64064/74412]\n",
      "loss: 2.575304  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 32.3%, Avg loss: 2.591999 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 2.820013  [   64/74412]\n",
      "loss: 2.658127  [ 6464/74412]\n",
      "loss: 2.469952  [12864/74412]\n",
      "loss: 2.433235  [19264/74412]\n",
      "loss: 2.592789  [25664/74412]\n",
      "loss: 2.636883  [32064/74412]\n",
      "loss: 2.614581  [38464/74412]\n",
      "loss: 2.371823  [44864/74412]\n",
      "loss: 2.538473  [51264/74412]\n",
      "loss: 2.222500  [57664/74412]\n",
      "loss: 2.462968  [64064/74412]\n",
      "loss: 2.564827  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 32.5%, Avg loss: 2.583459 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 2.809620  [   64/74412]\n",
      "loss: 2.646901  [ 6464/74412]\n",
      "loss: 2.457195  [12864/74412]\n",
      "loss: 2.424133  [19264/74412]\n",
      "loss: 2.585231  [25664/74412]\n",
      "loss: 2.624595  [32064/74412]\n",
      "loss: 2.605921  [38464/74412]\n",
      "loss: 2.364506  [44864/74412]\n",
      "loss: 2.529343  [51264/74412]\n",
      "loss: 2.213540  [57664/74412]\n",
      "loss: 2.455359  [64064/74412]\n",
      "loss: 2.554590  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 2.575252 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 2.799973  [   64/74412]\n",
      "loss: 2.636147  [ 6464/74412]\n",
      "loss: 2.444533  [12864/74412]\n",
      "loss: 2.415307  [19264/74412]\n",
      "loss: 2.577762  [25664/74412]\n",
      "loss: 2.612043  [32064/74412]\n",
      "loss: 2.597158  [38464/74412]\n",
      "loss: 2.356795  [44864/74412]\n",
      "loss: 2.521231  [51264/74412]\n",
      "loss: 2.204195  [57664/74412]\n",
      "loss: 2.447750  [64064/74412]\n",
      "loss: 2.544879  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 32.9%, Avg loss: 2.567155 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 2.790309  [   64/74412]\n",
      "loss: 2.625594  [ 6464/74412]\n",
      "loss: 2.432356  [12864/74412]\n",
      "loss: 2.406531  [19264/74412]\n",
      "loss: 2.570426  [25664/74412]\n",
      "loss: 2.599818  [32064/74412]\n",
      "loss: 2.588548  [38464/74412]\n",
      "loss: 2.349463  [44864/74412]\n",
      "loss: 2.512771  [51264/74412]\n",
      "loss: 2.195506  [57664/74412]\n",
      "loss: 2.440265  [64064/74412]\n",
      "loss: 2.535440  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 2.558630 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 2.780374  [   64/74412]\n",
      "loss: 2.615714  [ 6464/74412]\n",
      "loss: 2.419961  [12864/74412]\n",
      "loss: 2.397727  [19264/74412]\n",
      "loss: 2.562951  [25664/74412]\n",
      "loss: 2.587799  [32064/74412]\n",
      "loss: 2.579678  [38464/74412]\n",
      "loss: 2.341828  [44864/74412]\n",
      "loss: 2.504405  [51264/74412]\n",
      "loss: 2.187000  [57664/74412]\n",
      "loss: 2.433065  [64064/74412]\n",
      "loss: 2.526381  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 2.550639 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 2.771381  [   64/74412]\n",
      "loss: 2.604834  [ 6464/74412]\n",
      "loss: 2.407763  [12864/74412]\n",
      "loss: 2.389477  [19264/74412]\n",
      "loss: 2.555514  [25664/74412]\n",
      "loss: 2.576556  [32064/74412]\n",
      "loss: 2.571003  [38464/74412]\n",
      "loss: 2.334651  [44864/74412]\n",
      "loss: 2.495808  [51264/74412]\n",
      "loss: 2.178180  [57664/74412]\n",
      "loss: 2.426240  [64064/74412]\n",
      "loss: 2.516857  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 2.542707 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 2.762413  [   64/74412]\n",
      "loss: 2.595074  [ 6464/74412]\n",
      "loss: 2.395340  [12864/74412]\n",
      "loss: 2.381021  [19264/74412]\n",
      "loss: 2.547945  [25664/74412]\n",
      "loss: 2.565526  [32064/74412]\n",
      "loss: 2.563081  [38464/74412]\n",
      "loss: 2.327472  [44864/74412]\n",
      "loss: 2.486983  [51264/74412]\n",
      "loss: 2.169463  [57664/74412]\n",
      "loss: 2.419222  [64064/74412]\n",
      "loss: 2.507773  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 2.535577 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 2.754299  [   64/74412]\n",
      "loss: 2.584701  [ 6464/74412]\n",
      "loss: 2.383155  [12864/74412]\n",
      "loss: 2.372603  [19264/74412]\n",
      "loss: 2.540719  [25664/74412]\n",
      "loss: 2.555093  [32064/74412]\n",
      "loss: 2.554656  [38464/74412]\n",
      "loss: 2.320089  [44864/74412]\n",
      "loss: 2.477643  [51264/74412]\n",
      "loss: 2.160561  [57664/74412]\n",
      "loss: 2.412294  [64064/74412]\n",
      "loss: 2.499212  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 34.5%, Avg loss: 2.527787 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 2.745342  [   64/74412]\n",
      "loss: 2.574347  [ 6464/74412]\n",
      "loss: 2.371408  [12864/74412]\n",
      "loss: 2.364012  [19264/74412]\n",
      "loss: 2.533559  [25664/74412]\n",
      "loss: 2.544897  [32064/74412]\n",
      "loss: 2.546189  [38464/74412]\n",
      "loss: 2.313170  [44864/74412]\n",
      "loss: 2.469007  [51264/74412]\n",
      "loss: 2.152311  [57664/74412]\n",
      "loss: 2.405994  [64064/74412]\n",
      "loss: 2.490349  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 2.520030 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 2.736847  [   64/74412]\n",
      "loss: 2.564997  [ 6464/74412]\n",
      "loss: 2.359320  [12864/74412]\n",
      "loss: 2.355999  [19264/74412]\n",
      "loss: 2.526229  [25664/74412]\n",
      "loss: 2.535315  [32064/74412]\n",
      "loss: 2.537508  [38464/74412]\n",
      "loss: 2.305969  [44864/74412]\n",
      "loss: 2.460223  [51264/74412]\n",
      "loss: 2.144139  [57664/74412]\n",
      "loss: 2.399908  [64064/74412]\n",
      "loss: 2.481268  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.512572 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 2.728690  [   64/74412]\n",
      "loss: 2.555890  [ 6464/74412]\n",
      "loss: 2.347696  [12864/74412]\n",
      "loss: 2.348065  [19264/74412]\n",
      "loss: 2.518862  [25664/74412]\n",
      "loss: 2.525318  [32064/74412]\n",
      "loss: 2.529063  [38464/74412]\n",
      "loss: 2.299009  [44864/74412]\n",
      "loss: 2.451071  [51264/74412]\n",
      "loss: 2.135808  [57664/74412]\n",
      "loss: 2.393870  [64064/74412]\n",
      "loss: 2.472725  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 35.3%, Avg loss: 2.505222 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 2.720413  [   64/74412]\n",
      "loss: 2.546532  [ 6464/74412]\n",
      "loss: 2.336385  [12864/74412]\n",
      "loss: 2.339672  [19264/74412]\n",
      "loss: 2.511753  [25664/74412]\n",
      "loss: 2.516418  [32064/74412]\n",
      "loss: 2.520554  [38464/74412]\n",
      "loss: 2.291982  [44864/74412]\n",
      "loss: 2.443085  [51264/74412]\n",
      "loss: 2.127878  [57664/74412]\n",
      "loss: 2.387819  [64064/74412]\n",
      "loss: 2.463956  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 35.5%, Avg loss: 2.497484 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 2.712709  [   64/74412]\n",
      "loss: 2.537877  [ 6464/74412]\n",
      "loss: 2.325137  [12864/74412]\n",
      "loss: 2.329880  [19264/74412]\n",
      "loss: 2.504274  [25664/74412]\n",
      "loss: 2.507776  [32064/74412]\n",
      "loss: 2.512346  [38464/74412]\n",
      "loss: 2.284909  [44864/74412]\n",
      "loss: 2.434294  [51264/74412]\n",
      "loss: 2.120135  [57664/74412]\n",
      "loss: 2.382033  [64064/74412]\n",
      "loss: 2.455271  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 2.490453 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 2.705327  [   64/74412]\n",
      "loss: 2.528997  [ 6464/74412]\n",
      "loss: 2.313686  [12864/74412]\n",
      "loss: 2.320737  [19264/74412]\n",
      "loss: 2.496669  [25664/74412]\n",
      "loss: 2.500042  [32064/74412]\n",
      "loss: 2.504522  [38464/74412]\n",
      "loss: 2.277840  [44864/74412]\n",
      "loss: 2.424770  [51264/74412]\n",
      "loss: 2.112131  [57664/74412]\n",
      "loss: 2.376596  [64064/74412]\n",
      "loss: 2.446309  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 35.9%, Avg loss: 2.483791 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 2.698770  [   64/74412]\n",
      "loss: 2.520620  [ 6464/74412]\n",
      "loss: 2.302543  [12864/74412]\n",
      "loss: 2.311471  [19264/74412]\n",
      "loss: 2.489108  [25664/74412]\n",
      "loss: 2.492610  [32064/74412]\n",
      "loss: 2.496011  [38464/74412]\n",
      "loss: 2.271546  [44864/74412]\n",
      "loss: 2.416275  [51264/74412]\n",
      "loss: 2.103712  [57664/74412]\n",
      "loss: 2.371305  [64064/74412]\n",
      "loss: 2.437425  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 2.476429 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 2.691617  [   64/74412]\n",
      "loss: 2.511702  [ 6464/74412]\n",
      "loss: 2.291646  [12864/74412]\n",
      "loss: 2.302350  [19264/74412]\n",
      "loss: 2.481612  [25664/74412]\n",
      "loss: 2.485941  [32064/74412]\n",
      "loss: 2.487995  [38464/74412]\n",
      "loss: 2.264740  [44864/74412]\n",
      "loss: 2.406966  [51264/74412]\n",
      "loss: 2.095596  [57664/74412]\n",
      "loss: 2.366039  [64064/74412]\n",
      "loss: 2.428237  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 36.2%, Avg loss: 2.469694 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 2.684785  [   64/74412]\n",
      "loss: 2.502759  [ 6464/74412]\n",
      "loss: 2.280724  [12864/74412]\n",
      "loss: 2.293122  [19264/74412]\n",
      "loss: 2.473997  [25664/74412]\n",
      "loss: 2.478782  [32064/74412]\n",
      "loss: 2.479584  [38464/74412]\n",
      "loss: 2.259116  [44864/74412]\n",
      "loss: 2.398207  [51264/74412]\n",
      "loss: 2.087757  [57664/74412]\n",
      "loss: 2.361092  [64064/74412]\n",
      "loss: 2.419564  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 36.4%, Avg loss: 2.462573 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 2.677618  [   64/74412]\n",
      "loss: 2.494089  [ 6464/74412]\n",
      "loss: 2.269863  [12864/74412]\n",
      "loss: 2.283768  [19264/74412]\n",
      "loss: 2.466321  [25664/74412]\n",
      "loss: 2.471911  [32064/74412]\n",
      "loss: 2.472484  [38464/74412]\n",
      "loss: 2.252138  [44864/74412]\n",
      "loss: 2.389071  [51264/74412]\n",
      "loss: 2.079109  [57664/74412]\n",
      "loss: 2.356297  [64064/74412]\n",
      "loss: 2.410387  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 36.5%, Avg loss: 2.455153 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 2.670827  [   64/74412]\n",
      "loss: 2.485033  [ 6464/74412]\n",
      "loss: 2.259391  [12864/74412]\n",
      "loss: 2.274267  [19264/74412]\n",
      "loss: 2.458421  [25664/74412]\n",
      "loss: 2.465452  [32064/74412]\n",
      "loss: 2.464934  [38464/74412]\n",
      "loss: 2.245098  [44864/74412]\n",
      "loss: 2.379505  [51264/74412]\n",
      "loss: 2.070755  [57664/74412]\n",
      "loss: 2.351186  [64064/74412]\n",
      "loss: 2.400892  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 2.448172 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 2.664745  [   64/74412]\n",
      "loss: 2.476617  [ 6464/74412]\n",
      "loss: 2.248913  [12864/74412]\n",
      "loss: 2.264242  [19264/74412]\n",
      "loss: 2.450251  [25664/74412]\n",
      "loss: 2.458992  [32064/74412]\n",
      "loss: 2.457681  [38464/74412]\n",
      "loss: 2.238417  [44864/74412]\n",
      "loss: 2.369868  [51264/74412]\n",
      "loss: 2.062372  [57664/74412]\n",
      "loss: 2.346189  [64064/74412]\n",
      "loss: 2.391391  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 36.8%, Avg loss: 2.441173 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 2.658144  [   64/74412]\n",
      "loss: 2.468207  [ 6464/74412]\n",
      "loss: 2.239372  [12864/74412]\n",
      "loss: 2.254876  [19264/74412]\n",
      "loss: 2.442236  [25664/74412]\n",
      "loss: 2.452954  [32064/74412]\n",
      "loss: 2.449211  [38464/74412]\n",
      "loss: 2.231215  [44864/74412]\n",
      "loss: 2.360587  [51264/74412]\n",
      "loss: 2.054275  [57664/74412]\n",
      "loss: 2.341228  [64064/74412]\n",
      "loss: 2.381409  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 36.9%, Avg loss: 2.433920 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 2.651741  [   64/74412]\n",
      "loss: 2.459373  [ 6464/74412]\n",
      "loss: 2.229594  [12864/74412]\n",
      "loss: 2.244864  [19264/74412]\n",
      "loss: 2.433935  [25664/74412]\n",
      "loss: 2.447339  [32064/74412]\n",
      "loss: 2.441611  [38464/74412]\n",
      "loss: 2.224609  [44864/74412]\n",
      "loss: 2.351182  [51264/74412]\n",
      "loss: 2.046144  [57664/74412]\n",
      "loss: 2.336330  [64064/74412]\n",
      "loss: 2.371401  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 37.1%, Avg loss: 2.426399 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 2.645637  [   64/74412]\n",
      "loss: 2.450749  [ 6464/74412]\n",
      "loss: 2.219929  [12864/74412]\n",
      "loss: 2.234566  [19264/74412]\n",
      "loss: 2.425267  [25664/74412]\n",
      "loss: 2.441912  [32064/74412]\n",
      "loss: 2.434682  [38464/74412]\n",
      "loss: 2.217475  [44864/74412]\n",
      "loss: 2.341329  [51264/74412]\n",
      "loss: 2.036834  [57664/74412]\n",
      "loss: 2.331089  [64064/74412]\n",
      "loss: 2.360551  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 37.3%, Avg loss: 2.419189 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 2.639132  [   64/74412]\n",
      "loss: 2.441720  [ 6464/74412]\n",
      "loss: 2.210599  [12864/74412]\n",
      "loss: 2.224708  [19264/74412]\n",
      "loss: 2.416609  [25664/74412]\n",
      "loss: 2.437034  [32064/74412]\n",
      "loss: 2.427776  [38464/74412]\n",
      "loss: 2.210774  [44864/74412]\n",
      "loss: 2.331642  [51264/74412]\n",
      "loss: 2.027774  [57664/74412]\n",
      "loss: 2.326012  [64064/74412]\n",
      "loss: 2.350341  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 2.411653 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 2.632649  [   64/74412]\n",
      "loss: 2.432960  [ 6464/74412]\n",
      "loss: 2.201974  [12864/74412]\n",
      "loss: 2.214519  [19264/74412]\n",
      "loss: 2.407976  [25664/74412]\n",
      "loss: 2.432427  [32064/74412]\n",
      "loss: 2.421100  [38464/74412]\n",
      "loss: 2.202392  [44864/74412]\n",
      "loss: 2.322331  [51264/74412]\n",
      "loss: 2.018567  [57664/74412]\n",
      "loss: 2.320325  [64064/74412]\n",
      "loss: 2.340135  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 2.404423 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 2.626985  [   64/74412]\n",
      "loss: 2.424418  [ 6464/74412]\n",
      "loss: 2.193186  [12864/74412]\n",
      "loss: 2.204213  [19264/74412]\n",
      "loss: 2.398957  [25664/74412]\n",
      "loss: 2.428279  [32064/74412]\n",
      "loss: 2.413531  [38464/74412]\n",
      "loss: 2.195164  [44864/74412]\n",
      "loss: 2.311955  [51264/74412]\n",
      "loss: 2.009264  [57664/74412]\n",
      "loss: 2.315116  [64064/74412]\n",
      "loss: 2.330161  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 2.396926 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 2.620796  [   64/74412]\n",
      "loss: 2.415613  [ 6464/74412]\n",
      "loss: 2.184578  [12864/74412]\n",
      "loss: 2.193934  [19264/74412]\n",
      "loss: 2.389568  [25664/74412]\n",
      "loss: 2.423337  [32064/74412]\n",
      "loss: 2.405915  [38464/74412]\n",
      "loss: 2.187596  [44864/74412]\n",
      "loss: 2.301933  [51264/74412]\n",
      "loss: 2.000583  [57664/74412]\n",
      "loss: 2.309727  [64064/74412]\n",
      "loss: 2.319745  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 2.389916 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 2.614980  [   64/74412]\n",
      "loss: 2.406666  [ 6464/74412]\n",
      "loss: 2.175826  [12864/74412]\n",
      "loss: 2.183133  [19264/74412]\n",
      "loss: 2.380491  [25664/74412]\n",
      "loss: 2.419396  [32064/74412]\n",
      "loss: 2.397963  [38464/74412]\n",
      "loss: 2.180224  [44864/74412]\n",
      "loss: 2.291823  [51264/74412]\n",
      "loss: 1.991444  [57664/74412]\n",
      "loss: 2.304030  [64064/74412]\n",
      "loss: 2.308674  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 38.3%, Avg loss: 2.382438 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 2.609028  [   64/74412]\n",
      "loss: 2.397751  [ 6464/74412]\n",
      "loss: 2.167688  [12864/74412]\n",
      "loss: 2.172282  [19264/74412]\n",
      "loss: 2.371008  [25664/74412]\n",
      "loss: 2.414972  [32064/74412]\n",
      "loss: 2.390092  [38464/74412]\n",
      "loss: 2.173106  [44864/74412]\n",
      "loss: 2.281909  [51264/74412]\n",
      "loss: 1.982482  [57664/74412]\n",
      "loss: 2.298353  [64064/74412]\n",
      "loss: 2.298223  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.374949 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 2.602292  [   64/74412]\n",
      "loss: 2.388907  [ 6464/74412]\n",
      "loss: 2.159887  [12864/74412]\n",
      "loss: 2.161089  [19264/74412]\n",
      "loss: 2.361187  [25664/74412]\n",
      "loss: 2.410563  [32064/74412]\n",
      "loss: 2.381853  [38464/74412]\n",
      "loss: 2.164617  [44864/74412]\n",
      "loss: 2.271718  [51264/74412]\n",
      "loss: 1.972619  [57664/74412]\n",
      "loss: 2.292969  [64064/74412]\n",
      "loss: 2.286612  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 2.366586 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 2.594624  [   64/74412]\n",
      "loss: 2.378201  [ 6464/74412]\n",
      "loss: 2.152316  [12864/74412]\n",
      "loss: 2.150266  [19264/74412]\n",
      "loss: 2.351371  [25664/74412]\n",
      "loss: 2.405418  [32064/74412]\n",
      "loss: 2.373671  [38464/74412]\n",
      "loss: 2.156116  [44864/74412]\n",
      "loss: 2.261378  [51264/74412]\n",
      "loss: 1.961254  [57664/74412]\n",
      "loss: 2.287292  [64064/74412]\n",
      "loss: 2.275259  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.358318 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 2.587097  [   64/74412]\n",
      "loss: 2.368292  [ 6464/74412]\n",
      "loss: 2.145589  [12864/74412]\n",
      "loss: 2.139729  [19264/74412]\n",
      "loss: 2.341690  [25664/74412]\n",
      "loss: 2.400037  [32064/74412]\n",
      "loss: 2.366065  [38464/74412]\n",
      "loss: 2.147854  [44864/74412]\n",
      "loss: 2.250858  [51264/74412]\n",
      "loss: 1.951837  [57664/74412]\n",
      "loss: 2.280898  [64064/74412]\n",
      "loss: 2.262698  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.350959 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 2.580940  [   64/74412]\n",
      "loss: 2.357335  [ 6464/74412]\n",
      "loss: 2.139220  [12864/74412]\n",
      "loss: 2.128363  [19264/74412]\n",
      "loss: 2.331503  [25664/74412]\n",
      "loss: 2.395084  [32064/74412]\n",
      "loss: 2.356862  [38464/74412]\n",
      "loss: 2.140232  [44864/74412]\n",
      "loss: 2.240650  [51264/74412]\n",
      "loss: 1.943046  [57664/74412]\n",
      "loss: 2.274817  [64064/74412]\n",
      "loss: 2.251369  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.343815 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 2.575626  [   64/74412]\n",
      "loss: 2.348174  [ 6464/74412]\n",
      "loss: 2.132018  [12864/74412]\n",
      "loss: 2.116918  [19264/74412]\n",
      "loss: 2.321354  [25664/74412]\n",
      "loss: 2.390760  [32064/74412]\n",
      "loss: 2.348588  [38464/74412]\n",
      "loss: 2.131826  [44864/74412]\n",
      "loss: 2.230360  [51264/74412]\n",
      "loss: 1.933817  [57664/74412]\n",
      "loss: 2.269019  [64064/74412]\n",
      "loss: 2.240149  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 2.336059 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 2.569320  [   64/74412]\n",
      "loss: 2.337537  [ 6464/74412]\n",
      "loss: 2.124184  [12864/74412]\n",
      "loss: 2.104811  [19264/74412]\n",
      "loss: 2.310571  [25664/74412]\n",
      "loss: 2.387007  [32064/74412]\n",
      "loss: 2.339521  [38464/74412]\n",
      "loss: 2.124475  [44864/74412]\n",
      "loss: 2.219466  [51264/74412]\n",
      "loss: 1.924823  [57664/74412]\n",
      "loss: 2.263197  [64064/74412]\n",
      "loss: 2.227997  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 2.328194 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 2.562938  [   64/74412]\n",
      "loss: 2.327056  [ 6464/74412]\n",
      "loss: 2.116223  [12864/74412]\n",
      "loss: 2.092769  [19264/74412]\n",
      "loss: 2.299972  [25664/74412]\n",
      "loss: 2.381916  [32064/74412]\n",
      "loss: 2.330808  [38464/74412]\n",
      "loss: 2.116325  [44864/74412]\n",
      "loss: 2.208508  [51264/74412]\n",
      "loss: 1.915172  [57664/74412]\n",
      "loss: 2.256371  [64064/74412]\n",
      "loss: 2.215834  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.320449 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 2.556587  [   64/74412]\n",
      "loss: 2.318560  [ 6464/74412]\n",
      "loss: 2.108617  [12864/74412]\n",
      "loss: 2.080929  [19264/74412]\n",
      "loss: 2.288996  [25664/74412]\n",
      "loss: 2.377728  [32064/74412]\n",
      "loss: 2.321992  [38464/74412]\n",
      "loss: 2.108075  [44864/74412]\n",
      "loss: 2.198008  [51264/74412]\n",
      "loss: 1.905097  [57664/74412]\n",
      "loss: 2.250741  [64064/74412]\n",
      "loss: 2.203969  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 40.5%, Avg loss: 2.311520 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 2.548955  [   64/74412]\n",
      "loss: 2.308291  [ 6464/74412]\n",
      "loss: 2.101084  [12864/74412]\n",
      "loss: 2.069307  [19264/74412]\n",
      "loss: 2.277666  [25664/74412]\n",
      "loss: 2.372772  [32064/74412]\n",
      "loss: 2.313581  [38464/74412]\n",
      "loss: 2.100606  [44864/74412]\n",
      "loss: 2.187252  [51264/74412]\n",
      "loss: 1.894780  [57664/74412]\n",
      "loss: 2.244532  [64064/74412]\n",
      "loss: 2.192291  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 40.7%, Avg loss: 2.302509 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 2.541522  [   64/74412]\n",
      "loss: 2.298623  [ 6464/74412]\n",
      "loss: 2.093767  [12864/74412]\n",
      "loss: 2.057784  [19264/74412]\n",
      "loss: 2.266107  [25664/74412]\n",
      "loss: 2.368075  [32064/74412]\n",
      "loss: 2.304715  [38464/74412]\n",
      "loss: 2.090323  [44864/74412]\n",
      "loss: 2.177173  [51264/74412]\n",
      "loss: 1.884385  [57664/74412]\n",
      "loss: 2.238286  [64064/74412]\n",
      "loss: 2.180346  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 40.9%, Avg loss: 2.294579 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 2.534184  [   64/74412]\n",
      "loss: 2.288189  [ 6464/74412]\n",
      "loss: 2.086131  [12864/74412]\n",
      "loss: 2.046803  [19264/74412]\n",
      "loss: 2.254309  [25664/74412]\n",
      "loss: 2.363854  [32064/74412]\n",
      "loss: 2.295266  [38464/74412]\n",
      "loss: 2.080455  [44864/74412]\n",
      "loss: 2.166908  [51264/74412]\n",
      "loss: 1.874398  [57664/74412]\n",
      "loss: 2.231826  [64064/74412]\n",
      "loss: 2.169006  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 2.285520 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 2.527288  [   64/74412]\n",
      "loss: 2.278104  [ 6464/74412]\n",
      "loss: 2.079085  [12864/74412]\n",
      "loss: 2.035166  [19264/74412]\n",
      "loss: 2.242894  [25664/74412]\n",
      "loss: 2.358447  [32064/74412]\n",
      "loss: 2.286339  [38464/74412]\n",
      "loss: 2.071225  [44864/74412]\n",
      "loss: 2.156149  [51264/74412]\n",
      "loss: 1.864353  [57664/74412]\n",
      "loss: 2.225028  [64064/74412]\n",
      "loss: 2.156944  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 2.276827 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 2.520163  [   64/74412]\n",
      "loss: 2.268429  [ 6464/74412]\n",
      "loss: 2.072087  [12864/74412]\n",
      "loss: 2.023883  [19264/74412]\n",
      "loss: 2.231156  [25664/74412]\n",
      "loss: 2.352588  [32064/74412]\n",
      "loss: 2.278156  [38464/74412]\n",
      "loss: 2.061136  [44864/74412]\n",
      "loss: 2.145910  [51264/74412]\n",
      "loss: 1.853334  [57664/74412]\n",
      "loss: 2.218966  [64064/74412]\n",
      "loss: 2.144999  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 2.268087 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 2.512990  [   64/74412]\n",
      "loss: 2.259296  [ 6464/74412]\n",
      "loss: 2.064963  [12864/74412]\n",
      "loss: 2.012844  [19264/74412]\n",
      "loss: 2.219579  [25664/74412]\n",
      "loss: 2.348184  [32064/74412]\n",
      "loss: 2.268797  [38464/74412]\n",
      "loss: 2.051540  [44864/74412]\n",
      "loss: 2.135945  [51264/74412]\n",
      "loss: 1.843508  [57664/74412]\n",
      "loss: 2.212268  [64064/74412]\n",
      "loss: 2.134144  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 41.7%, Avg loss: 2.259823 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 2.505936  [   64/74412]\n",
      "loss: 2.249778  [ 6464/74412]\n",
      "loss: 2.058266  [12864/74412]\n",
      "loss: 2.001683  [19264/74412]\n",
      "loss: 2.208484  [25664/74412]\n",
      "loss: 2.343608  [32064/74412]\n",
      "loss: 2.258080  [38464/74412]\n",
      "loss: 2.041330  [44864/74412]\n",
      "loss: 2.126149  [51264/74412]\n",
      "loss: 1.834125  [57664/74412]\n",
      "loss: 2.205852  [64064/74412]\n",
      "loss: 2.122670  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg loss: 2.252016 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 2.499110  [   64/74412]\n",
      "loss: 2.239230  [ 6464/74412]\n",
      "loss: 2.051285  [12864/74412]\n",
      "loss: 1.990936  [19264/74412]\n",
      "loss: 2.196621  [25664/74412]\n",
      "loss: 2.336961  [32064/74412]\n",
      "loss: 2.248667  [38464/74412]\n",
      "loss: 2.031051  [44864/74412]\n",
      "loss: 2.116867  [51264/74412]\n",
      "loss: 1.824159  [57664/74412]\n",
      "loss: 2.199132  [64064/74412]\n",
      "loss: 2.111630  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 42.1%, Avg loss: 2.243778 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 2.491885  [   64/74412]\n",
      "loss: 2.230245  [ 6464/74412]\n",
      "loss: 2.044536  [12864/74412]\n",
      "loss: 1.980369  [19264/74412]\n",
      "loss: 2.184983  [25664/74412]\n",
      "loss: 2.332908  [32064/74412]\n",
      "loss: 2.239538  [38464/74412]\n",
      "loss: 2.020990  [44864/74412]\n",
      "loss: 2.105938  [51264/74412]\n",
      "loss: 1.814128  [57664/74412]\n",
      "loss: 2.192136  [64064/74412]\n",
      "loss: 2.101502  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 42.4%, Avg loss: 2.234907 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 2.484255  [   64/74412]\n",
      "loss: 2.221296  [ 6464/74412]\n",
      "loss: 2.038224  [12864/74412]\n",
      "loss: 1.969837  [19264/74412]\n",
      "loss: 2.173324  [25664/74412]\n",
      "loss: 2.329492  [32064/74412]\n",
      "loss: 2.230043  [38464/74412]\n",
      "loss: 2.010821  [44864/74412]\n",
      "loss: 2.095547  [51264/74412]\n",
      "loss: 1.804434  [57664/74412]\n",
      "loss: 2.185240  [64064/74412]\n",
      "loss: 2.091085  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 42.7%, Avg loss: 2.226198 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 2.476591  [   64/74412]\n",
      "loss: 2.211917  [ 6464/74412]\n",
      "loss: 2.031593  [12864/74412]\n",
      "loss: 1.959232  [19264/74412]\n",
      "loss: 2.161738  [25664/74412]\n",
      "loss: 2.324755  [32064/74412]\n",
      "loss: 2.220598  [38464/74412]\n",
      "loss: 2.001524  [44864/74412]\n",
      "loss: 2.085438  [51264/74412]\n",
      "loss: 1.795365  [57664/74412]\n",
      "loss: 2.178485  [64064/74412]\n",
      "loss: 2.080040  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 2.217722 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 2.468807  [   64/74412]\n",
      "loss: 2.203070  [ 6464/74412]\n",
      "loss: 2.025305  [12864/74412]\n",
      "loss: 1.948842  [19264/74412]\n",
      "loss: 2.150672  [25664/74412]\n",
      "loss: 2.320447  [32064/74412]\n",
      "loss: 2.211572  [38464/74412]\n",
      "loss: 1.991363  [44864/74412]\n",
      "loss: 2.075405  [51264/74412]\n",
      "loss: 1.786114  [57664/74412]\n",
      "loss: 2.169813  [64064/74412]\n",
      "loss: 2.069504  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 2.208388 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 2.460446  [   64/74412]\n",
      "loss: 2.193549  [ 6464/74412]\n",
      "loss: 2.019030  [12864/74412]\n",
      "loss: 1.939013  [19264/74412]\n",
      "loss: 2.139706  [25664/74412]\n",
      "loss: 2.316183  [32064/74412]\n",
      "loss: 2.202534  [38464/74412]\n",
      "loss: 1.981192  [44864/74412]\n",
      "loss: 2.065973  [51264/74412]\n",
      "loss: 1.775322  [57664/74412]\n",
      "loss: 2.163653  [64064/74412]\n",
      "loss: 2.059161  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 2.199968 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 2.452801  [   64/74412]\n",
      "loss: 2.184319  [ 6464/74412]\n",
      "loss: 2.012570  [12864/74412]\n",
      "loss: 1.928810  [19264/74412]\n",
      "loss: 2.129316  [25664/74412]\n",
      "loss: 2.311359  [32064/74412]\n",
      "loss: 2.192198  [38464/74412]\n",
      "loss: 1.971012  [44864/74412]\n",
      "loss: 2.056123  [51264/74412]\n",
      "loss: 1.764732  [57664/74412]\n",
      "loss: 2.155079  [64064/74412]\n",
      "loss: 2.048888  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 2.191340 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 2.444788  [   64/74412]\n",
      "loss: 2.174795  [ 6464/74412]\n",
      "loss: 2.006081  [12864/74412]\n",
      "loss: 1.919091  [19264/74412]\n",
      "loss: 2.119271  [25664/74412]\n",
      "loss: 2.307138  [32064/74412]\n",
      "loss: 2.182195  [38464/74412]\n",
      "loss: 1.961189  [44864/74412]\n",
      "loss: 2.046152  [51264/74412]\n",
      "loss: 1.755256  [57664/74412]\n",
      "loss: 2.147378  [64064/74412]\n",
      "loss: 2.039123  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 44.0%, Avg loss: 2.182988 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 2.437063  [   64/74412]\n",
      "loss: 2.166697  [ 6464/74412]\n",
      "loss: 1.999348  [12864/74412]\n",
      "loss: 1.909276  [19264/74412]\n",
      "loss: 2.108791  [25664/74412]\n",
      "loss: 2.303380  [32064/74412]\n",
      "loss: 2.172071  [38464/74412]\n",
      "loss: 1.951262  [44864/74412]\n",
      "loss: 2.036524  [51264/74412]\n",
      "loss: 1.746143  [57664/74412]\n",
      "loss: 2.139679  [64064/74412]\n",
      "loss: 2.030088  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 2.174972 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 2.429474  [   64/74412]\n",
      "loss: 2.156769  [ 6464/74412]\n",
      "loss: 1.992621  [12864/74412]\n",
      "loss: 1.899579  [19264/74412]\n",
      "loss: 2.098362  [25664/74412]\n",
      "loss: 2.298620  [32064/74412]\n",
      "loss: 2.163245  [38464/74412]\n",
      "loss: 1.941440  [44864/74412]\n",
      "loss: 2.027246  [51264/74412]\n",
      "loss: 1.736398  [57664/74412]\n",
      "loss: 2.131949  [64064/74412]\n",
      "loss: 2.021359  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 2.166923 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 2.422031  [   64/74412]\n",
      "loss: 2.147790  [ 6464/74412]\n",
      "loss: 1.986613  [12864/74412]\n",
      "loss: 1.889917  [19264/74412]\n",
      "loss: 2.088001  [25664/74412]\n",
      "loss: 2.294950  [32064/74412]\n",
      "loss: 2.154495  [38464/74412]\n",
      "loss: 1.932189  [44864/74412]\n",
      "loss: 2.017624  [51264/74412]\n",
      "loss: 1.727482  [57664/74412]\n",
      "loss: 2.124801  [64064/74412]\n",
      "loss: 2.012813  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 44.6%, Avg loss: 2.158039 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 2.413359  [   64/74412]\n",
      "loss: 2.138686  [ 6464/74412]\n",
      "loss: 1.980434  [12864/74412]\n",
      "loss: 1.880683  [19264/74412]\n",
      "loss: 2.077833  [25664/74412]\n",
      "loss: 2.291055  [32064/74412]\n",
      "loss: 2.144636  [38464/74412]\n",
      "loss: 1.922633  [44864/74412]\n",
      "loss: 2.008340  [51264/74412]\n",
      "loss: 1.719359  [57664/74412]\n",
      "loss: 2.117193  [64064/74412]\n",
      "loss: 2.003777  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 44.9%, Avg loss: 2.149054 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 2.404804  [   64/74412]\n",
      "loss: 2.129675  [ 6464/74412]\n",
      "loss: 1.973469  [12864/74412]\n",
      "loss: 1.871788  [19264/74412]\n",
      "loss: 2.067705  [25664/74412]\n",
      "loss: 2.286850  [32064/74412]\n",
      "loss: 2.135094  [38464/74412]\n",
      "loss: 1.912880  [44864/74412]\n",
      "loss: 1.998366  [51264/74412]\n",
      "loss: 1.710599  [57664/74412]\n",
      "loss: 2.109417  [64064/74412]\n",
      "loss: 1.995494  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 2.142181 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 2.397752  [   64/74412]\n",
      "loss: 2.121470  [ 6464/74412]\n",
      "loss: 1.967533  [12864/74412]\n",
      "loss: 1.862652  [19264/74412]\n",
      "loss: 2.057687  [25664/74412]\n",
      "loss: 2.282969  [32064/74412]\n",
      "loss: 2.126687  [38464/74412]\n",
      "loss: 1.903793  [44864/74412]\n",
      "loss: 1.990062  [51264/74412]\n",
      "loss: 1.701904  [57664/74412]\n",
      "loss: 2.101570  [64064/74412]\n",
      "loss: 1.987025  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 45.2%, Avg loss: 2.133258 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 2.389182  [   64/74412]\n",
      "loss: 2.112439  [ 6464/74412]\n",
      "loss: 1.961357  [12864/74412]\n",
      "loss: 1.854136  [19264/74412]\n",
      "loss: 2.047659  [25664/74412]\n",
      "loss: 2.278356  [32064/74412]\n",
      "loss: 2.116915  [38464/74412]\n",
      "loss: 1.894460  [44864/74412]\n",
      "loss: 1.980734  [51264/74412]\n",
      "loss: 1.693842  [57664/74412]\n",
      "loss: 2.093122  [64064/74412]\n",
      "loss: 1.978252  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 2.125062 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 2.381076  [   64/74412]\n",
      "loss: 2.102623  [ 6464/74412]\n",
      "loss: 1.954804  [12864/74412]\n",
      "loss: 1.845665  [19264/74412]\n",
      "loss: 2.038187  [25664/74412]\n",
      "loss: 2.274503  [32064/74412]\n",
      "loss: 2.107196  [38464/74412]\n",
      "loss: 1.885545  [44864/74412]\n",
      "loss: 1.972018  [51264/74412]\n",
      "loss: 1.685887  [57664/74412]\n",
      "loss: 2.085638  [64064/74412]\n",
      "loss: 1.970265  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 2.117285 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 2.373540  [   64/74412]\n",
      "loss: 2.093883  [ 6464/74412]\n",
      "loss: 1.948109  [12864/74412]\n",
      "loss: 1.836815  [19264/74412]\n",
      "loss: 2.028125  [25664/74412]\n",
      "loss: 2.270399  [32064/74412]\n",
      "loss: 2.098759  [38464/74412]\n",
      "loss: 1.877163  [44864/74412]\n",
      "loss: 1.963697  [51264/74412]\n",
      "loss: 1.677843  [57664/74412]\n",
      "loss: 2.077953  [64064/74412]\n",
      "loss: 1.962367  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 45.9%, Avg loss: 2.109149 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 2.365166  [   64/74412]\n",
      "loss: 2.083473  [ 6464/74412]\n",
      "loss: 1.941283  [12864/74412]\n",
      "loss: 1.827729  [19264/74412]\n",
      "loss: 2.018540  [25664/74412]\n",
      "loss: 2.267350  [32064/74412]\n",
      "loss: 2.090196  [38464/74412]\n",
      "loss: 1.868252  [44864/74412]\n",
      "loss: 1.954745  [51264/74412]\n",
      "loss: 1.669828  [57664/74412]\n",
      "loss: 2.069800  [64064/74412]\n",
      "loss: 1.954296  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 46.1%, Avg loss: 2.101086 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 2.356677  [   64/74412]\n",
      "loss: 2.073930  [ 6464/74412]\n",
      "loss: 1.934523  [12864/74412]\n",
      "loss: 1.819358  [19264/74412]\n",
      "loss: 2.008655  [25664/74412]\n",
      "loss: 2.263442  [32064/74412]\n",
      "loss: 2.081353  [38464/74412]\n",
      "loss: 1.858558  [44864/74412]\n",
      "loss: 1.944993  [51264/74412]\n",
      "loss: 1.661446  [57664/74412]\n",
      "loss: 2.061604  [64064/74412]\n",
      "loss: 1.946261  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 46.3%, Avg loss: 2.092856 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 2.348281  [   64/74412]\n",
      "loss: 2.064641  [ 6464/74412]\n",
      "loss: 1.927921  [12864/74412]\n",
      "loss: 1.810490  [19264/74412]\n",
      "loss: 1.999452  [25664/74412]\n",
      "loss: 2.259069  [32064/74412]\n",
      "loss: 2.072742  [38464/74412]\n",
      "loss: 1.850255  [44864/74412]\n",
      "loss: 1.936295  [51264/74412]\n",
      "loss: 1.654117  [57664/74412]\n",
      "loss: 2.053182  [64064/74412]\n",
      "loss: 1.938543  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 46.5%, Avg loss: 2.085064 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 2.340762  [   64/74412]\n",
      "loss: 2.054773  [ 6464/74412]\n",
      "loss: 1.921656  [12864/74412]\n",
      "loss: 1.802135  [19264/74412]\n",
      "loss: 1.989572  [25664/74412]\n",
      "loss: 2.255552  [32064/74412]\n",
      "loss: 2.063273  [38464/74412]\n",
      "loss: 1.841779  [44864/74412]\n",
      "loss: 1.927992  [51264/74412]\n",
      "loss: 1.645990  [57664/74412]\n",
      "loss: 2.044970  [64064/74412]\n",
      "loss: 1.930531  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.076503 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 2.331772  [   64/74412]\n",
      "loss: 2.045049  [ 6464/74412]\n",
      "loss: 1.915214  [12864/74412]\n",
      "loss: 1.794270  [19264/74412]\n",
      "loss: 1.980097  [25664/74412]\n",
      "loss: 2.251646  [32064/74412]\n",
      "loss: 2.054487  [38464/74412]\n",
      "loss: 1.833814  [44864/74412]\n",
      "loss: 1.919578  [51264/74412]\n",
      "loss: 1.638463  [57664/74412]\n",
      "loss: 2.036668  [64064/74412]\n",
      "loss: 1.923344  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 46.9%, Avg loss: 2.069773 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 2.324355  [   64/74412]\n",
      "loss: 2.037521  [ 6464/74412]\n",
      "loss: 1.908392  [12864/74412]\n",
      "loss: 1.786090  [19264/74412]\n",
      "loss: 1.970338  [25664/74412]\n",
      "loss: 2.246685  [32064/74412]\n",
      "loss: 2.045763  [38464/74412]\n",
      "loss: 1.827028  [44864/74412]\n",
      "loss: 1.912345  [51264/74412]\n",
      "loss: 1.630410  [57664/74412]\n",
      "loss: 2.028287  [64064/74412]\n",
      "loss: 1.915999  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 47.2%, Avg loss: 2.061921 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 2.316276  [   64/74412]\n",
      "loss: 2.027692  [ 6464/74412]\n",
      "loss: 1.902419  [12864/74412]\n",
      "loss: 1.778093  [19264/74412]\n",
      "loss: 1.960682  [25664/74412]\n",
      "loss: 2.242688  [32064/74412]\n",
      "loss: 2.036736  [38464/74412]\n",
      "loss: 1.819334  [44864/74412]\n",
      "loss: 1.904600  [51264/74412]\n",
      "loss: 1.622924  [57664/74412]\n",
      "loss: 2.020046  [64064/74412]\n",
      "loss: 1.908848  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 47.3%, Avg loss: 2.056997 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 2.310720  [   64/74412]\n",
      "loss: 2.018038  [ 6464/74412]\n",
      "loss: 1.896240  [12864/74412]\n",
      "loss: 1.769756  [19264/74412]\n",
      "loss: 1.950840  [25664/74412]\n",
      "loss: 2.238007  [32064/74412]\n",
      "loss: 2.028024  [38464/74412]\n",
      "loss: 1.812573  [44864/74412]\n",
      "loss: 1.896924  [51264/74412]\n",
      "loss: 1.615343  [57664/74412]\n",
      "loss: 2.011978  [64064/74412]\n",
      "loss: 1.901529  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 47.6%, Avg loss: 2.046257 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 2.300126  [   64/74412]\n",
      "loss: 2.006615  [ 6464/74412]\n",
      "loss: 1.890227  [12864/74412]\n",
      "loss: 1.761265  [19264/74412]\n",
      "loss: 1.941838  [25664/74412]\n",
      "loss: 2.233925  [32064/74412]\n",
      "loss: 2.018112  [38464/74412]\n",
      "loss: 1.805104  [44864/74412]\n",
      "loss: 1.890047  [51264/74412]\n",
      "loss: 1.608060  [57664/74412]\n",
      "loss: 2.004133  [64064/74412]\n",
      "loss: 1.893750  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 47.8%, Avg loss: 2.038948 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 2.292242  [   64/74412]\n",
      "loss: 1.996308  [ 6464/74412]\n",
      "loss: 1.884215  [12864/74412]\n",
      "loss: 1.753195  [19264/74412]\n",
      "loss: 1.932619  [25664/74412]\n",
      "loss: 2.228546  [32064/74412]\n",
      "loss: 2.010223  [38464/74412]\n",
      "loss: 1.797822  [44864/74412]\n",
      "loss: 1.882920  [51264/74412]\n",
      "loss: 1.600986  [57664/74412]\n",
      "loss: 1.996770  [64064/74412]\n",
      "loss: 1.885273  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 2.031387 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 2.285109  [   64/74412]\n",
      "loss: 1.985886  [ 6464/74412]\n",
      "loss: 1.877933  [12864/74412]\n",
      "loss: 1.745344  [19264/74412]\n",
      "loss: 1.923489  [25664/74412]\n",
      "loss: 2.223286  [32064/74412]\n",
      "loss: 2.000938  [38464/74412]\n",
      "loss: 1.789845  [44864/74412]\n",
      "loss: 1.876091  [51264/74412]\n",
      "loss: 1.593078  [57664/74412]\n",
      "loss: 1.989132  [64064/74412]\n",
      "loss: 1.878021  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 48.3%, Avg loss: 2.022854 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 2.276072  [   64/74412]\n",
      "loss: 1.976882  [ 6464/74412]\n",
      "loss: 1.871458  [12864/74412]\n",
      "loss: 1.737488  [19264/74412]\n",
      "loss: 1.913614  [25664/74412]\n",
      "loss: 2.218709  [32064/74412]\n",
      "loss: 1.991190  [38464/74412]\n",
      "loss: 1.782324  [44864/74412]\n",
      "loss: 1.868498  [51264/74412]\n",
      "loss: 1.586692  [57664/74412]\n",
      "loss: 1.980781  [64064/74412]\n",
      "loss: 1.869837  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 2.015733 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 2.268357  [   64/74412]\n",
      "loss: 1.966771  [ 6464/74412]\n",
      "loss: 1.864774  [12864/74412]\n",
      "loss: 1.729851  [19264/74412]\n",
      "loss: 1.903750  [25664/74412]\n",
      "loss: 2.213985  [32064/74412]\n",
      "loss: 1.982016  [38464/74412]\n",
      "loss: 1.775511  [44864/74412]\n",
      "loss: 1.862578  [51264/74412]\n",
      "loss: 1.580017  [57664/74412]\n",
      "loss: 1.972889  [64064/74412]\n",
      "loss: 1.861842  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 2.008412 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 2.260588  [   64/74412]\n",
      "loss: 1.957524  [ 6464/74412]\n",
      "loss: 1.858467  [12864/74412]\n",
      "loss: 1.722273  [19264/74412]\n",
      "loss: 1.893755  [25664/74412]\n",
      "loss: 2.209127  [32064/74412]\n",
      "loss: 1.971342  [38464/74412]\n",
      "loss: 1.770037  [44864/74412]\n",
      "loss: 1.855291  [51264/74412]\n",
      "loss: 1.573162  [57664/74412]\n",
      "loss: 1.965168  [64064/74412]\n",
      "loss: 1.854962  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 48.8%, Avg loss: 2.000769 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 2.253003  [   64/74412]\n",
      "loss: 1.947579  [ 6464/74412]\n",
      "loss: 1.852169  [12864/74412]\n",
      "loss: 1.714705  [19264/74412]\n",
      "loss: 1.884218  [25664/74412]\n",
      "loss: 2.204434  [32064/74412]\n",
      "loss: 1.962405  [38464/74412]\n",
      "loss: 1.763408  [44864/74412]\n",
      "loss: 1.848467  [51264/74412]\n",
      "loss: 1.566242  [57664/74412]\n",
      "loss: 1.957258  [64064/74412]\n",
      "loss: 1.847305  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.993547 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 2.245636  [   64/74412]\n",
      "loss: 1.937979  [ 6464/74412]\n",
      "loss: 1.845723  [12864/74412]\n",
      "loss: 1.707191  [19264/74412]\n",
      "loss: 1.874494  [25664/74412]\n",
      "loss: 2.199699  [32064/74412]\n",
      "loss: 1.953500  [38464/74412]\n",
      "loss: 1.756289  [44864/74412]\n",
      "loss: 1.841011  [51264/74412]\n",
      "loss: 1.560125  [57664/74412]\n",
      "loss: 1.949523  [64064/74412]\n",
      "loss: 1.838817  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 49.1%, Avg loss: 1.987400 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 2.239313  [   64/74412]\n",
      "loss: 1.929005  [ 6464/74412]\n",
      "loss: 1.839801  [12864/74412]\n",
      "loss: 1.699717  [19264/74412]\n",
      "loss: 1.864999  [25664/74412]\n",
      "loss: 2.195647  [32064/74412]\n",
      "loss: 1.943486  [38464/74412]\n",
      "loss: 1.749486  [44864/74412]\n",
      "loss: 1.834702  [51264/74412]\n",
      "loss: 1.554268  [57664/74412]\n",
      "loss: 1.941775  [64064/74412]\n",
      "loss: 1.830878  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.980370 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 2.232066  [   64/74412]\n",
      "loss: 1.919932  [ 6464/74412]\n",
      "loss: 1.833685  [12864/74412]\n",
      "loss: 1.692052  [19264/74412]\n",
      "loss: 1.855440  [25664/74412]\n",
      "loss: 2.190580  [32064/74412]\n",
      "loss: 1.935272  [38464/74412]\n",
      "loss: 1.742351  [44864/74412]\n",
      "loss: 1.828967  [51264/74412]\n",
      "loss: 1.548618  [57664/74412]\n",
      "loss: 1.934254  [64064/74412]\n",
      "loss: 1.822721  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.973829 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 2.225481  [   64/74412]\n",
      "loss: 1.910063  [ 6464/74412]\n",
      "loss: 1.827736  [12864/74412]\n",
      "loss: 1.684790  [19264/74412]\n",
      "loss: 1.846275  [25664/74412]\n",
      "loss: 2.185833  [32064/74412]\n",
      "loss: 1.926061  [38464/74412]\n",
      "loss: 1.735911  [44864/74412]\n",
      "loss: 1.822218  [51264/74412]\n",
      "loss: 1.542395  [57664/74412]\n",
      "loss: 1.926362  [64064/74412]\n",
      "loss: 1.814282  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 1.966769 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 2.219471  [   64/74412]\n",
      "loss: 1.900692  [ 6464/74412]\n",
      "loss: 1.821621  [12864/74412]\n",
      "loss: 1.677341  [19264/74412]\n",
      "loss: 1.836743  [25664/74412]\n",
      "loss: 2.180259  [32064/74412]\n",
      "loss: 1.918893  [38464/74412]\n",
      "loss: 1.728562  [44864/74412]\n",
      "loss: 1.815486  [51264/74412]\n",
      "loss: 1.537486  [57664/74412]\n",
      "loss: 1.918411  [64064/74412]\n",
      "loss: 1.805902  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 49.9%, Avg loss: 1.960131 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 2.213092  [   64/74412]\n",
      "loss: 1.891119  [ 6464/74412]\n",
      "loss: 1.815780  [12864/74412]\n",
      "loss: 1.670266  [19264/74412]\n",
      "loss: 1.827698  [25664/74412]\n",
      "loss: 2.175356  [32064/74412]\n",
      "loss: 1.908568  [38464/74412]\n",
      "loss: 1.721368  [44864/74412]\n",
      "loss: 1.809663  [51264/74412]\n",
      "loss: 1.531682  [57664/74412]\n",
      "loss: 1.909969  [64064/74412]\n",
      "loss: 1.797643  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 1.952078 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 2.204962  [   64/74412]\n",
      "loss: 1.882067  [ 6464/74412]\n",
      "loss: 1.809598  [12864/74412]\n",
      "loss: 1.663261  [19264/74412]\n",
      "loss: 1.818234  [25664/74412]\n",
      "loss: 2.170039  [32064/74412]\n",
      "loss: 1.900239  [38464/74412]\n",
      "loss: 1.714888  [44864/74412]\n",
      "loss: 1.803359  [51264/74412]\n",
      "loss: 1.526049  [57664/74412]\n",
      "loss: 1.902236  [64064/74412]\n",
      "loss: 1.789658  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.946079 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 2.199155  [   64/74412]\n",
      "loss: 1.872861  [ 6464/74412]\n",
      "loss: 1.804093  [12864/74412]\n",
      "loss: 1.656507  [19264/74412]\n",
      "loss: 1.809775  [25664/74412]\n",
      "loss: 2.165562  [32064/74412]\n",
      "loss: 1.891472  [38464/74412]\n",
      "loss: 1.707639  [44864/74412]\n",
      "loss: 1.796893  [51264/74412]\n",
      "loss: 1.520029  [57664/74412]\n",
      "loss: 1.894828  [64064/74412]\n",
      "loss: 1.782313  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.938634 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 2.191967  [   64/74412]\n",
      "loss: 1.863317  [ 6464/74412]\n",
      "loss: 1.798854  [12864/74412]\n",
      "loss: 1.649433  [19264/74412]\n",
      "loss: 1.801405  [25664/74412]\n",
      "loss: 2.161376  [32064/74412]\n",
      "loss: 1.881894  [38464/74412]\n",
      "loss: 1.700261  [44864/74412]\n",
      "loss: 1.791371  [51264/74412]\n",
      "loss: 1.514770  [57664/74412]\n",
      "loss: 1.887318  [64064/74412]\n",
      "loss: 1.773721  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.932804 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 2.186551  [   64/74412]\n",
      "loss: 1.853954  [ 6464/74412]\n",
      "loss: 1.793582  [12864/74412]\n",
      "loss: 1.642788  [19264/74412]\n",
      "loss: 1.792752  [25664/74412]\n",
      "loss: 2.156599  [32064/74412]\n",
      "loss: 1.873813  [38464/74412]\n",
      "loss: 1.693455  [44864/74412]\n",
      "loss: 1.786103  [51264/74412]\n",
      "loss: 1.509742  [57664/74412]\n",
      "loss: 1.880211  [64064/74412]\n",
      "loss: 1.765611  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.926201 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 2.180306  [   64/74412]\n",
      "loss: 1.845059  [ 6464/74412]\n",
      "loss: 1.787843  [12864/74412]\n",
      "loss: 1.635978  [19264/74412]\n",
      "loss: 1.785000  [25664/74412]\n",
      "loss: 2.151697  [32064/74412]\n",
      "loss: 1.865361  [38464/74412]\n",
      "loss: 1.685697  [44864/74412]\n",
      "loss: 1.780156  [51264/74412]\n",
      "loss: 1.504634  [57664/74412]\n",
      "loss: 1.873487  [64064/74412]\n",
      "loss: 1.757701  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.919677 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 2.174207  [   64/74412]\n",
      "loss: 1.835899  [ 6464/74412]\n",
      "loss: 1.782231  [12864/74412]\n",
      "loss: 1.629166  [19264/74412]\n",
      "loss: 1.777357  [25664/74412]\n",
      "loss: 2.146694  [32064/74412]\n",
      "loss: 1.857416  [38464/74412]\n",
      "loss: 1.678692  [44864/74412]\n",
      "loss: 1.773323  [51264/74412]\n",
      "loss: 1.499469  [57664/74412]\n",
      "loss: 1.866628  [64064/74412]\n",
      "loss: 1.749900  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 1.912837 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 2.167452  [   64/74412]\n",
      "loss: 1.825826  [ 6464/74412]\n",
      "loss: 1.776930  [12864/74412]\n",
      "loss: 1.622399  [19264/74412]\n",
      "loss: 1.769648  [25664/74412]\n",
      "loss: 2.140653  [32064/74412]\n",
      "loss: 1.849142  [38464/74412]\n",
      "loss: 1.671365  [44864/74412]\n",
      "loss: 1.767881  [51264/74412]\n",
      "loss: 1.494630  [57664/74412]\n",
      "loss: 1.859569  [64064/74412]\n",
      "loss: 1.742548  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 51.4%, Avg loss: 1.906431 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 2.161458  [   64/74412]\n",
      "loss: 1.817030  [ 6464/74412]\n",
      "loss: 1.771290  [12864/74412]\n",
      "loss: 1.615593  [19264/74412]\n",
      "loss: 1.761927  [25664/74412]\n",
      "loss: 2.135620  [32064/74412]\n",
      "loss: 1.841416  [38464/74412]\n",
      "loss: 1.664663  [44864/74412]\n",
      "loss: 1.763193  [51264/74412]\n",
      "loss: 1.490197  [57664/74412]\n",
      "loss: 1.853549  [64064/74412]\n",
      "loss: 1.734642  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.900098 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 2.155239  [   64/74412]\n",
      "loss: 1.807950  [ 6464/74412]\n",
      "loss: 1.765298  [12864/74412]\n",
      "loss: 1.609280  [19264/74412]\n",
      "loss: 1.754611  [25664/74412]\n",
      "loss: 2.131058  [32064/74412]\n",
      "loss: 1.833707  [38464/74412]\n",
      "loss: 1.657934  [44864/74412]\n",
      "loss: 1.758320  [51264/74412]\n",
      "loss: 1.485714  [57664/74412]\n",
      "loss: 1.846839  [64064/74412]\n",
      "loss: 1.726207  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.893832 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 2.149711  [   64/74412]\n",
      "loss: 1.799469  [ 6464/74412]\n",
      "loss: 1.759529  [12864/74412]\n",
      "loss: 1.602819  [19264/74412]\n",
      "loss: 1.747011  [25664/74412]\n",
      "loss: 2.126471  [32064/74412]\n",
      "loss: 1.826156  [38464/74412]\n",
      "loss: 1.650768  [44864/74412]\n",
      "loss: 1.753362  [51264/74412]\n",
      "loss: 1.481256  [57664/74412]\n",
      "loss: 1.840538  [64064/74412]\n",
      "loss: 1.717907  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 1.887415 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 2.143881  [   64/74412]\n",
      "loss: 1.791431  [ 6464/74412]\n",
      "loss: 1.753581  [12864/74412]\n",
      "loss: 1.596525  [19264/74412]\n",
      "loss: 1.739809  [25664/74412]\n",
      "loss: 2.122380  [32064/74412]\n",
      "loss: 1.818686  [38464/74412]\n",
      "loss: 1.644366  [44864/74412]\n",
      "loss: 1.748058  [51264/74412]\n",
      "loss: 1.476742  [57664/74412]\n",
      "loss: 1.834470  [64064/74412]\n",
      "loss: 1.709833  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.880384 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 2.137546  [   64/74412]\n",
      "loss: 1.783456  [ 6464/74412]\n",
      "loss: 1.748061  [12864/74412]\n",
      "loss: 1.589990  [19264/74412]\n",
      "loss: 1.732996  [25664/74412]\n",
      "loss: 2.117152  [32064/74412]\n",
      "loss: 1.811371  [38464/74412]\n",
      "loss: 1.637382  [44864/74412]\n",
      "loss: 1.742688  [51264/74412]\n",
      "loss: 1.472022  [57664/74412]\n",
      "loss: 1.828707  [64064/74412]\n",
      "loss: 1.701946  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 1.874653 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 2.132355  [   64/74412]\n",
      "loss: 1.775470  [ 6464/74412]\n",
      "loss: 1.742026  [12864/74412]\n",
      "loss: 1.583794  [19264/74412]\n",
      "loss: 1.726011  [25664/74412]\n",
      "loss: 2.113030  [32064/74412]\n",
      "loss: 1.803970  [38464/74412]\n",
      "loss: 1.631192  [44864/74412]\n",
      "loss: 1.738515  [51264/74412]\n",
      "loss: 1.468520  [57664/74412]\n",
      "loss: 1.823583  [64064/74412]\n",
      "loss: 1.694105  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.869574 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 2.127831  [   64/74412]\n",
      "loss: 1.767476  [ 6464/74412]\n",
      "loss: 1.736876  [12864/74412]\n",
      "loss: 1.578008  [19264/74412]\n",
      "loss: 1.719090  [25664/74412]\n",
      "loss: 2.108141  [32064/74412]\n",
      "loss: 1.796530  [38464/74412]\n",
      "loss: 1.624182  [44864/74412]\n",
      "loss: 1.733846  [51264/74412]\n",
      "loss: 1.464540  [57664/74412]\n",
      "loss: 1.817777  [64064/74412]\n",
      "loss: 1.686074  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.864642 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 2.124058  [   64/74412]\n",
      "loss: 1.759310  [ 6464/74412]\n",
      "loss: 1.731421  [12864/74412]\n",
      "loss: 1.572381  [19264/74412]\n",
      "loss: 1.712739  [25664/74412]\n",
      "loss: 2.105048  [32064/74412]\n",
      "loss: 1.789217  [38464/74412]\n",
      "loss: 1.618170  [44864/74412]\n",
      "loss: 1.728921  [51264/74412]\n",
      "loss: 1.460636  [57664/74412]\n",
      "loss: 1.812644  [64064/74412]\n",
      "loss: 1.679027  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 1.858747 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 2.118969  [   64/74412]\n",
      "loss: 1.752221  [ 6464/74412]\n",
      "loss: 1.725280  [12864/74412]\n",
      "loss: 1.566242  [19264/74412]\n",
      "loss: 1.706239  [25664/74412]\n",
      "loss: 2.098649  [32064/74412]\n",
      "loss: 1.782690  [38464/74412]\n",
      "loss: 1.611537  [44864/74412]\n",
      "loss: 1.724238  [51264/74412]\n",
      "loss: 1.456931  [57664/74412]\n",
      "loss: 1.807271  [64064/74412]\n",
      "loss: 1.671589  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.852716 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 2.114019  [   64/74412]\n",
      "loss: 1.744723  [ 6464/74412]\n",
      "loss: 1.720415  [12864/74412]\n",
      "loss: 1.560736  [19264/74412]\n",
      "loss: 1.700226  [25664/74412]\n",
      "loss: 2.094803  [32064/74412]\n",
      "loss: 1.775959  [38464/74412]\n",
      "loss: 1.604791  [44864/74412]\n",
      "loss: 1.719887  [51264/74412]\n",
      "loss: 1.453313  [57664/74412]\n",
      "loss: 1.802405  [64064/74412]\n",
      "loss: 1.664052  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 1.847336 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 2.109585  [   64/74412]\n",
      "loss: 1.737667  [ 6464/74412]\n",
      "loss: 1.714643  [12864/74412]\n",
      "loss: 1.555060  [19264/74412]\n",
      "loss: 1.694602  [25664/74412]\n",
      "loss: 2.091134  [32064/74412]\n",
      "loss: 1.768801  [38464/74412]\n",
      "loss: 1.598985  [44864/74412]\n",
      "loss: 1.714460  [51264/74412]\n",
      "loss: 1.449901  [57664/74412]\n",
      "loss: 1.797572  [64064/74412]\n",
      "loss: 1.657086  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 1.841443 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 2.103910  [   64/74412]\n",
      "loss: 1.730851  [ 6464/74412]\n",
      "loss: 1.708877  [12864/74412]\n",
      "loss: 1.549563  [19264/74412]\n",
      "loss: 1.688997  [25664/74412]\n",
      "loss: 2.086313  [32064/74412]\n",
      "loss: 1.761898  [38464/74412]\n",
      "loss: 1.592652  [44864/74412]\n",
      "loss: 1.709565  [51264/74412]\n",
      "loss: 1.446653  [57664/74412]\n",
      "loss: 1.792580  [64064/74412]\n",
      "loss: 1.650152  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.835759 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 2.099374  [   64/74412]\n",
      "loss: 1.724210  [ 6464/74412]\n",
      "loss: 1.703112  [12864/74412]\n",
      "loss: 1.544054  [19264/74412]\n",
      "loss: 1.683380  [25664/74412]\n",
      "loss: 2.081488  [32064/74412]\n",
      "loss: 1.755786  [38464/74412]\n",
      "loss: 1.585983  [44864/74412]\n",
      "loss: 1.704795  [51264/74412]\n",
      "loss: 1.443593  [57664/74412]\n",
      "loss: 1.787499  [64064/74412]\n",
      "loss: 1.643123  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 1.830928 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 2.095506  [   64/74412]\n",
      "loss: 1.717905  [ 6464/74412]\n",
      "loss: 1.696619  [12864/74412]\n",
      "loss: 1.538455  [19264/74412]\n",
      "loss: 1.678207  [25664/74412]\n",
      "loss: 2.077192  [32064/74412]\n",
      "loss: 1.749528  [38464/74412]\n",
      "loss: 1.580626  [44864/74412]\n",
      "loss: 1.700077  [51264/74412]\n",
      "loss: 1.440199  [57664/74412]\n",
      "loss: 1.782543  [64064/74412]\n",
      "loss: 1.635949  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 1.826091 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 2.091539  [   64/74412]\n",
      "loss: 1.712409  [ 6464/74412]\n",
      "loss: 1.690478  [12864/74412]\n",
      "loss: 1.533169  [19264/74412]\n",
      "loss: 1.673208  [25664/74412]\n",
      "loss: 2.072787  [32064/74412]\n",
      "loss: 1.742671  [38464/74412]\n",
      "loss: 1.575269  [44864/74412]\n",
      "loss: 1.695651  [51264/74412]\n",
      "loss: 1.436477  [57664/74412]\n",
      "loss: 1.777822  [64064/74412]\n",
      "loss: 1.628969  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 1.820171 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 2.086107  [   64/74412]\n",
      "loss: 1.705896  [ 6464/74412]\n",
      "loss: 1.684963  [12864/74412]\n",
      "loss: 1.527874  [19264/74412]\n",
      "loss: 1.667868  [25664/74412]\n",
      "loss: 2.068711  [32064/74412]\n",
      "loss: 1.736200  [38464/74412]\n",
      "loss: 1.569275  [44864/74412]\n",
      "loss: 1.690688  [51264/74412]\n",
      "loss: 1.433602  [57664/74412]\n",
      "loss: 1.773237  [64064/74412]\n",
      "loss: 1.622362  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 1.815390 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 2.082004  [   64/74412]\n",
      "loss: 1.699303  [ 6464/74412]\n",
      "loss: 1.679695  [12864/74412]\n",
      "loss: 1.522800  [19264/74412]\n",
      "loss: 1.662686  [25664/74412]\n",
      "loss: 2.063440  [32064/74412]\n",
      "loss: 1.729495  [38464/74412]\n",
      "loss: 1.564254  [44864/74412]\n",
      "loss: 1.686577  [51264/74412]\n",
      "loss: 1.430538  [57664/74412]\n",
      "loss: 1.768765  [64064/74412]\n",
      "loss: 1.615943  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 1.810821 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 2.078458  [   64/74412]\n",
      "loss: 1.693779  [ 6464/74412]\n",
      "loss: 1.673972  [12864/74412]\n",
      "loss: 1.518032  [19264/74412]\n",
      "loss: 1.657186  [25664/74412]\n",
      "loss: 2.059833  [32064/74412]\n",
      "loss: 1.723419  [38464/74412]\n",
      "loss: 1.558181  [44864/74412]\n",
      "loss: 1.682272  [51264/74412]\n",
      "loss: 1.427305  [57664/74412]\n",
      "loss: 1.764228  [64064/74412]\n",
      "loss: 1.609283  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 1.806402 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 2.074567  [   64/74412]\n",
      "loss: 1.687575  [ 6464/74412]\n",
      "loss: 1.668842  [12864/74412]\n",
      "loss: 1.513226  [19264/74412]\n",
      "loss: 1.652283  [25664/74412]\n",
      "loss: 2.055316  [32064/74412]\n",
      "loss: 1.716766  [38464/74412]\n",
      "loss: 1.552581  [44864/74412]\n",
      "loss: 1.677953  [51264/74412]\n",
      "loss: 1.424616  [57664/74412]\n",
      "loss: 1.760259  [64064/74412]\n",
      "loss: 1.602320  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: 1.802054 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 2.070870  [   64/74412]\n",
      "loss: 1.683172  [ 6464/74412]\n",
      "loss: 1.663745  [12864/74412]\n",
      "loss: 1.508189  [19264/74412]\n",
      "loss: 1.646952  [25664/74412]\n",
      "loss: 2.052260  [32064/74412]\n",
      "loss: 1.712370  [38464/74412]\n",
      "loss: 1.547967  [44864/74412]\n",
      "loss: 1.673775  [51264/74412]\n",
      "loss: 1.422119  [57664/74412]\n",
      "loss: 1.755119  [64064/74412]\n",
      "loss: 1.595521  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 1.797791 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 2.067325  [   64/74412]\n",
      "loss: 1.676876  [ 6464/74412]\n",
      "loss: 1.657868  [12864/74412]\n",
      "loss: 1.503427  [19264/74412]\n",
      "loss: 1.641838  [25664/74412]\n",
      "loss: 2.048166  [32064/74412]\n",
      "loss: 1.707421  [38464/74412]\n",
      "loss: 1.542995  [44864/74412]\n",
      "loss: 1.668846  [51264/74412]\n",
      "loss: 1.419191  [57664/74412]\n",
      "loss: 1.750291  [64064/74412]\n",
      "loss: 1.588924  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.793554 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 2.063229  [   64/74412]\n",
      "loss: 1.671554  [ 6464/74412]\n",
      "loss: 1.653133  [12864/74412]\n",
      "loss: 1.498915  [19264/74412]\n",
      "loss: 1.636975  [25664/74412]\n",
      "loss: 2.044112  [32064/74412]\n",
      "loss: 1.701288  [38464/74412]\n",
      "loss: 1.538093  [44864/74412]\n",
      "loss: 1.665026  [51264/74412]\n",
      "loss: 1.416436  [57664/74412]\n",
      "loss: 1.746021  [64064/74412]\n",
      "loss: 1.582127  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1.789387 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 2.059372  [   64/74412]\n",
      "loss: 1.665121  [ 6464/74412]\n",
      "loss: 1.647345  [12864/74412]\n",
      "loss: 1.494301  [19264/74412]\n",
      "loss: 1.632024  [25664/74412]\n",
      "loss: 2.040302  [32064/74412]\n",
      "loss: 1.696098  [38464/74412]\n",
      "loss: 1.533330  [44864/74412]\n",
      "loss: 1.660294  [51264/74412]\n",
      "loss: 1.413400  [57664/74412]\n",
      "loss: 1.741327  [64064/74412]\n",
      "loss: 1.575856  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 1.785232 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 2.056027  [   64/74412]\n",
      "loss: 1.660121  [ 6464/74412]\n",
      "loss: 1.641910  [12864/74412]\n",
      "loss: 1.490543  [19264/74412]\n",
      "loss: 1.627348  [25664/74412]\n",
      "loss: 2.036988  [32064/74412]\n",
      "loss: 1.690235  [38464/74412]\n",
      "loss: 1.529120  [44864/74412]\n",
      "loss: 1.655722  [51264/74412]\n",
      "loss: 1.410192  [57664/74412]\n",
      "loss: 1.737197  [64064/74412]\n",
      "loss: 1.569568  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.780780 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 2.051317  [   64/74412]\n",
      "loss: 1.654967  [ 6464/74412]\n",
      "loss: 1.635763  [12864/74412]\n",
      "loss: 1.486549  [19264/74412]\n",
      "loss: 1.622811  [25664/74412]\n",
      "loss: 2.033139  [32064/74412]\n",
      "loss: 1.685164  [38464/74412]\n",
      "loss: 1.523793  [44864/74412]\n",
      "loss: 1.651188  [51264/74412]\n",
      "loss: 1.407569  [57664/74412]\n",
      "loss: 1.731356  [64064/74412]\n",
      "loss: 1.563184  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 1.776652 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 2.047358  [   64/74412]\n",
      "loss: 1.650104  [ 6464/74412]\n",
      "loss: 1.630904  [12864/74412]\n",
      "loss: 1.482752  [19264/74412]\n",
      "loss: 1.618417  [25664/74412]\n",
      "loss: 2.029810  [32064/74412]\n",
      "loss: 1.679139  [38464/74412]\n",
      "loss: 1.519212  [44864/74412]\n",
      "loss: 1.646392  [51264/74412]\n",
      "loss: 1.404480  [57664/74412]\n",
      "loss: 1.727377  [64064/74412]\n",
      "loss: 1.557299  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.772121 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 2.043017  [   64/74412]\n",
      "loss: 1.645669  [ 6464/74412]\n",
      "loss: 1.625242  [12864/74412]\n",
      "loss: 1.478434  [19264/74412]\n",
      "loss: 1.613694  [25664/74412]\n",
      "loss: 2.025583  [32064/74412]\n",
      "loss: 1.673896  [38464/74412]\n",
      "loss: 1.514205  [44864/74412]\n",
      "loss: 1.641890  [51264/74412]\n",
      "loss: 1.401751  [57664/74412]\n",
      "loss: 1.722792  [64064/74412]\n",
      "loss: 1.550912  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 1.767931 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 2.039010  [   64/74412]\n",
      "loss: 1.639948  [ 6464/74412]\n",
      "loss: 1.619571  [12864/74412]\n",
      "loss: 1.474304  [19264/74412]\n",
      "loss: 1.609217  [25664/74412]\n",
      "loss: 2.022422  [32064/74412]\n",
      "loss: 1.668309  [38464/74412]\n",
      "loss: 1.509627  [44864/74412]\n",
      "loss: 1.637595  [51264/74412]\n",
      "loss: 1.399173  [57664/74412]\n",
      "loss: 1.718124  [64064/74412]\n",
      "loss: 1.544811  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 1.763830 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 2.035645  [   64/74412]\n",
      "loss: 1.635037  [ 6464/74412]\n",
      "loss: 1.613836  [12864/74412]\n",
      "loss: 1.470111  [19264/74412]\n",
      "loss: 1.604960  [25664/74412]\n",
      "loss: 2.018869  [32064/74412]\n",
      "loss: 1.663121  [38464/74412]\n",
      "loss: 1.505187  [44864/74412]\n",
      "loss: 1.633363  [51264/74412]\n",
      "loss: 1.396740  [57664/74412]\n",
      "loss: 1.713740  [64064/74412]\n",
      "loss: 1.538877  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.759753 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 2.032020  [   64/74412]\n",
      "loss: 1.630079  [ 6464/74412]\n",
      "loss: 1.608123  [12864/74412]\n",
      "loss: 1.465855  [19264/74412]\n",
      "loss: 1.600547  [25664/74412]\n",
      "loss: 2.015439  [32064/74412]\n",
      "loss: 1.657580  [38464/74412]\n",
      "loss: 1.500954  [44864/74412]\n",
      "loss: 1.628384  [51264/74412]\n",
      "loss: 1.394619  [57664/74412]\n",
      "loss: 1.709149  [64064/74412]\n",
      "loss: 1.532553  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 1.755372 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 2.027155  [   64/74412]\n",
      "loss: 1.624473  [ 6464/74412]\n",
      "loss: 1.602397  [12864/74412]\n",
      "loss: 1.461343  [19264/74412]\n",
      "loss: 1.596241  [25664/74412]\n",
      "loss: 2.011150  [32064/74412]\n",
      "loss: 1.652833  [38464/74412]\n",
      "loss: 1.495585  [44864/74412]\n",
      "loss: 1.624165  [51264/74412]\n",
      "loss: 1.392717  [57664/74412]\n",
      "loss: 1.704725  [64064/74412]\n",
      "loss: 1.526787  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 1.750883 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 2.022676  [   64/74412]\n",
      "loss: 1.620014  [ 6464/74412]\n",
      "loss: 1.596618  [12864/74412]\n",
      "loss: 1.456692  [19264/74412]\n",
      "loss: 1.592161  [25664/74412]\n",
      "loss: 2.007395  [32064/74412]\n",
      "loss: 1.649248  [38464/74412]\n",
      "loss: 1.491057  [44864/74412]\n",
      "loss: 1.619014  [51264/74412]\n",
      "loss: 1.390176  [57664/74412]\n",
      "loss: 1.699990  [64064/74412]\n",
      "loss: 1.520064  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 1.746896 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 2.019192  [   64/74412]\n",
      "loss: 1.615087  [ 6464/74412]\n",
      "loss: 1.591182  [12864/74412]\n",
      "loss: 1.452215  [19264/74412]\n",
      "loss: 1.587884  [25664/74412]\n",
      "loss: 2.003777  [32064/74412]\n",
      "loss: 1.643587  [38464/74412]\n",
      "loss: 1.487248  [44864/74412]\n",
      "loss: 1.614920  [51264/74412]\n",
      "loss: 1.387889  [57664/74412]\n",
      "loss: 1.695312  [64064/74412]\n",
      "loss: 1.516478  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 1.742750 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 2.015068  [   64/74412]\n",
      "loss: 1.610890  [ 6464/74412]\n",
      "loss: 1.585323  [12864/74412]\n",
      "loss: 1.447980  [19264/74412]\n",
      "loss: 1.583490  [25664/74412]\n",
      "loss: 1.999551  [32064/74412]\n",
      "loss: 1.638811  [38464/74412]\n",
      "loss: 1.482755  [44864/74412]\n",
      "loss: 1.610942  [51264/74412]\n",
      "loss: 1.385475  [57664/74412]\n",
      "loss: 1.690601  [64064/74412]\n",
      "loss: 1.511502  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 1.738653 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 2.011479  [   64/74412]\n",
      "loss: 1.606261  [ 6464/74412]\n",
      "loss: 1.579886  [12864/74412]\n",
      "loss: 1.443751  [19264/74412]\n",
      "loss: 1.578747  [25664/74412]\n",
      "loss: 1.996170  [32064/74412]\n",
      "loss: 1.634034  [38464/74412]\n",
      "loss: 1.478608  [44864/74412]\n",
      "loss: 1.606173  [51264/74412]\n",
      "loss: 1.382942  [57664/74412]\n",
      "loss: 1.686413  [64064/74412]\n",
      "loss: 1.506035  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 1.734624 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 2.007734  [   64/74412]\n",
      "loss: 1.602148  [ 6464/74412]\n",
      "loss: 1.574776  [12864/74412]\n",
      "loss: 1.439547  [19264/74412]\n",
      "loss: 1.574147  [25664/74412]\n",
      "loss: 1.992875  [32064/74412]\n",
      "loss: 1.630074  [38464/74412]\n",
      "loss: 1.474513  [44864/74412]\n",
      "loss: 1.601473  [51264/74412]\n",
      "loss: 1.380570  [57664/74412]\n",
      "loss: 1.682155  [64064/74412]\n",
      "loss: 1.500675  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 1.731782 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 2.005414  [   64/74412]\n",
      "loss: 1.598512  [ 6464/74412]\n",
      "loss: 1.569069  [12864/74412]\n",
      "loss: 1.435513  [19264/74412]\n",
      "loss: 1.570100  [25664/74412]\n",
      "loss: 1.989203  [32064/74412]\n",
      "loss: 1.625363  [38464/74412]\n",
      "loss: 1.470321  [44864/74412]\n",
      "loss: 1.596697  [51264/74412]\n",
      "loss: 1.378392  [57664/74412]\n",
      "loss: 1.677891  [64064/74412]\n",
      "loss: 1.495651  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 1.726470 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 1.999628  [   64/74412]\n",
      "loss: 1.594293  [ 6464/74412]\n",
      "loss: 1.563884  [12864/74412]\n",
      "loss: 1.432199  [19264/74412]\n",
      "loss: 1.565763  [25664/74412]\n",
      "loss: 1.986134  [32064/74412]\n",
      "loss: 1.620742  [38464/74412]\n",
      "loss: 1.466815  [44864/74412]\n",
      "loss: 1.591983  [51264/74412]\n",
      "loss: 1.376017  [57664/74412]\n",
      "loss: 1.673516  [64064/74412]\n",
      "loss: 1.490264  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 1.722818 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 1.996550  [   64/74412]\n",
      "loss: 1.590201  [ 6464/74412]\n",
      "loss: 1.558473  [12864/74412]\n",
      "loss: 1.428592  [19264/74412]\n",
      "loss: 1.561700  [25664/74412]\n",
      "loss: 1.983202  [32064/74412]\n",
      "loss: 1.616220  [38464/74412]\n",
      "loss: 1.462883  [44864/74412]\n",
      "loss: 1.587433  [51264/74412]\n",
      "loss: 1.373475  [57664/74412]\n",
      "loss: 1.669373  [64064/74412]\n",
      "loss: 1.485098  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.718908 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 1.992931  [   64/74412]\n",
      "loss: 1.586247  [ 6464/74412]\n",
      "loss: 1.553090  [12864/74412]\n",
      "loss: 1.424503  [19264/74412]\n",
      "loss: 1.557570  [25664/74412]\n",
      "loss: 1.980574  [32064/74412]\n",
      "loss: 1.611727  [38464/74412]\n",
      "loss: 1.458357  [44864/74412]\n",
      "loss: 1.582909  [51264/74412]\n",
      "loss: 1.371068  [57664/74412]\n",
      "loss: 1.665415  [64064/74412]\n",
      "loss: 1.480365  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 1.715338 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 1.989138  [   64/74412]\n",
      "loss: 1.582009  [ 6464/74412]\n",
      "loss: 1.547791  [12864/74412]\n",
      "loss: 1.421422  [19264/74412]\n",
      "loss: 1.553399  [25664/74412]\n",
      "loss: 1.977496  [32064/74412]\n",
      "loss: 1.606850  [38464/74412]\n",
      "loss: 1.454834  [44864/74412]\n",
      "loss: 1.578847  [51264/74412]\n",
      "loss: 1.368605  [57664/74412]\n",
      "loss: 1.661075  [64064/74412]\n",
      "loss: 1.475992  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 1.711161 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 1.984590  [   64/74412]\n",
      "loss: 1.578117  [ 6464/74412]\n",
      "loss: 1.542308  [12864/74412]\n",
      "loss: 1.417583  [19264/74412]\n",
      "loss: 1.549234  [25664/74412]\n",
      "loss: 1.975088  [32064/74412]\n",
      "loss: 1.603063  [38464/74412]\n",
      "loss: 1.450397  [44864/74412]\n",
      "loss: 1.575166  [51264/74412]\n",
      "loss: 1.366202  [57664/74412]\n",
      "loss: 1.657069  [64064/74412]\n",
      "loss: 1.470694  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 1.707615 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 1.981625  [   64/74412]\n",
      "loss: 1.574184  [ 6464/74412]\n",
      "loss: 1.536405  [12864/74412]\n",
      "loss: 1.414513  [19264/74412]\n",
      "loss: 1.545281  [25664/74412]\n",
      "loss: 1.972269  [32064/74412]\n",
      "loss: 1.598122  [38464/74412]\n",
      "loss: 1.446565  [44864/74412]\n",
      "loss: 1.571084  [51264/74412]\n",
      "loss: 1.363423  [57664/74412]\n",
      "loss: 1.652826  [64064/74412]\n",
      "loss: 1.466014  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 1.703743 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 1.977768  [   64/74412]\n",
      "loss: 1.571224  [ 6464/74412]\n",
      "loss: 1.531485  [12864/74412]\n",
      "loss: 1.410898  [19264/74412]\n",
      "loss: 1.541666  [25664/74412]\n",
      "loss: 1.969844  [32064/74412]\n",
      "loss: 1.594028  [38464/74412]\n",
      "loss: 1.442603  [44864/74412]\n",
      "loss: 1.566875  [51264/74412]\n",
      "loss: 1.361442  [57664/74412]\n",
      "loss: 1.648506  [64064/74412]\n",
      "loss: 1.461124  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 1.700091 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 1.974233  [   64/74412]\n",
      "loss: 1.567347  [ 6464/74412]\n",
      "loss: 1.527012  [12864/74412]\n",
      "loss: 1.407703  [19264/74412]\n",
      "loss: 1.537660  [25664/74412]\n",
      "loss: 1.967021  [32064/74412]\n",
      "loss: 1.589786  [38464/74412]\n",
      "loss: 1.438083  [44864/74412]\n",
      "loss: 1.562807  [51264/74412]\n",
      "loss: 1.358841  [57664/74412]\n",
      "loss: 1.644843  [64064/74412]\n",
      "loss: 1.456678  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.696408 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 1.970470  [   64/74412]\n",
      "loss: 1.563642  [ 6464/74412]\n",
      "loss: 1.522025  [12864/74412]\n",
      "loss: 1.404235  [19264/74412]\n",
      "loss: 1.533928  [25664/74412]\n",
      "loss: 1.963862  [32064/74412]\n",
      "loss: 1.585927  [38464/74412]\n",
      "loss: 1.433689  [44864/74412]\n",
      "loss: 1.558583  [51264/74412]\n",
      "loss: 1.356413  [57664/74412]\n",
      "loss: 1.640575  [64064/74412]\n",
      "loss: 1.452172  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.693391 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 1.967532  [   64/74412]\n",
      "loss: 1.560397  [ 6464/74412]\n",
      "loss: 1.516958  [12864/74412]\n",
      "loss: 1.400932  [19264/74412]\n",
      "loss: 1.530528  [25664/74412]\n",
      "loss: 1.961912  [32064/74412]\n",
      "loss: 1.581468  [38464/74412]\n",
      "loss: 1.430114  [44864/74412]\n",
      "loss: 1.554588  [51264/74412]\n",
      "loss: 1.354120  [57664/74412]\n",
      "loss: 1.637051  [64064/74412]\n",
      "loss: 1.447573  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 1.689786 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 1.963839  [   64/74412]\n",
      "loss: 1.556554  [ 6464/74412]\n",
      "loss: 1.511825  [12864/74412]\n",
      "loss: 1.397502  [19264/74412]\n",
      "loss: 1.526609  [25664/74412]\n",
      "loss: 1.959309  [32064/74412]\n",
      "loss: 1.577845  [38464/74412]\n",
      "loss: 1.426466  [44864/74412]\n",
      "loss: 1.550527  [51264/74412]\n",
      "loss: 1.352333  [57664/74412]\n",
      "loss: 1.633468  [64064/74412]\n",
      "loss: 1.442810  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.685805 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 1.959802  [   64/74412]\n",
      "loss: 1.553301  [ 6464/74412]\n",
      "loss: 1.506707  [12864/74412]\n",
      "loss: 1.393805  [19264/74412]\n",
      "loss: 1.522772  [25664/74412]\n",
      "loss: 1.956056  [32064/74412]\n",
      "loss: 1.574525  [38464/74412]\n",
      "loss: 1.422683  [44864/74412]\n",
      "loss: 1.546854  [51264/74412]\n",
      "loss: 1.350295  [57664/74412]\n",
      "loss: 1.629249  [64064/74412]\n",
      "loss: 1.437760  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 1.682044 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 1.956086  [   64/74412]\n",
      "loss: 1.549244  [ 6464/74412]\n",
      "loss: 1.501546  [12864/74412]\n",
      "loss: 1.390231  [19264/74412]\n",
      "loss: 1.519073  [25664/74412]\n",
      "loss: 1.953190  [32064/74412]\n",
      "loss: 1.570514  [38464/74412]\n",
      "loss: 1.418296  [44864/74412]\n",
      "loss: 1.543834  [51264/74412]\n",
      "loss: 1.348844  [57664/74412]\n",
      "loss: 1.625497  [64064/74412]\n",
      "loss: 1.433361  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.679021 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 1.953092  [   64/74412]\n",
      "loss: 1.545719  [ 6464/74412]\n",
      "loss: 1.496340  [12864/74412]\n",
      "loss: 1.386767  [19264/74412]\n",
      "loss: 1.515620  [25664/74412]\n",
      "loss: 1.950047  [32064/74412]\n",
      "loss: 1.567345  [38464/74412]\n",
      "loss: 1.415164  [44864/74412]\n",
      "loss: 1.540058  [51264/74412]\n",
      "loss: 1.347320  [57664/74412]\n",
      "loss: 1.621272  [64064/74412]\n",
      "loss: 1.429035  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.675981 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 1.949147  [   64/74412]\n",
      "loss: 1.542193  [ 6464/74412]\n",
      "loss: 1.491016  [12864/74412]\n",
      "loss: 1.383682  [19264/74412]\n",
      "loss: 1.512232  [25664/74412]\n",
      "loss: 1.947559  [32064/74412]\n",
      "loss: 1.563679  [38464/74412]\n",
      "loss: 1.410317  [44864/74412]\n",
      "loss: 1.536431  [51264/74412]\n",
      "loss: 1.345432  [57664/74412]\n",
      "loss: 1.617536  [64064/74412]\n",
      "loss: 1.424292  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 1.672542 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 1.945568  [   64/74412]\n",
      "loss: 1.538238  [ 6464/74412]\n",
      "loss: 1.485551  [12864/74412]\n",
      "loss: 1.380310  [19264/74412]\n",
      "loss: 1.508639  [25664/74412]\n",
      "loss: 1.945234  [32064/74412]\n",
      "loss: 1.561329  [38464/74412]\n",
      "loss: 1.407697  [44864/74412]\n",
      "loss: 1.533026  [51264/74412]\n",
      "loss: 1.343469  [57664/74412]\n",
      "loss: 1.613844  [64064/74412]\n",
      "loss: 1.419718  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 1.668953 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 1.942030  [   64/74412]\n",
      "loss: 1.534908  [ 6464/74412]\n",
      "loss: 1.480763  [12864/74412]\n",
      "loss: 1.377170  [19264/74412]\n",
      "loss: 1.505424  [25664/74412]\n",
      "loss: 1.941971  [32064/74412]\n",
      "loss: 1.557849  [38464/74412]\n",
      "loss: 1.404606  [44864/74412]\n",
      "loss: 1.530359  [51264/74412]\n",
      "loss: 1.341468  [57664/74412]\n",
      "loss: 1.609791  [64064/74412]\n",
      "loss: 1.415523  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.665528 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 1.938778  [   64/74412]\n",
      "loss: 1.531647  [ 6464/74412]\n",
      "loss: 1.476076  [12864/74412]\n",
      "loss: 1.374102  [19264/74412]\n",
      "loss: 1.502003  [25664/74412]\n",
      "loss: 1.939138  [32064/74412]\n",
      "loss: 1.554726  [38464/74412]\n",
      "loss: 1.401319  [44864/74412]\n",
      "loss: 1.527153  [51264/74412]\n",
      "loss: 1.339079  [57664/74412]\n",
      "loss: 1.606164  [64064/74412]\n",
      "loss: 1.411423  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 1.662213 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 1.935680  [   64/74412]\n",
      "loss: 1.527890  [ 6464/74412]\n",
      "loss: 1.471292  [12864/74412]\n",
      "loss: 1.370932  [19264/74412]\n",
      "loss: 1.498860  [25664/74412]\n",
      "loss: 1.937217  [32064/74412]\n",
      "loss: 1.551659  [38464/74412]\n",
      "loss: 1.397846  [44864/74412]\n",
      "loss: 1.524084  [51264/74412]\n",
      "loss: 1.336920  [57664/74412]\n",
      "loss: 1.602115  [64064/74412]\n",
      "loss: 1.408679  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.658522 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 1.932188  [   64/74412]\n",
      "loss: 1.524742  [ 6464/74412]\n",
      "loss: 1.466493  [12864/74412]\n",
      "loss: 1.368009  [19264/74412]\n",
      "loss: 1.495619  [25664/74412]\n",
      "loss: 1.934311  [32064/74412]\n",
      "loss: 1.548869  [38464/74412]\n",
      "loss: 1.393619  [44864/74412]\n",
      "loss: 1.521167  [51264/74412]\n",
      "loss: 1.335058  [57664/74412]\n",
      "loss: 1.598254  [64064/74412]\n",
      "loss: 1.404192  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.655469 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 1.929080  [   64/74412]\n",
      "loss: 1.521655  [ 6464/74412]\n",
      "loss: 1.461882  [12864/74412]\n",
      "loss: 1.364980  [19264/74412]\n",
      "loss: 1.492362  [25664/74412]\n",
      "loss: 1.932509  [32064/74412]\n",
      "loss: 1.545783  [38464/74412]\n",
      "loss: 1.390672  [44864/74412]\n",
      "loss: 1.517740  [51264/74412]\n",
      "loss: 1.332855  [57664/74412]\n",
      "loss: 1.594718  [64064/74412]\n",
      "loss: 1.399761  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.652039 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 1.925608  [   64/74412]\n",
      "loss: 1.518518  [ 6464/74412]\n",
      "loss: 1.456866  [12864/74412]\n",
      "loss: 1.362317  [19264/74412]\n",
      "loss: 1.489081  [25664/74412]\n",
      "loss: 1.928819  [32064/74412]\n",
      "loss: 1.543459  [38464/74412]\n",
      "loss: 1.387626  [44864/74412]\n",
      "loss: 1.514604  [51264/74412]\n",
      "loss: 1.330920  [57664/74412]\n",
      "loss: 1.591167  [64064/74412]\n",
      "loss: 1.395420  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.648818 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 1.922325  [   64/74412]\n",
      "loss: 1.515087  [ 6464/74412]\n",
      "loss: 1.452074  [12864/74412]\n",
      "loss: 1.359436  [19264/74412]\n",
      "loss: 1.487224  [25664/74412]\n",
      "loss: 1.926351  [32064/74412]\n",
      "loss: 1.540446  [38464/74412]\n",
      "loss: 1.384819  [44864/74412]\n",
      "loss: 1.511268  [51264/74412]\n",
      "loss: 1.329065  [57664/74412]\n",
      "loss: 1.587653  [64064/74412]\n",
      "loss: 1.391401  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 1.645593 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 1.919392  [   64/74412]\n",
      "loss: 1.511670  [ 6464/74412]\n",
      "loss: 1.447898  [12864/74412]\n",
      "loss: 1.356613  [19264/74412]\n",
      "loss: 1.483821  [25664/74412]\n",
      "loss: 1.924354  [32064/74412]\n",
      "loss: 1.537623  [38464/74412]\n",
      "loss: 1.381202  [44864/74412]\n",
      "loss: 1.506897  [51264/74412]\n",
      "loss: 1.327711  [57664/74412]\n",
      "loss: 1.584161  [64064/74412]\n",
      "loss: 1.386982  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.642570 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 1.916088  [   64/74412]\n",
      "loss: 1.508108  [ 6464/74412]\n",
      "loss: 1.443207  [12864/74412]\n",
      "loss: 1.353514  [19264/74412]\n",
      "loss: 1.480858  [25664/74412]\n",
      "loss: 1.921353  [32064/74412]\n",
      "loss: 1.534654  [38464/74412]\n",
      "loss: 1.378551  [44864/74412]\n",
      "loss: 1.503930  [51264/74412]\n",
      "loss: 1.325554  [57664/74412]\n",
      "loss: 1.580293  [64064/74412]\n",
      "loss: 1.383027  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 1.639889 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 1.913521  [   64/74412]\n",
      "loss: 1.505067  [ 6464/74412]\n",
      "loss: 1.438509  [12864/74412]\n",
      "loss: 1.350488  [19264/74412]\n",
      "loss: 1.477572  [25664/74412]\n",
      "loss: 1.918214  [32064/74412]\n",
      "loss: 1.531967  [38464/74412]\n",
      "loss: 1.375463  [44864/74412]\n",
      "loss: 1.501036  [51264/74412]\n",
      "loss: 1.323864  [57664/74412]\n",
      "loss: 1.576718  [64064/74412]\n",
      "loss: 1.378819  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 1.636777 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 1.910517  [   64/74412]\n",
      "loss: 1.501733  [ 6464/74412]\n",
      "loss: 1.433993  [12864/74412]\n",
      "loss: 1.347536  [19264/74412]\n",
      "loss: 1.473498  [25664/74412]\n",
      "loss: 1.916273  [32064/74412]\n",
      "loss: 1.530507  [38464/74412]\n",
      "loss: 1.372326  [44864/74412]\n",
      "loss: 1.497981  [51264/74412]\n",
      "loss: 1.322076  [57664/74412]\n",
      "loss: 1.573357  [64064/74412]\n",
      "loss: 1.374980  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.633324 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 1.907421  [   64/74412]\n",
      "loss: 1.498611  [ 6464/74412]\n",
      "loss: 1.429451  [12864/74412]\n",
      "loss: 1.345033  [19264/74412]\n",
      "loss: 1.470565  [25664/74412]\n",
      "loss: 1.913289  [32064/74412]\n",
      "loss: 1.527426  [38464/74412]\n",
      "loss: 1.368912  [44864/74412]\n",
      "loss: 1.495281  [51264/74412]\n",
      "loss: 1.320485  [57664/74412]\n",
      "loss: 1.569623  [64064/74412]\n",
      "loss: 1.371091  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.630597 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 1.904986  [   64/74412]\n",
      "loss: 1.495601  [ 6464/74412]\n",
      "loss: 1.424952  [12864/74412]\n",
      "loss: 1.342309  [19264/74412]\n",
      "loss: 1.468169  [25664/74412]\n",
      "loss: 1.910475  [32064/74412]\n",
      "loss: 1.524649  [38464/74412]\n",
      "loss: 1.366029  [44864/74412]\n",
      "loss: 1.492798  [51264/74412]\n",
      "loss: 1.318631  [57664/74412]\n",
      "loss: 1.566622  [64064/74412]\n",
      "loss: 1.366831  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 1.627320 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 1.901829  [   64/74412]\n",
      "loss: 1.492506  [ 6464/74412]\n",
      "loss: 1.420550  [12864/74412]\n",
      "loss: 1.339429  [19264/74412]\n",
      "loss: 1.465406  [25664/74412]\n",
      "loss: 1.906363  [32064/74412]\n",
      "loss: 1.521477  [38464/74412]\n",
      "loss: 1.363246  [44864/74412]\n",
      "loss: 1.489967  [51264/74412]\n",
      "loss: 1.316980  [57664/74412]\n",
      "loss: 1.563106  [64064/74412]\n",
      "loss: 1.362880  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.623858 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 1.898575  [   64/74412]\n",
      "loss: 1.489778  [ 6464/74412]\n",
      "loss: 1.416065  [12864/74412]\n",
      "loss: 1.336570  [19264/74412]\n",
      "loss: 1.462581  [25664/74412]\n",
      "loss: 1.903159  [32064/74412]\n",
      "loss: 1.518739  [38464/74412]\n",
      "loss: 1.360101  [44864/74412]\n",
      "loss: 1.487412  [51264/74412]\n",
      "loss: 1.315044  [57664/74412]\n",
      "loss: 1.559841  [64064/74412]\n",
      "loss: 1.358724  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.620770 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 1.895682  [   64/74412]\n",
      "loss: 1.486349  [ 6464/74412]\n",
      "loss: 1.411987  [12864/74412]\n",
      "loss: 1.334032  [19264/74412]\n",
      "loss: 1.460433  [25664/74412]\n",
      "loss: 1.900313  [32064/74412]\n",
      "loss: 1.516742  [38464/74412]\n",
      "loss: 1.357116  [44864/74412]\n",
      "loss: 1.484240  [51264/74412]\n",
      "loss: 1.313018  [57664/74412]\n",
      "loss: 1.556358  [64064/74412]\n",
      "loss: 1.355214  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 1.618089 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 1.893126  [   64/74412]\n",
      "loss: 1.483222  [ 6464/74412]\n",
      "loss: 1.407686  [12864/74412]\n",
      "loss: 1.331665  [19264/74412]\n",
      "loss: 1.457821  [25664/74412]\n",
      "loss: 1.897827  [32064/74412]\n",
      "loss: 1.513836  [38464/74412]\n",
      "loss: 1.353834  [44864/74412]\n",
      "loss: 1.481346  [51264/74412]\n",
      "loss: 1.311308  [57664/74412]\n",
      "loss: 1.553365  [64064/74412]\n",
      "loss: 1.350844  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 1.614660 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 1.889950  [   64/74412]\n",
      "loss: 1.479944  [ 6464/74412]\n",
      "loss: 1.403313  [12864/74412]\n",
      "loss: 1.328878  [19264/74412]\n",
      "loss: 1.455317  [25664/74412]\n",
      "loss: 1.893677  [32064/74412]\n",
      "loss: 1.511562  [38464/74412]\n",
      "loss: 1.349876  [44864/74412]\n",
      "loss: 1.477409  [51264/74412]\n",
      "loss: 1.309533  [57664/74412]\n",
      "loss: 1.551229  [64064/74412]\n",
      "loss: 1.347147  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.612477 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 1.887688  [   64/74412]\n",
      "loss: 1.476282  [ 6464/74412]\n",
      "loss: 1.399143  [12864/74412]\n",
      "loss: 1.326318  [19264/74412]\n",
      "loss: 1.452606  [25664/74412]\n",
      "loss: 1.892337  [32064/74412]\n",
      "loss: 1.508873  [38464/74412]\n",
      "loss: 1.347120  [44864/74412]\n",
      "loss: 1.474918  [51264/74412]\n",
      "loss: 1.307115  [57664/74412]\n",
      "loss: 1.548340  [64064/74412]\n",
      "loss: 1.343118  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 1.609286 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 1.884483  [   64/74412]\n",
      "loss: 1.473070  [ 6464/74412]\n",
      "loss: 1.395482  [12864/74412]\n",
      "loss: 1.323811  [19264/74412]\n",
      "loss: 1.449896  [25664/74412]\n",
      "loss: 1.890213  [32064/74412]\n",
      "loss: 1.506045  [38464/74412]\n",
      "loss: 1.343831  [44864/74412]\n",
      "loss: 1.473200  [51264/74412]\n",
      "loss: 1.307521  [57664/74412]\n",
      "loss: 1.545299  [64064/74412]\n",
      "loss: 1.339008  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.606338 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 1.881835  [   64/74412]\n",
      "loss: 1.469530  [ 6464/74412]\n",
      "loss: 1.391274  [12864/74412]\n",
      "loss: 1.321388  [19264/74412]\n",
      "loss: 1.447303  [25664/74412]\n",
      "loss: 1.886753  [32064/74412]\n",
      "loss: 1.503408  [38464/74412]\n",
      "loss: 1.341276  [44864/74412]\n",
      "loss: 1.469958  [51264/74412]\n",
      "loss: 1.305261  [57664/74412]\n",
      "loss: 1.542319  [64064/74412]\n",
      "loss: 1.335323  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.603290 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 1.879436  [   64/74412]\n",
      "loss: 1.465848  [ 6464/74412]\n",
      "loss: 1.387355  [12864/74412]\n",
      "loss: 1.319206  [19264/74412]\n",
      "loss: 1.444728  [25664/74412]\n",
      "loss: 1.883539  [32064/74412]\n",
      "loss: 1.501033  [38464/74412]\n",
      "loss: 1.337643  [44864/74412]\n",
      "loss: 1.467914  [51264/74412]\n",
      "loss: 1.303484  [57664/74412]\n",
      "loss: 1.539440  [64064/74412]\n",
      "loss: 1.331409  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.600188 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 1.876718  [   64/74412]\n",
      "loss: 1.462478  [ 6464/74412]\n",
      "loss: 1.383427  [12864/74412]\n",
      "loss: 1.316529  [19264/74412]\n",
      "loss: 1.442326  [25664/74412]\n",
      "loss: 1.882353  [32064/74412]\n",
      "loss: 1.498069  [38464/74412]\n",
      "loss: 1.334462  [44864/74412]\n",
      "loss: 1.465582  [51264/74412]\n",
      "loss: 1.301667  [57664/74412]\n",
      "loss: 1.536584  [64064/74412]\n",
      "loss: 1.326776  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.597097 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 1.873407  [   64/74412]\n",
      "loss: 1.458767  [ 6464/74412]\n",
      "loss: 1.379448  [12864/74412]\n",
      "loss: 1.313864  [19264/74412]\n",
      "loss: 1.439859  [25664/74412]\n",
      "loss: 1.879629  [32064/74412]\n",
      "loss: 1.495284  [38464/74412]\n",
      "loss: 1.331575  [44864/74412]\n",
      "loss: 1.463468  [51264/74412]\n",
      "loss: 1.299089  [57664/74412]\n",
      "loss: 1.533937  [64064/74412]\n",
      "loss: 1.323121  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 1.594229 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 1.871068  [   64/74412]\n",
      "loss: 1.455434  [ 6464/74412]\n",
      "loss: 1.375543  [12864/74412]\n",
      "loss: 1.311220  [19264/74412]\n",
      "loss: 1.437530  [25664/74412]\n",
      "loss: 1.876614  [32064/74412]\n",
      "loss: 1.492833  [38464/74412]\n",
      "loss: 1.329973  [44864/74412]\n",
      "loss: 1.460494  [51264/74412]\n",
      "loss: 1.296981  [57664/74412]\n",
      "loss: 1.530398  [64064/74412]\n",
      "loss: 1.320301  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 1.591376 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 1.868623  [   64/74412]\n",
      "loss: 1.452640  [ 6464/74412]\n",
      "loss: 1.371617  [12864/74412]\n",
      "loss: 1.308974  [19264/74412]\n",
      "loss: 1.435498  [25664/74412]\n",
      "loss: 1.873819  [32064/74412]\n",
      "loss: 1.490625  [38464/74412]\n",
      "loss: 1.326596  [44864/74412]\n",
      "loss: 1.458308  [51264/74412]\n",
      "loss: 1.294583  [57664/74412]\n",
      "loss: 1.527550  [64064/74412]\n",
      "loss: 1.316586  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.587988 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 1.865630  [   64/74412]\n",
      "loss: 1.449153  [ 6464/74412]\n",
      "loss: 1.367494  [12864/74412]\n",
      "loss: 1.306625  [19264/74412]\n",
      "loss: 1.433171  [25664/74412]\n",
      "loss: 1.871658  [32064/74412]\n",
      "loss: 1.488212  [38464/74412]\n",
      "loss: 1.323538  [44864/74412]\n",
      "loss: 1.456126  [51264/74412]\n",
      "loss: 1.292649  [57664/74412]\n",
      "loss: 1.524931  [64064/74412]\n",
      "loss: 1.312223  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 1.585352 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 1.863573  [   64/74412]\n",
      "loss: 1.445944  [ 6464/74412]\n",
      "loss: 1.362970  [12864/74412]\n",
      "loss: 1.304164  [19264/74412]\n",
      "loss: 1.430708  [25664/74412]\n",
      "loss: 1.868856  [32064/74412]\n",
      "loss: 1.485153  [38464/74412]\n",
      "loss: 1.320801  [44864/74412]\n",
      "loss: 1.454481  [51264/74412]\n",
      "loss: 1.290734  [57664/74412]\n",
      "loss: 1.522163  [64064/74412]\n",
      "loss: 1.308889  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 1.582456 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 1.861187  [   64/74412]\n",
      "loss: 1.442973  [ 6464/74412]\n",
      "loss: 1.358853  [12864/74412]\n",
      "loss: 1.301723  [19264/74412]\n",
      "loss: 1.428427  [25664/74412]\n",
      "loss: 1.866187  [32064/74412]\n",
      "loss: 1.482746  [38464/74412]\n",
      "loss: 1.317883  [44864/74412]\n",
      "loss: 1.452662  [51264/74412]\n",
      "loss: 1.288979  [57664/74412]\n",
      "loss: 1.519513  [64064/74412]\n",
      "loss: 1.304872  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.579416 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 1.858567  [   64/74412]\n",
      "loss: 1.440683  [ 6464/74412]\n",
      "loss: 1.354655  [12864/74412]\n",
      "loss: 1.299803  [19264/74412]\n",
      "loss: 1.426087  [25664/74412]\n",
      "loss: 1.862716  [32064/74412]\n",
      "loss: 1.480045  [38464/74412]\n",
      "loss: 1.314693  [44864/74412]\n",
      "loss: 1.450050  [51264/74412]\n",
      "loss: 1.287267  [57664/74412]\n",
      "loss: 1.516843  [64064/74412]\n",
      "loss: 1.301361  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.576220 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 1.855391  [   64/74412]\n",
      "loss: 1.437359  [ 6464/74412]\n",
      "loss: 1.350559  [12864/74412]\n",
      "loss: 1.297545  [19264/74412]\n",
      "loss: 1.423980  [25664/74412]\n",
      "loss: 1.860213  [32064/74412]\n",
      "loss: 1.477268  [38464/74412]\n",
      "loss: 1.312615  [44864/74412]\n",
      "loss: 1.449636  [51264/74412]\n",
      "loss: 1.285430  [57664/74412]\n",
      "loss: 1.514519  [64064/74412]\n",
      "loss: 1.298180  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.573660 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 1.853230  [   64/74412]\n",
      "loss: 1.434144  [ 6464/74412]\n",
      "loss: 1.346692  [12864/74412]\n",
      "loss: 1.295071  [19264/74412]\n",
      "loss: 1.421922  [25664/74412]\n",
      "loss: 1.857860  [32064/74412]\n",
      "loss: 1.474869  [38464/74412]\n",
      "loss: 1.308976  [44864/74412]\n",
      "loss: 1.445378  [51264/74412]\n",
      "loss: 1.283482  [57664/74412]\n",
      "loss: 1.512077  [64064/74412]\n",
      "loss: 1.294901  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.570893 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 1.851470  [   64/74412]\n",
      "loss: 1.430752  [ 6464/74412]\n",
      "loss: 1.343157  [12864/74412]\n",
      "loss: 1.292853  [19264/74412]\n",
      "loss: 1.419459  [25664/74412]\n",
      "loss: 1.854867  [32064/74412]\n",
      "loss: 1.472384  [38464/74412]\n",
      "loss: 1.306331  [44864/74412]\n",
      "loss: 1.443012  [51264/74412]\n",
      "loss: 1.281528  [57664/74412]\n",
      "loss: 1.509929  [64064/74412]\n",
      "loss: 1.291478  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 1.568424 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 1.849436  [   64/74412]\n",
      "loss: 1.427598  [ 6464/74412]\n",
      "loss: 1.339396  [12864/74412]\n",
      "loss: 1.290936  [19264/74412]\n",
      "loss: 1.417501  [25664/74412]\n",
      "loss: 1.851941  [32064/74412]\n",
      "loss: 1.469811  [38464/74412]\n",
      "loss: 1.304684  [44864/74412]\n",
      "loss: 1.440824  [51264/74412]\n",
      "loss: 1.279336  [57664/74412]\n",
      "loss: 1.507563  [64064/74412]\n",
      "loss: 1.288115  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 1.565694 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 1.847487  [   64/74412]\n",
      "loss: 1.424434  [ 6464/74412]\n",
      "loss: 1.335618  [12864/74412]\n",
      "loss: 1.288915  [19264/74412]\n",
      "loss: 1.415430  [25664/74412]\n",
      "loss: 1.849744  [32064/74412]\n",
      "loss: 1.467824  [38464/74412]\n",
      "loss: 1.301948  [44864/74412]\n",
      "loss: 1.439114  [51264/74412]\n",
      "loss: 1.275182  [57664/74412]\n",
      "loss: 1.505368  [64064/74412]\n",
      "loss: 1.284587  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.563016 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 1.845249  [   64/74412]\n",
      "loss: 1.421163  [ 6464/74412]\n",
      "loss: 1.331660  [12864/74412]\n",
      "loss: 1.286267  [19264/74412]\n",
      "loss: 1.413753  [25664/74412]\n",
      "loss: 1.846344  [32064/74412]\n",
      "loss: 1.465266  [38464/74412]\n",
      "loss: 1.298721  [44864/74412]\n",
      "loss: 1.436973  [51264/74412]\n",
      "loss: 1.273678  [57664/74412]\n",
      "loss: 1.503227  [64064/74412]\n",
      "loss: 1.281699  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.560173 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 1.843456  [   64/74412]\n",
      "loss: 1.417922  [ 6464/74412]\n",
      "loss: 1.327962  [12864/74412]\n",
      "loss: 1.283867  [19264/74412]\n",
      "loss: 1.412051  [25664/74412]\n",
      "loss: 1.843509  [32064/74412]\n",
      "loss: 1.462668  [38464/74412]\n",
      "loss: 1.297068  [44864/74412]\n",
      "loss: 1.435874  [51264/74412]\n",
      "loss: 1.272094  [57664/74412]\n",
      "loss: 1.501266  [64064/74412]\n",
      "loss: 1.278373  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.556523 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 1.839763  [   64/74412]\n",
      "loss: 1.414933  [ 6464/74412]\n",
      "loss: 1.324041  [12864/74412]\n",
      "loss: 1.282169  [19264/74412]\n",
      "loss: 1.410509  [25664/74412]\n",
      "loss: 1.840901  [32064/74412]\n",
      "loss: 1.460527  [38464/74412]\n",
      "loss: 1.294080  [44864/74412]\n",
      "loss: 1.433667  [51264/74412]\n",
      "loss: 1.270141  [57664/74412]\n",
      "loss: 1.499188  [64064/74412]\n",
      "loss: 1.275126  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.554714 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 1.838737  [   64/74412]\n",
      "loss: 1.412663  [ 6464/74412]\n",
      "loss: 1.320509  [12864/74412]\n",
      "loss: 1.279832  [19264/74412]\n",
      "loss: 1.408759  [25664/74412]\n",
      "loss: 1.838405  [32064/74412]\n",
      "loss: 1.458493  [38464/74412]\n",
      "loss: 1.291634  [44864/74412]\n",
      "loss: 1.433754  [51264/74412]\n",
      "loss: 1.268566  [57664/74412]\n",
      "loss: 1.496682  [64064/74412]\n",
      "loss: 1.271784  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 1.552378 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 1.837195  [   64/74412]\n",
      "loss: 1.409348  [ 6464/74412]\n",
      "loss: 1.317011  [12864/74412]\n",
      "loss: 1.277908  [19264/74412]\n",
      "loss: 1.406928  [25664/74412]\n",
      "loss: 1.836045  [32064/74412]\n",
      "loss: 1.456631  [38464/74412]\n",
      "loss: 1.289185  [44864/74412]\n",
      "loss: 1.431675  [51264/74412]\n",
      "loss: 1.265871  [57664/74412]\n",
      "loss: 1.494570  [64064/74412]\n",
      "loss: 1.268614  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.549721 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 1.834919  [   64/74412]\n",
      "loss: 1.406788  [ 6464/74412]\n",
      "loss: 1.313239  [12864/74412]\n",
      "loss: 1.275999  [19264/74412]\n",
      "loss: 1.404989  [25664/74412]\n",
      "loss: 1.832485  [32064/74412]\n",
      "loss: 1.454328  [38464/74412]\n",
      "loss: 1.286890  [44864/74412]\n",
      "loss: 1.429359  [51264/74412]\n",
      "loss: 1.264880  [57664/74412]\n",
      "loss: 1.492492  [64064/74412]\n",
      "loss: 1.265413  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.546355 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 1.832381  [   64/74412]\n",
      "loss: 1.403577  [ 6464/74412]\n",
      "loss: 1.309600  [12864/74412]\n",
      "loss: 1.273744  [19264/74412]\n",
      "loss: 1.403256  [25664/74412]\n",
      "loss: 1.829577  [32064/74412]\n",
      "loss: 1.452074  [38464/74412]\n",
      "loss: 1.284668  [44864/74412]\n",
      "loss: 1.427346  [51264/74412]\n",
      "loss: 1.262541  [57664/74412]\n",
      "loss: 1.490418  [64064/74412]\n",
      "loss: 1.261913  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.544620 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 1.831557  [   64/74412]\n",
      "loss: 1.400704  [ 6464/74412]\n",
      "loss: 1.305534  [12864/74412]\n",
      "loss: 1.271572  [19264/74412]\n",
      "loss: 1.401957  [25664/74412]\n",
      "loss: 1.827100  [32064/74412]\n",
      "loss: 1.449820  [38464/74412]\n",
      "loss: 1.282181  [44864/74412]\n",
      "loss: 1.425435  [51264/74412]\n",
      "loss: 1.261140  [57664/74412]\n",
      "loss: 1.488830  [64064/74412]\n",
      "loss: 1.259086  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.543539 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 1.831297  [   64/74412]\n",
      "loss: 1.398007  [ 6464/74412]\n",
      "loss: 1.301783  [12864/74412]\n",
      "loss: 1.269365  [19264/74412]\n",
      "loss: 1.400327  [25664/74412]\n",
      "loss: 1.824943  [32064/74412]\n",
      "loss: 1.447584  [38464/74412]\n",
      "loss: 1.279862  [44864/74412]\n",
      "loss: 1.422412  [51264/74412]\n",
      "loss: 1.258922  [57664/74412]\n",
      "loss: 1.486496  [64064/74412]\n",
      "loss: 1.254843  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.541260 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 1.829914  [   64/74412]\n",
      "loss: 1.395085  [ 6464/74412]\n",
      "loss: 1.297958  [12864/74412]\n",
      "loss: 1.267398  [19264/74412]\n",
      "loss: 1.398800  [25664/74412]\n",
      "loss: 1.823324  [32064/74412]\n",
      "loss: 1.445200  [38464/74412]\n",
      "loss: 1.276939  [44864/74412]\n",
      "loss: 1.420423  [51264/74412]\n",
      "loss: 1.256917  [57664/74412]\n",
      "loss: 1.484780  [64064/74412]\n",
      "loss: 1.251880  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.538262 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 1.828256  [   64/74412]\n",
      "loss: 1.392375  [ 6464/74412]\n",
      "loss: 1.294849  [12864/74412]\n",
      "loss: 1.265125  [19264/74412]\n",
      "loss: 1.397148  [25664/74412]\n",
      "loss: 1.820779  [32064/74412]\n",
      "loss: 1.443161  [38464/74412]\n",
      "loss: 1.274222  [44864/74412]\n",
      "loss: 1.418009  [51264/74412]\n",
      "loss: 1.254839  [57664/74412]\n",
      "loss: 1.482799  [64064/74412]\n",
      "loss: 1.248088  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.536006 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 1.826816  [   64/74412]\n",
      "loss: 1.389339  [ 6464/74412]\n",
      "loss: 1.291216  [12864/74412]\n",
      "loss: 1.263180  [19264/74412]\n",
      "loss: 1.395829  [25664/74412]\n",
      "loss: 1.818398  [32064/74412]\n",
      "loss: 1.438951  [38464/74412]\n",
      "loss: 1.271730  [44864/74412]\n",
      "loss: 1.415830  [51264/74412]\n",
      "loss: 1.252559  [57664/74412]\n",
      "loss: 1.481168  [64064/74412]\n",
      "loss: 1.244740  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.533984 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 1.825938  [   64/74412]\n",
      "loss: 1.386556  [ 6464/74412]\n",
      "loss: 1.287238  [12864/74412]\n",
      "loss: 1.261162  [19264/74412]\n",
      "loss: 1.394655  [25664/74412]\n",
      "loss: 1.815526  [32064/74412]\n",
      "loss: 1.439101  [38464/74412]\n",
      "loss: 1.269313  [44864/74412]\n",
      "loss: 1.413059  [51264/74412]\n",
      "loss: 1.250079  [57664/74412]\n",
      "loss: 1.479452  [64064/74412]\n",
      "loss: 1.242193  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.531827 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 1.824889  [   64/74412]\n",
      "loss: 1.384128  [ 6464/74412]\n",
      "loss: 1.283612  [12864/74412]\n",
      "loss: 1.258783  [19264/74412]\n",
      "loss: 1.393286  [25664/74412]\n",
      "loss: 1.813087  [32064/74412]\n",
      "loss: 1.437750  [38464/74412]\n",
      "loss: 1.266487  [44864/74412]\n",
      "loss: 1.409110  [51264/74412]\n",
      "loss: 1.248188  [57664/74412]\n",
      "loss: 1.477927  [64064/74412]\n",
      "loss: 1.238907  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.529449 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 1.823648  [   64/74412]\n",
      "loss: 1.381393  [ 6464/74412]\n",
      "loss: 1.280336  [12864/74412]\n",
      "loss: 1.256754  [19264/74412]\n",
      "loss: 1.391718  [25664/74412]\n",
      "loss: 1.809885  [32064/74412]\n",
      "loss: 1.434158  [38464/74412]\n",
      "loss: 1.264517  [44864/74412]\n",
      "loss: 1.406511  [51264/74412]\n",
      "loss: 1.246119  [57664/74412]\n",
      "loss: 1.475948  [64064/74412]\n",
      "loss: 1.236053  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.526579 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 1.821409  [   64/74412]\n",
      "loss: 1.378505  [ 6464/74412]\n",
      "loss: 1.277187  [12864/74412]\n",
      "loss: 1.255013  [19264/74412]\n",
      "loss: 1.390293  [25664/74412]\n",
      "loss: 1.808082  [32064/74412]\n",
      "loss: 1.432016  [38464/74412]\n",
      "loss: 1.261854  [44864/74412]\n",
      "loss: 1.403695  [51264/74412]\n",
      "loss: 1.244346  [57664/74412]\n",
      "loss: 1.475722  [64064/74412]\n",
      "loss: 1.233140  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.524435 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 1.820404  [   64/74412]\n",
      "loss: 1.376016  [ 6464/74412]\n",
      "loss: 1.273615  [12864/74412]\n",
      "loss: 1.253384  [19264/74412]\n",
      "loss: 1.388727  [25664/74412]\n",
      "loss: 1.805316  [32064/74412]\n",
      "loss: 1.430103  [38464/74412]\n",
      "loss: 1.259598  [44864/74412]\n",
      "loss: 1.401595  [51264/74412]\n",
      "loss: 1.242360  [57664/74412]\n",
      "loss: 1.474074  [64064/74412]\n",
      "loss: 1.230019  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 1.522220 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 1.819545  [   64/74412]\n",
      "loss: 1.373237  [ 6464/74412]\n",
      "loss: 1.270283  [12864/74412]\n",
      "loss: 1.250976  [19264/74412]\n",
      "loss: 1.387235  [25664/74412]\n",
      "loss: 1.802534  [32064/74412]\n",
      "loss: 1.427542  [38464/74412]\n",
      "loss: 1.257707  [44864/74412]\n",
      "loss: 1.399625  [51264/74412]\n",
      "loss: 1.240881  [57664/74412]\n",
      "loss: 1.471976  [64064/74412]\n",
      "loss: 1.226913  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 1.519864 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 1.817976  [   64/74412]\n",
      "loss: 1.370987  [ 6464/74412]\n",
      "loss: 1.266976  [12864/74412]\n",
      "loss: 1.249632  [19264/74412]\n",
      "loss: 1.385396  [25664/74412]\n",
      "loss: 1.799918  [32064/74412]\n",
      "loss: 1.425558  [38464/74412]\n",
      "loss: 1.256438  [44864/74412]\n",
      "loss: 1.397654  [51264/74412]\n",
      "loss: 1.239152  [57664/74412]\n",
      "loss: 1.469690  [64064/74412]\n",
      "loss: 1.224129  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 1.517659 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 1.816818  [   64/74412]\n",
      "loss: 1.368924  [ 6464/74412]\n",
      "loss: 1.263628  [12864/74412]\n",
      "loss: 1.247619  [19264/74412]\n",
      "loss: 1.383823  [25664/74412]\n",
      "loss: 1.797600  [32064/74412]\n",
      "loss: 1.423205  [38464/74412]\n",
      "loss: 1.253585  [44864/74412]\n",
      "loss: 1.395414  [51264/74412]\n",
      "loss: 1.237209  [57664/74412]\n",
      "loss: 1.468279  [64064/74412]\n",
      "loss: 1.221486  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.515428 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 1.815861  [   64/74412]\n",
      "loss: 1.366018  [ 6464/74412]\n",
      "loss: 1.261114  [12864/74412]\n",
      "loss: 1.246000  [19264/74412]\n",
      "loss: 1.382055  [25664/74412]\n",
      "loss: 1.795571  [32064/74412]\n",
      "loss: 1.421033  [38464/74412]\n",
      "loss: 1.250381  [44864/74412]\n",
      "loss: 1.393474  [51264/74412]\n",
      "loss: 1.235131  [57664/74412]\n",
      "loss: 1.466455  [64064/74412]\n",
      "loss: 1.218701  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.513085 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 1.814815  [   64/74412]\n",
      "loss: 1.363621  [ 6464/74412]\n",
      "loss: 1.257977  [12864/74412]\n",
      "loss: 1.243424  [19264/74412]\n",
      "loss: 1.380563  [25664/74412]\n",
      "loss: 1.793220  [32064/74412]\n",
      "loss: 1.419180  [38464/74412]\n",
      "loss: 1.248379  [44864/74412]\n",
      "loss: 1.391335  [51264/74412]\n",
      "loss: 1.235357  [57664/74412]\n",
      "loss: 1.464196  [64064/74412]\n",
      "loss: 1.215444  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.510774 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 1.813784  [   64/74412]\n",
      "loss: 1.361537  [ 6464/74412]\n",
      "loss: 1.254664  [12864/74412]\n",
      "loss: 1.242455  [19264/74412]\n",
      "loss: 1.379504  [25664/74412]\n",
      "loss: 1.791168  [32064/74412]\n",
      "loss: 1.417452  [38464/74412]\n",
      "loss: 1.246574  [44864/74412]\n",
      "loss: 1.389874  [51264/74412]\n",
      "loss: 1.232961  [57664/74412]\n",
      "loss: 1.462369  [64064/74412]\n",
      "loss: 1.212245  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.508584 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 1.812799  [   64/74412]\n",
      "loss: 1.359085  [ 6464/74412]\n",
      "loss: 1.251743  [12864/74412]\n",
      "loss: 1.240529  [19264/74412]\n",
      "loss: 1.378024  [25664/74412]\n",
      "loss: 1.789133  [32064/74412]\n",
      "loss: 1.415181  [38464/74412]\n",
      "loss: 1.244484  [44864/74412]\n",
      "loss: 1.388074  [51264/74412]\n",
      "loss: 1.231403  [57664/74412]\n",
      "loss: 1.460392  [64064/74412]\n",
      "loss: 1.209563  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.506330 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 1.811798  [   64/74412]\n",
      "loss: 1.356202  [ 6464/74412]\n",
      "loss: 1.248430  [12864/74412]\n",
      "loss: 1.239376  [19264/74412]\n",
      "loss: 1.376386  [25664/74412]\n",
      "loss: 1.786686  [32064/74412]\n",
      "loss: 1.412561  [38464/74412]\n",
      "loss: 1.242615  [44864/74412]\n",
      "loss: 1.386421  [51264/74412]\n",
      "loss: 1.229468  [57664/74412]\n",
      "loss: 1.458355  [64064/74412]\n",
      "loss: 1.206850  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.503749 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 1.810219  [   64/74412]\n",
      "loss: 1.353287  [ 6464/74412]\n",
      "loss: 1.245239  [12864/74412]\n",
      "loss: 1.237378  [19264/74412]\n",
      "loss: 1.374992  [25664/74412]\n",
      "loss: 1.783896  [32064/74412]\n",
      "loss: 1.410811  [38464/74412]\n",
      "loss: 1.240724  [44864/74412]\n",
      "loss: 1.384131  [51264/74412]\n",
      "loss: 1.226119  [57664/74412]\n",
      "loss: 1.456921  [64064/74412]\n",
      "loss: 1.203948  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.501698 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 1.809462  [   64/74412]\n",
      "loss: 1.350328  [ 6464/74412]\n",
      "loss: 1.241992  [12864/74412]\n",
      "loss: 1.234883  [19264/74412]\n",
      "loss: 1.373803  [25664/74412]\n",
      "loss: 1.781009  [32064/74412]\n",
      "loss: 1.409203  [38464/74412]\n",
      "loss: 1.239021  [44864/74412]\n",
      "loss: 1.382160  [51264/74412]\n",
      "loss: 1.223620  [57664/74412]\n",
      "loss: 1.455374  [64064/74412]\n",
      "loss: 1.201121  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.499494 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 1.808274  [   64/74412]\n",
      "loss: 1.348261  [ 6464/74412]\n",
      "loss: 1.238822  [12864/74412]\n",
      "loss: 1.233118  [19264/74412]\n",
      "loss: 1.372207  [25664/74412]\n",
      "loss: 1.778871  [32064/74412]\n",
      "loss: 1.406606  [38464/74412]\n",
      "loss: 1.237249  [44864/74412]\n",
      "loss: 1.380271  [51264/74412]\n",
      "loss: 1.221847  [57664/74412]\n",
      "loss: 1.453387  [64064/74412]\n",
      "loss: 1.198344  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.497404 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 1.807515  [   64/74412]\n",
      "loss: 1.345652  [ 6464/74412]\n",
      "loss: 1.235319  [12864/74412]\n",
      "loss: 1.231452  [19264/74412]\n",
      "loss: 1.371091  [25664/74412]\n",
      "loss: 1.775995  [32064/74412]\n",
      "loss: 1.405182  [38464/74412]\n",
      "loss: 1.234622  [44864/74412]\n",
      "loss: 1.378034  [51264/74412]\n",
      "loss: 1.220030  [57664/74412]\n",
      "loss: 1.452018  [64064/74412]\n",
      "loss: 1.196122  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.495232 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 1.806621  [   64/74412]\n",
      "loss: 1.342738  [ 6464/74412]\n",
      "loss: 1.232706  [12864/74412]\n",
      "loss: 1.229705  [19264/74412]\n",
      "loss: 1.369815  [25664/74412]\n",
      "loss: 1.773641  [32064/74412]\n",
      "loss: 1.403273  [38464/74412]\n",
      "loss: 1.233833  [44864/74412]\n",
      "loss: 1.376256  [51264/74412]\n",
      "loss: 1.218106  [57664/74412]\n",
      "loss: 1.450186  [64064/74412]\n",
      "loss: 1.193250  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.493687 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 1.806043  [   64/74412]\n",
      "loss: 1.340277  [ 6464/74412]\n",
      "loss: 1.230116  [12864/74412]\n",
      "loss: 1.228154  [19264/74412]\n",
      "loss: 1.368613  [25664/74412]\n",
      "loss: 1.770914  [32064/74412]\n",
      "loss: 1.401742  [38464/74412]\n",
      "loss: 1.231913  [44864/74412]\n",
      "loss: 1.374440  [51264/74412]\n",
      "loss: 1.216344  [57664/74412]\n",
      "loss: 1.448778  [64064/74412]\n",
      "loss: 1.190802  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.491404 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 1.805176  [   64/74412]\n",
      "loss: 1.337526  [ 6464/74412]\n",
      "loss: 1.227119  [12864/74412]\n",
      "loss: 1.226692  [19264/74412]\n",
      "loss: 1.367533  [25664/74412]\n",
      "loss: 1.768932  [32064/74412]\n",
      "loss: 1.399504  [38464/74412]\n",
      "loss: 1.230001  [44864/74412]\n",
      "loss: 1.373078  [51264/74412]\n",
      "loss: 1.214301  [57664/74412]\n",
      "loss: 1.446890  [64064/74412]\n",
      "loss: 1.188309  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.489837 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 1.804988  [   64/74412]\n",
      "loss: 1.335529  [ 6464/74412]\n",
      "loss: 1.223863  [12864/74412]\n",
      "loss: 1.225679  [19264/74412]\n",
      "loss: 1.366038  [25664/74412]\n",
      "loss: 1.766458  [32064/74412]\n",
      "loss: 1.397940  [38464/74412]\n",
      "loss: 1.228787  [44864/74412]\n",
      "loss: 1.371359  [51264/74412]\n",
      "loss: 1.212309  [57664/74412]\n",
      "loss: 1.445617  [64064/74412]\n",
      "loss: 1.184951  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 1.487546 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 1.803738  [   64/74412]\n",
      "loss: 1.333145  [ 6464/74412]\n",
      "loss: 1.220875  [12864/74412]\n",
      "loss: 1.224482  [19264/74412]\n",
      "loss: 1.364771  [25664/74412]\n",
      "loss: 1.763928  [32064/74412]\n",
      "loss: 1.396729  [38464/74412]\n",
      "loss: 1.227069  [44864/74412]\n",
      "loss: 1.369415  [51264/74412]\n",
      "loss: 1.210754  [57664/74412]\n",
      "loss: 1.444095  [64064/74412]\n",
      "loss: 1.182429  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 1.485766 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 1.803010  [   64/74412]\n",
      "loss: 1.331369  [ 6464/74412]\n",
      "loss: 1.217650  [12864/74412]\n",
      "loss: 1.223678  [19264/74412]\n",
      "loss: 1.363296  [25664/74412]\n",
      "loss: 1.762468  [32064/74412]\n",
      "loss: 1.394081  [38464/74412]\n",
      "loss: 1.224557  [44864/74412]\n",
      "loss: 1.367041  [51264/74412]\n",
      "loss: 1.208014  [57664/74412]\n",
      "loss: 1.442967  [64064/74412]\n",
      "loss: 1.179702  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.483842 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 1.801952  [   64/74412]\n",
      "loss: 1.328720  [ 6464/74412]\n",
      "loss: 1.215088  [12864/74412]\n",
      "loss: 1.222544  [19264/74412]\n",
      "loss: 1.361387  [25664/74412]\n",
      "loss: 1.759748  [32064/74412]\n",
      "loss: 1.392408  [38464/74412]\n",
      "loss: 1.222930  [44864/74412]\n",
      "loss: 1.366536  [51264/74412]\n",
      "loss: 1.206076  [57664/74412]\n",
      "loss: 1.441228  [64064/74412]\n",
      "loss: 1.176654  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.481837 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 1.801240  [   64/74412]\n",
      "loss: 1.326361  [ 6464/74412]\n",
      "loss: 1.212346  [12864/74412]\n",
      "loss: 1.221628  [19264/74412]\n",
      "loss: 1.360419  [25664/74412]\n",
      "loss: 1.757332  [32064/74412]\n",
      "loss: 1.390448  [38464/74412]\n",
      "loss: 1.221926  [44864/74412]\n",
      "loss: 1.363020  [51264/74412]\n",
      "loss: 1.204136  [57664/74412]\n",
      "loss: 1.439689  [64064/74412]\n",
      "loss: 1.174634  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 1.480022 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 1.801216  [   64/74412]\n",
      "loss: 1.324161  [ 6464/74412]\n",
      "loss: 1.209301  [12864/74412]\n",
      "loss: 1.219579  [19264/74412]\n",
      "loss: 1.358471  [25664/74412]\n",
      "loss: 1.754679  [32064/74412]\n",
      "loss: 1.388526  [38464/74412]\n",
      "loss: 1.220783  [44864/74412]\n",
      "loss: 1.361606  [51264/74412]\n",
      "loss: 1.202103  [57664/74412]\n",
      "loss: 1.438818  [64064/74412]\n",
      "loss: 1.171507  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 1.477575 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 1.799291  [   64/74412]\n",
      "loss: 1.321661  [ 6464/74412]\n",
      "loss: 1.206406  [12864/74412]\n",
      "loss: 1.218393  [19264/74412]\n",
      "loss: 1.357512  [25664/74412]\n",
      "loss: 1.751635  [32064/74412]\n",
      "loss: 1.386723  [38464/74412]\n",
      "loss: 1.219256  [44864/74412]\n",
      "loss: 1.359711  [51264/74412]\n",
      "loss: 1.200304  [57664/74412]\n",
      "loss: 1.437160  [64064/74412]\n",
      "loss: 1.168771  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.475050 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 1.797884  [   64/74412]\n",
      "loss: 1.319300  [ 6464/74412]\n",
      "loss: 1.203284  [12864/74412]\n",
      "loss: 1.216979  [19264/74412]\n",
      "loss: 1.356510  [25664/74412]\n",
      "loss: 1.749339  [32064/74412]\n",
      "loss: 1.385769  [38464/74412]\n",
      "loss: 1.217583  [44864/74412]\n",
      "loss: 1.358042  [51264/74412]\n",
      "loss: 1.198559  [57664/74412]\n",
      "loss: 1.436369  [64064/74412]\n",
      "loss: 1.165926  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.472663 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 1.796687  [   64/74412]\n",
      "loss: 1.316885  [ 6464/74412]\n",
      "loss: 1.200825  [12864/74412]\n",
      "loss: 1.215652  [19264/74412]\n",
      "loss: 1.354824  [25664/74412]\n",
      "loss: 1.746931  [32064/74412]\n",
      "loss: 1.384357  [38464/74412]\n",
      "loss: 1.216218  [44864/74412]\n",
      "loss: 1.355693  [51264/74412]\n",
      "loss: 1.196460  [57664/74412]\n",
      "loss: 1.434500  [64064/74412]\n",
      "loss: 1.163470  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.471413 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 1.796726  [   64/74412]\n",
      "loss: 1.314454  [ 6464/74412]\n",
      "loss: 1.198393  [12864/74412]\n",
      "loss: 1.214248  [19264/74412]\n",
      "loss: 1.353863  [25664/74412]\n",
      "loss: 1.744950  [32064/74412]\n",
      "loss: 1.382896  [38464/74412]\n",
      "loss: 1.215581  [44864/74412]\n",
      "loss: 1.354148  [51264/74412]\n",
      "loss: 1.194282  [57664/74412]\n",
      "loss: 1.433532  [64064/74412]\n",
      "loss: 1.160958  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.468910 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 1.795382  [   64/74412]\n",
      "loss: 1.312204  [ 6464/74412]\n",
      "loss: 1.195394  [12864/74412]\n",
      "loss: 1.212826  [19264/74412]\n",
      "loss: 1.352358  [25664/74412]\n",
      "loss: 1.741934  [32064/74412]\n",
      "loss: 1.380882  [38464/74412]\n",
      "loss: 1.214793  [44864/74412]\n",
      "loss: 1.352316  [51264/74412]\n",
      "loss: 1.192172  [57664/74412]\n",
      "loss: 1.432287  [64064/74412]\n",
      "loss: 1.158135  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.467121 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 1.794243  [   64/74412]\n",
      "loss: 1.309730  [ 6464/74412]\n",
      "loss: 1.193135  [12864/74412]\n",
      "loss: 1.211536  [19264/74412]\n",
      "loss: 1.350805  [25664/74412]\n",
      "loss: 1.738983  [32064/74412]\n",
      "loss: 1.379479  [38464/74412]\n",
      "loss: 1.213855  [44864/74412]\n",
      "loss: 1.350568  [51264/74412]\n",
      "loss: 1.190141  [57664/74412]\n",
      "loss: 1.430778  [64064/74412]\n",
      "loss: 1.155207  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.465394 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 1.793365  [   64/74412]\n",
      "loss: 1.307681  [ 6464/74412]\n",
      "loss: 1.189970  [12864/74412]\n",
      "loss: 1.210326  [19264/74412]\n",
      "loss: 1.349472  [25664/74412]\n",
      "loss: 1.737407  [32064/74412]\n",
      "loss: 1.377626  [38464/74412]\n",
      "loss: 1.212463  [44864/74412]\n",
      "loss: 1.349824  [51264/74412]\n",
      "loss: 1.188853  [57664/74412]\n",
      "loss: 1.429618  [64064/74412]\n",
      "loss: 1.152645  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.463654 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 1.792966  [   64/74412]\n",
      "loss: 1.305021  [ 6464/74412]\n",
      "loss: 1.187475  [12864/74412]\n",
      "loss: 1.209591  [19264/74412]\n",
      "loss: 1.348164  [25664/74412]\n",
      "loss: 1.734219  [32064/74412]\n",
      "loss: 1.376198  [38464/74412]\n",
      "loss: 1.212066  [44864/74412]\n",
      "loss: 1.347553  [51264/74412]\n",
      "loss: 1.186364  [57664/74412]\n",
      "loss: 1.428482  [64064/74412]\n",
      "loss: 1.150156  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.461676 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 1.792001  [   64/74412]\n",
      "loss: 1.302451  [ 6464/74412]\n",
      "loss: 1.184561  [12864/74412]\n",
      "loss: 1.208112  [19264/74412]\n",
      "loss: 1.347050  [25664/74412]\n",
      "loss: 1.731752  [32064/74412]\n",
      "loss: 1.374905  [38464/74412]\n",
      "loss: 1.211573  [44864/74412]\n",
      "loss: 1.345599  [51264/74412]\n",
      "loss: 1.184719  [57664/74412]\n",
      "loss: 1.427365  [64064/74412]\n",
      "loss: 1.147277  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.459921 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 1.791817  [   64/74412]\n",
      "loss: 1.300188  [ 6464/74412]\n",
      "loss: 1.181812  [12864/74412]\n",
      "loss: 1.207138  [19264/74412]\n",
      "loss: 1.345858  [25664/74412]\n",
      "loss: 1.728631  [32064/74412]\n",
      "loss: 1.372716  [38464/74412]\n",
      "loss: 1.210178  [44864/74412]\n",
      "loss: 1.344484  [51264/74412]\n",
      "loss: 1.181938  [57664/74412]\n",
      "loss: 1.426427  [64064/74412]\n",
      "loss: 1.145131  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.458686 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 1.791421  [   64/74412]\n",
      "loss: 1.298114  [ 6464/74412]\n",
      "loss: 1.179036  [12864/74412]\n",
      "loss: 1.205379  [19264/74412]\n",
      "loss: 1.344876  [25664/74412]\n",
      "loss: 1.726479  [32064/74412]\n",
      "loss: 1.370963  [38464/74412]\n",
      "loss: 1.208724  [44864/74412]\n",
      "loss: 1.342625  [51264/74412]\n",
      "loss: 1.180263  [57664/74412]\n",
      "loss: 1.424926  [64064/74412]\n",
      "loss: 1.142324  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.453242 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 1.784749  [   64/74412]\n",
      "loss: 1.295732  [ 6464/74412]\n",
      "loss: 1.176232  [12864/74412]\n",
      "loss: 1.204544  [19264/74412]\n",
      "loss: 1.343899  [25664/74412]\n",
      "loss: 1.723904  [32064/74412]\n",
      "loss: 1.369707  [38464/74412]\n",
      "loss: 1.206733  [44864/74412]\n",
      "loss: 1.341408  [51264/74412]\n",
      "loss: 1.178923  [57664/74412]\n",
      "loss: 1.423967  [64064/74412]\n",
      "loss: 1.139944  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.451248 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 1.784106  [   64/74412]\n",
      "loss: 1.293535  [ 6464/74412]\n",
      "loss: 1.173771  [12864/74412]\n",
      "loss: 1.202963  [19264/74412]\n",
      "loss: 1.342935  [25664/74412]\n",
      "loss: 1.721769  [32064/74412]\n",
      "loss: 1.367731  [38464/74412]\n",
      "loss: 1.205852  [44864/74412]\n",
      "loss: 1.342146  [51264/74412]\n",
      "loss: 1.177210  [57664/74412]\n",
      "loss: 1.422978  [64064/74412]\n",
      "loss: 1.137547  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.449086 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 1.782910  [   64/74412]\n",
      "loss: 1.291092  [ 6464/74412]\n",
      "loss: 1.170754  [12864/74412]\n",
      "loss: 1.201705  [19264/74412]\n",
      "loss: 1.341955  [25664/74412]\n",
      "loss: 1.719198  [32064/74412]\n",
      "loss: 1.365896  [38464/74412]\n",
      "loss: 1.204924  [44864/74412]\n",
      "loss: 1.340310  [51264/74412]\n",
      "loss: 1.174702  [57664/74412]\n",
      "loss: 1.422091  [64064/74412]\n",
      "loss: 1.134946  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.447663 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 1.783243  [   64/74412]\n",
      "loss: 1.288649  [ 6464/74412]\n",
      "loss: 1.168771  [12864/74412]\n",
      "loss: 1.199168  [19264/74412]\n",
      "loss: 1.340809  [25664/74412]\n",
      "loss: 1.717335  [32064/74412]\n",
      "loss: 1.364254  [38464/74412]\n",
      "loss: 1.203467  [44864/74412]\n",
      "loss: 1.337278  [51264/74412]\n",
      "loss: 1.172873  [57664/74412]\n",
      "loss: 1.421209  [64064/74412]\n",
      "loss: 1.132813  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.446097 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 1.782048  [   64/74412]\n",
      "loss: 1.286335  [ 6464/74412]\n",
      "loss: 1.165931  [12864/74412]\n",
      "loss: 1.197919  [19264/74412]\n",
      "loss: 1.338706  [25664/74412]\n",
      "loss: 1.715080  [32064/74412]\n",
      "loss: 1.363164  [38464/74412]\n",
      "loss: 1.202384  [44864/74412]\n",
      "loss: 1.335553  [51264/74412]\n",
      "loss: 1.171033  [57664/74412]\n",
      "loss: 1.419619  [64064/74412]\n",
      "loss: 1.130916  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.444645 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 1.781265  [   64/74412]\n",
      "loss: 1.283912  [ 6464/74412]\n",
      "loss: 1.163062  [12864/74412]\n",
      "loss: 1.197006  [19264/74412]\n",
      "loss: 1.337825  [25664/74412]\n",
      "loss: 1.712820  [32064/74412]\n",
      "loss: 1.361207  [38464/74412]\n",
      "loss: 1.201052  [44864/74412]\n",
      "loss: 1.333583  [51264/74412]\n",
      "loss: 1.169284  [57664/74412]\n",
      "loss: 1.418537  [64064/74412]\n",
      "loss: 1.128549  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 1.442377 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 1.779646  [   64/74412]\n",
      "loss: 1.281655  [ 6464/74412]\n",
      "loss: 1.160815  [12864/74412]\n",
      "loss: 1.196671  [19264/74412]\n",
      "loss: 1.336795  [25664/74412]\n",
      "loss: 1.710510  [32064/74412]\n",
      "loss: 1.359525  [38464/74412]\n",
      "loss: 1.200200  [44864/74412]\n",
      "loss: 1.332176  [51264/74412]\n",
      "loss: 1.167605  [57664/74412]\n",
      "loss: 1.417397  [64064/74412]\n",
      "loss: 1.126317  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 1.440779 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 1.779120  [   64/74412]\n",
      "loss: 1.279534  [ 6464/74412]\n",
      "loss: 1.158415  [12864/74412]\n",
      "loss: 1.194834  [19264/74412]\n",
      "loss: 1.335977  [25664/74412]\n",
      "loss: 1.707803  [32064/74412]\n",
      "loss: 1.357786  [38464/74412]\n",
      "loss: 1.199475  [44864/74412]\n",
      "loss: 1.330053  [51264/74412]\n",
      "loss: 1.165518  [57664/74412]\n",
      "loss: 1.416420  [64064/74412]\n",
      "loss: 1.123921  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 1.439689 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 1.779720  [   64/74412]\n",
      "loss: 1.277299  [ 6464/74412]\n",
      "loss: 1.155883  [12864/74412]\n",
      "loss: 1.193843  [19264/74412]\n",
      "loss: 1.335039  [25664/74412]\n",
      "loss: 1.706123  [32064/74412]\n",
      "loss: 1.355967  [38464/74412]\n",
      "loss: 1.197856  [44864/74412]\n",
      "loss: 1.328572  [51264/74412]\n",
      "loss: 1.163254  [57664/74412]\n",
      "loss: 1.415267  [64064/74412]\n",
      "loss: 1.121381  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.438092 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 1.778805  [   64/74412]\n",
      "loss: 1.275066  [ 6464/74412]\n",
      "loss: 1.153702  [12864/74412]\n",
      "loss: 1.192448  [19264/74412]\n",
      "loss: 1.333617  [25664/74412]\n",
      "loss: 1.703715  [32064/74412]\n",
      "loss: 1.354429  [38464/74412]\n",
      "loss: 1.196422  [44864/74412]\n",
      "loss: 1.326839  [51264/74412]\n",
      "loss: 1.162334  [57664/74412]\n",
      "loss: 1.414025  [64064/74412]\n",
      "loss: 1.118314  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.436193 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 1.778386  [   64/74412]\n",
      "loss: 1.272761  [ 6464/74412]\n",
      "loss: 1.151645  [12864/74412]\n",
      "loss: 1.191455  [19264/74412]\n",
      "loss: 1.332506  [25664/74412]\n",
      "loss: 1.701369  [32064/74412]\n",
      "loss: 1.353147  [38464/74412]\n",
      "loss: 1.195082  [44864/74412]\n",
      "loss: 1.325514  [51264/74412]\n",
      "loss: 1.159553  [57664/74412]\n",
      "loss: 1.413341  [64064/74412]\n",
      "loss: 1.117143  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 1.434231 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 1.776656  [   64/74412]\n",
      "loss: 1.271169  [ 6464/74412]\n",
      "loss: 1.148887  [12864/74412]\n",
      "loss: 1.189585  [19264/74412]\n",
      "loss: 1.331362  [25664/74412]\n",
      "loss: 1.699115  [32064/74412]\n",
      "loss: 1.351595  [38464/74412]\n",
      "loss: 1.194604  [44864/74412]\n",
      "loss: 1.323439  [51264/74412]\n",
      "loss: 1.157510  [57664/74412]\n",
      "loss: 1.411890  [64064/74412]\n",
      "loss: 1.114601  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 1.432646 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 1.775506  [   64/74412]\n",
      "loss: 1.268879  [ 6464/74412]\n",
      "loss: 1.146702  [12864/74412]\n",
      "loss: 1.188490  [19264/74412]\n",
      "loss: 1.330468  [25664/74412]\n",
      "loss: 1.696731  [32064/74412]\n",
      "loss: 1.350129  [38464/74412]\n",
      "loss: 1.193823  [44864/74412]\n",
      "loss: 1.321630  [51264/74412]\n",
      "loss: 1.156392  [57664/74412]\n",
      "loss: 1.410831  [64064/74412]\n",
      "loss: 1.112319  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.431216 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 1.774619  [   64/74412]\n",
      "loss: 1.266808  [ 6464/74412]\n",
      "loss: 1.144452  [12864/74412]\n",
      "loss: 1.187572  [19264/74412]\n",
      "loss: 1.329688  [25664/74412]\n",
      "loss: 1.693479  [32064/74412]\n",
      "loss: 1.349155  [38464/74412]\n",
      "loss: 1.193465  [44864/74412]\n",
      "loss: 1.319729  [51264/74412]\n",
      "loss: 1.154322  [57664/74412]\n",
      "loss: 1.408947  [64064/74412]\n",
      "loss: 1.109707  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.429708 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 1.774654  [   64/74412]\n",
      "loss: 1.264328  [ 6464/74412]\n",
      "loss: 1.142304  [12864/74412]\n",
      "loss: 1.186555  [19264/74412]\n",
      "loss: 1.328749  [25664/74412]\n",
      "loss: 1.691593  [32064/74412]\n",
      "loss: 1.348355  [38464/74412]\n",
      "loss: 1.192244  [44864/74412]\n",
      "loss: 1.318014  [51264/74412]\n",
      "loss: 1.152314  [57664/74412]\n",
      "loss: 1.407920  [64064/74412]\n",
      "loss: 1.108267  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 1.427985 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 1.773823  [   64/74412]\n",
      "loss: 1.262842  [ 6464/74412]\n",
      "loss: 1.139384  [12864/74412]\n",
      "loss: 1.186411  [19264/74412]\n",
      "loss: 1.328081  [25664/74412]\n",
      "loss: 1.688593  [32064/74412]\n",
      "loss: 1.346948  [38464/74412]\n",
      "loss: 1.191225  [44864/74412]\n",
      "loss: 1.317523  [51264/74412]\n",
      "loss: 1.150636  [57664/74412]\n",
      "loss: 1.406576  [64064/74412]\n",
      "loss: 1.105196  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 1.426363 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 1.772789  [   64/74412]\n",
      "loss: 1.260640  [ 6464/74412]\n",
      "loss: 1.137390  [12864/74412]\n",
      "loss: 1.185319  [19264/74412]\n",
      "loss: 1.326952  [25664/74412]\n",
      "loss: 1.686687  [32064/74412]\n",
      "loss: 1.345871  [38464/74412]\n",
      "loss: 1.190404  [44864/74412]\n",
      "loss: 1.314765  [51264/74412]\n",
      "loss: 1.149056  [57664/74412]\n",
      "loss: 1.405871  [64064/74412]\n",
      "loss: 1.103040  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.424304 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 1.771328  [   64/74412]\n",
      "loss: 1.258344  [ 6464/74412]\n",
      "loss: 1.136028  [12864/74412]\n",
      "loss: 1.184284  [19264/74412]\n",
      "loss: 1.325840  [25664/74412]\n",
      "loss: 1.684050  [32064/74412]\n",
      "loss: 1.344538  [38464/74412]\n",
      "loss: 1.189006  [44864/74412]\n",
      "loss: 1.313347  [51264/74412]\n",
      "loss: 1.147050  [57664/74412]\n",
      "loss: 1.404295  [64064/74412]\n",
      "loss: 1.101008  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.422962 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 1.771082  [   64/74412]\n",
      "loss: 1.256580  [ 6464/74412]\n",
      "loss: 1.134220  [12864/74412]\n",
      "loss: 1.182682  [19264/74412]\n",
      "loss: 1.324928  [25664/74412]\n",
      "loss: 1.681078  [32064/74412]\n",
      "loss: 1.343274  [38464/74412]\n",
      "loss: 1.188397  [44864/74412]\n",
      "loss: 1.311210  [51264/74412]\n",
      "loss: 1.145287  [57664/74412]\n",
      "loss: 1.403665  [64064/74412]\n",
      "loss: 1.098749  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.421280 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 1.769704  [   64/74412]\n",
      "loss: 1.253927  [ 6464/74412]\n",
      "loss: 1.131829  [12864/74412]\n",
      "loss: 1.182292  [19264/74412]\n",
      "loss: 1.323528  [25664/74412]\n",
      "loss: 1.678200  [32064/74412]\n",
      "loss: 1.342019  [38464/74412]\n",
      "loss: 1.187370  [44864/74412]\n",
      "loss: 1.309540  [51264/74412]\n",
      "loss: 1.143646  [57664/74412]\n",
      "loss: 1.402244  [64064/74412]\n",
      "loss: 1.096308  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.419783 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 1.768866  [   64/74412]\n",
      "loss: 1.251941  [ 6464/74412]\n",
      "loss: 1.129927  [12864/74412]\n",
      "loss: 1.179632  [19264/74412]\n",
      "loss: 1.322974  [25664/74412]\n",
      "loss: 1.675601  [32064/74412]\n",
      "loss: 1.340234  [38464/74412]\n",
      "loss: 1.186142  [44864/74412]\n",
      "loss: 1.307815  [51264/74412]\n",
      "loss: 1.142007  [57664/74412]\n",
      "loss: 1.401374  [64064/74412]\n",
      "loss: 1.094846  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.418682 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 1.768584  [   64/74412]\n",
      "loss: 1.250422  [ 6464/74412]\n",
      "loss: 1.128007  [12864/74412]\n",
      "loss: 1.178876  [19264/74412]\n",
      "loss: 1.322933  [25664/74412]\n",
      "loss: 1.672637  [32064/74412]\n",
      "loss: 1.339535  [38464/74412]\n",
      "loss: 1.185096  [44864/74412]\n",
      "loss: 1.306140  [51264/74412]\n",
      "loss: 1.139923  [57664/74412]\n",
      "loss: 1.400957  [64064/74412]\n",
      "loss: 1.092095  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.416582 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 1.766272  [   64/74412]\n",
      "loss: 1.248449  [ 6464/74412]\n",
      "loss: 1.125701  [12864/74412]\n",
      "loss: 1.177926  [19264/74412]\n",
      "loss: 1.321815  [25664/74412]\n",
      "loss: 1.670234  [32064/74412]\n",
      "loss: 1.338156  [38464/74412]\n",
      "loss: 1.184019  [44864/74412]\n",
      "loss: 1.304295  [51264/74412]\n",
      "loss: 1.137956  [57664/74412]\n",
      "loss: 1.400116  [64064/74412]\n",
      "loss: 1.089774  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.415204 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 1.765648  [   64/74412]\n",
      "loss: 1.246470  [ 6464/74412]\n",
      "loss: 1.123560  [12864/74412]\n",
      "loss: 1.177051  [19264/74412]\n",
      "loss: 1.319974  [25664/74412]\n",
      "loss: 1.668573  [32064/74412]\n",
      "loss: 1.337307  [38464/74412]\n",
      "loss: 1.182914  [44864/74412]\n",
      "loss: 1.302877  [51264/74412]\n",
      "loss: 1.136282  [57664/74412]\n",
      "loss: 1.398982  [64064/74412]\n",
      "loss: 1.087376  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.414893 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 1.765976  [   64/74412]\n",
      "loss: 1.244572  [ 6464/74412]\n",
      "loss: 1.122198  [12864/74412]\n",
      "loss: 1.176482  [19264/74412]\n",
      "loss: 1.319615  [25664/74412]\n",
      "loss: 1.665667  [32064/74412]\n",
      "loss: 1.335400  [38464/74412]\n",
      "loss: 1.181785  [44864/74412]\n",
      "loss: 1.300712  [51264/74412]\n",
      "loss: 1.135702  [57664/74412]\n",
      "loss: 1.398357  [64064/74412]\n",
      "loss: 1.085169  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.412278 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 1.764114  [   64/74412]\n",
      "loss: 1.242579  [ 6464/74412]\n",
      "loss: 1.120102  [12864/74412]\n",
      "loss: 1.175330  [19264/74412]\n",
      "loss: 1.319461  [25664/74412]\n",
      "loss: 1.663480  [32064/74412]\n",
      "loss: 1.334315  [38464/74412]\n",
      "loss: 1.180993  [44864/74412]\n",
      "loss: 1.299201  [51264/74412]\n",
      "loss: 1.133924  [57664/74412]\n",
      "loss: 1.397054  [64064/74412]\n",
      "loss: 1.083082  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.412192 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 1.765153  [   64/74412]\n",
      "loss: 1.240816  [ 6464/74412]\n",
      "loss: 1.117628  [12864/74412]\n",
      "loss: 1.174478  [19264/74412]\n",
      "loss: 1.317446  [25664/74412]\n",
      "loss: 1.660123  [32064/74412]\n",
      "loss: 1.333002  [38464/74412]\n",
      "loss: 1.179685  [44864/74412]\n",
      "loss: 1.297335  [51264/74412]\n",
      "loss: 1.132366  [57664/74412]\n",
      "loss: 1.395923  [64064/74412]\n",
      "loss: 1.081640  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.409521 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 1.763301  [   64/74412]\n",
      "loss: 1.239390  [ 6464/74412]\n",
      "loss: 1.115548  [12864/74412]\n",
      "loss: 1.173077  [19264/74412]\n",
      "loss: 1.317746  [25664/74412]\n",
      "loss: 1.657980  [32064/74412]\n",
      "loss: 1.331933  [38464/74412]\n",
      "loss: 1.178770  [44864/74412]\n",
      "loss: 1.295945  [51264/74412]\n",
      "loss: 1.130687  [57664/74412]\n",
      "loss: 1.394842  [64064/74412]\n",
      "loss: 1.079501  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.407390 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 1.761253  [   64/74412]\n",
      "loss: 1.237329  [ 6464/74412]\n",
      "loss: 1.113963  [12864/74412]\n",
      "loss: 1.172130  [19264/74412]\n",
      "loss: 1.315234  [25664/74412]\n",
      "loss: 1.655242  [32064/74412]\n",
      "loss: 1.330867  [38464/74412]\n",
      "loss: 1.177989  [44864/74412]\n",
      "loss: 1.295172  [51264/74412]\n",
      "loss: 1.128990  [57664/74412]\n",
      "loss: 1.393668  [64064/74412]\n",
      "loss: 1.077706  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 1.407208 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 1.761922  [   64/74412]\n",
      "loss: 1.235042  [ 6464/74412]\n",
      "loss: 1.111844  [12864/74412]\n",
      "loss: 1.170449  [19264/74412]\n",
      "loss: 1.315427  [25664/74412]\n",
      "loss: 1.653461  [32064/74412]\n",
      "loss: 1.329597  [38464/74412]\n",
      "loss: 1.176857  [44864/74412]\n",
      "loss: 1.293647  [51264/74412]\n",
      "loss: 1.125969  [57664/74412]\n",
      "loss: 1.392288  [64064/74412]\n",
      "loss: 1.075676  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 1.405853 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 1.761420  [   64/74412]\n",
      "loss: 1.233358  [ 6464/74412]\n",
      "loss: 1.109928  [12864/74412]\n",
      "loss: 1.169708  [19264/74412]\n",
      "loss: 1.314711  [25664/74412]\n",
      "loss: 1.651185  [32064/74412]\n",
      "loss: 1.328576  [38464/74412]\n",
      "loss: 1.175611  [44864/74412]\n",
      "loss: 1.291781  [51264/74412]\n",
      "loss: 1.125910  [57664/74412]\n",
      "loss: 1.391290  [64064/74412]\n",
      "loss: 1.073608  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.404429 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 1.761042  [   64/74412]\n",
      "loss: 1.231712  [ 6464/74412]\n",
      "loss: 1.108299  [12864/74412]\n",
      "loss: 1.168236  [19264/74412]\n",
      "loss: 1.314217  [25664/74412]\n",
      "loss: 1.648726  [32064/74412]\n",
      "loss: 1.328067  [38464/74412]\n",
      "loss: 1.174385  [44864/74412]\n",
      "loss: 1.290203  [51264/74412]\n",
      "loss: 1.124989  [57664/74412]\n",
      "loss: 1.389448  [64064/74412]\n",
      "loss: 1.071439  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.403526 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 1.761652  [   64/74412]\n",
      "loss: 1.229914  [ 6464/74412]\n",
      "loss: 1.106947  [12864/74412]\n",
      "loss: 1.167472  [19264/74412]\n",
      "loss: 1.313133  [25664/74412]\n",
      "loss: 1.646245  [32064/74412]\n",
      "loss: 1.327358  [38464/74412]\n",
      "loss: 1.172946  [44864/74412]\n",
      "loss: 1.288189  [51264/74412]\n",
      "loss: 1.122588  [57664/74412]\n",
      "loss: 1.388181  [64064/74412]\n",
      "loss: 1.070117  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.401488 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 1.760448  [   64/74412]\n",
      "loss: 1.228331  [ 6464/74412]\n",
      "loss: 1.104624  [12864/74412]\n",
      "loss: 1.166236  [19264/74412]\n",
      "loss: 1.312298  [25664/74412]\n",
      "loss: 1.644951  [32064/74412]\n",
      "loss: 1.325674  [38464/74412]\n",
      "loss: 1.171914  [44864/74412]\n",
      "loss: 1.286104  [51264/74412]\n",
      "loss: 1.121407  [57664/74412]\n",
      "loss: 1.387002  [64064/74412]\n",
      "loss: 1.067523  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.400664 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 1.760244  [   64/74412]\n",
      "loss: 1.226159  [ 6464/74412]\n",
      "loss: 1.103578  [12864/74412]\n",
      "loss: 1.165337  [19264/74412]\n",
      "loss: 1.310783  [25664/74412]\n",
      "loss: 1.642471  [32064/74412]\n",
      "loss: 1.324486  [38464/74412]\n",
      "loss: 1.170679  [44864/74412]\n",
      "loss: 1.285217  [51264/74412]\n",
      "loss: 1.119392  [57664/74412]\n",
      "loss: 1.386878  [64064/74412]\n",
      "loss: 1.065149  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.399026 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 1.759247  [   64/74412]\n",
      "loss: 1.224422  [ 6464/74412]\n",
      "loss: 1.101366  [12864/74412]\n",
      "loss: 1.164516  [19264/74412]\n",
      "loss: 1.309744  [25664/74412]\n",
      "loss: 1.639188  [32064/74412]\n",
      "loss: 1.324252  [38464/74412]\n",
      "loss: 1.169889  [44864/74412]\n",
      "loss: 1.283278  [51264/74412]\n",
      "loss: 1.117947  [57664/74412]\n",
      "loss: 1.385722  [64064/74412]\n",
      "loss: 1.062855  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.397489 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 1.758115  [   64/74412]\n",
      "loss: 1.222379  [ 6464/74412]\n",
      "loss: 1.099535  [12864/74412]\n",
      "loss: 1.164018  [19264/74412]\n",
      "loss: 1.308363  [25664/74412]\n",
      "loss: 1.636999  [32064/74412]\n",
      "loss: 1.323131  [38464/74412]\n",
      "loss: 1.168737  [44864/74412]\n",
      "loss: 1.281468  [51264/74412]\n",
      "loss: 1.116334  [57664/74412]\n",
      "loss: 1.384798  [64064/74412]\n",
      "loss: 1.061198  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.396698 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 1.757761  [   64/74412]\n",
      "loss: 1.220731  [ 6464/74412]\n",
      "loss: 1.097750  [12864/74412]\n",
      "loss: 1.162846  [19264/74412]\n",
      "loss: 1.307991  [25664/74412]\n",
      "loss: 1.634367  [32064/74412]\n",
      "loss: 1.321142  [38464/74412]\n",
      "loss: 1.168169  [44864/74412]\n",
      "loss: 1.279841  [51264/74412]\n",
      "loss: 1.114698  [57664/74412]\n",
      "loss: 1.383816  [64064/74412]\n",
      "loss: 1.058874  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.394758 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 1.756148  [   64/74412]\n",
      "loss: 1.218863  [ 6464/74412]\n",
      "loss: 1.096705  [12864/74412]\n",
      "loss: 1.161990  [19264/74412]\n",
      "loss: 1.307302  [25664/74412]\n",
      "loss: 1.632095  [32064/74412]\n",
      "loss: 1.320375  [38464/74412]\n",
      "loss: 1.167039  [44864/74412]\n",
      "loss: 1.278380  [51264/74412]\n",
      "loss: 1.112853  [57664/74412]\n",
      "loss: 1.383051  [64064/74412]\n",
      "loss: 1.056839  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.393274 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 1.754959  [   64/74412]\n",
      "loss: 1.216832  [ 6464/74412]\n",
      "loss: 1.094662  [12864/74412]\n",
      "loss: 1.161198  [19264/74412]\n",
      "loss: 1.306068  [25664/74412]\n",
      "loss: 1.629323  [32064/74412]\n",
      "loss: 1.320004  [38464/74412]\n",
      "loss: 1.165247  [44864/74412]\n",
      "loss: 1.276178  [51264/74412]\n",
      "loss: 1.111479  [57664/74412]\n",
      "loss: 1.381984  [64064/74412]\n",
      "loss: 1.054887  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.391557 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 1.753492  [   64/74412]\n",
      "loss: 1.215159  [ 6464/74412]\n",
      "loss: 1.093020  [12864/74412]\n",
      "loss: 1.160219  [19264/74412]\n",
      "loss: 1.304948  [25664/74412]\n",
      "loss: 1.625793  [32064/74412]\n",
      "loss: 1.318426  [38464/74412]\n",
      "loss: 1.164515  [44864/74412]\n",
      "loss: 1.274724  [51264/74412]\n",
      "loss: 1.109419  [57664/74412]\n",
      "loss: 1.381160  [64064/74412]\n",
      "loss: 1.053115  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.390190 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 1.752602  [   64/74412]\n",
      "loss: 1.212939  [ 6464/74412]\n",
      "loss: 1.091737  [12864/74412]\n",
      "loss: 1.158896  [19264/74412]\n",
      "loss: 1.304633  [25664/74412]\n",
      "loss: 1.622916  [32064/74412]\n",
      "loss: 1.318504  [38464/74412]\n",
      "loss: 1.163133  [44864/74412]\n",
      "loss: 1.272908  [51264/74412]\n",
      "loss: 1.107480  [57664/74412]\n",
      "loss: 1.379917  [64064/74412]\n",
      "loss: 1.050695  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.389202 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 1.752688  [   64/74412]\n",
      "loss: 1.210861  [ 6464/74412]\n",
      "loss: 1.090342  [12864/74412]\n",
      "loss: 1.158239  [19264/74412]\n",
      "loss: 1.303879  [25664/74412]\n",
      "loss: 1.620590  [32064/74412]\n",
      "loss: 1.316274  [38464/74412]\n",
      "loss: 1.162482  [44864/74412]\n",
      "loss: 1.270914  [51264/74412]\n",
      "loss: 1.106676  [57664/74412]\n",
      "loss: 1.378457  [64064/74412]\n",
      "loss: 1.048762  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.387781 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 1.751660  [   64/74412]\n",
      "loss: 1.209268  [ 6464/74412]\n",
      "loss: 1.089072  [12864/74412]\n",
      "loss: 1.157231  [19264/74412]\n",
      "loss: 1.302897  [25664/74412]\n",
      "loss: 1.618178  [32064/74412]\n",
      "loss: 1.315573  [38464/74412]\n",
      "loss: 1.161384  [44864/74412]\n",
      "loss: 1.269010  [51264/74412]\n",
      "loss: 1.104849  [57664/74412]\n",
      "loss: 1.377311  [64064/74412]\n",
      "loss: 1.047355  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.386295 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 1.750766  [   64/74412]\n",
      "loss: 1.207597  [ 6464/74412]\n",
      "loss: 1.087739  [12864/74412]\n",
      "loss: 1.155819  [19264/74412]\n",
      "loss: 1.302187  [25664/74412]\n",
      "loss: 1.615346  [32064/74412]\n",
      "loss: 1.314486  [38464/74412]\n",
      "loss: 1.160617  [44864/74412]\n",
      "loss: 1.267051  [51264/74412]\n",
      "loss: 1.103256  [57664/74412]\n",
      "loss: 1.376240  [64064/74412]\n",
      "loss: 1.044996  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.384741 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 1.749672  [   64/74412]\n",
      "loss: 1.205704  [ 6464/74412]\n",
      "loss: 1.086499  [12864/74412]\n",
      "loss: 1.154732  [19264/74412]\n",
      "loss: 1.301087  [25664/74412]\n",
      "loss: 1.612377  [32064/74412]\n",
      "loss: 1.313839  [38464/74412]\n",
      "loss: 1.159588  [44864/74412]\n",
      "loss: 1.265873  [51264/74412]\n",
      "loss: 1.102164  [57664/74412]\n",
      "loss: 1.375719  [64064/74412]\n",
      "loss: 1.043368  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.383368 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 1.748860  [   64/74412]\n",
      "loss: 1.204005  [ 6464/74412]\n",
      "loss: 1.085540  [12864/74412]\n",
      "loss: 1.153501  [19264/74412]\n",
      "loss: 1.300800  [25664/74412]\n",
      "loss: 1.609733  [32064/74412]\n",
      "loss: 1.312687  [38464/74412]\n",
      "loss: 1.158880  [44864/74412]\n",
      "loss: 1.264645  [51264/74412]\n",
      "loss: 1.100903  [57664/74412]\n",
      "loss: 1.374921  [64064/74412]\n",
      "loss: 1.041757  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.382063 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 1.747536  [   64/74412]\n",
      "loss: 1.202358  [ 6464/74412]\n",
      "loss: 1.084477  [12864/74412]\n",
      "loss: 1.152482  [19264/74412]\n",
      "loss: 1.299677  [25664/74412]\n",
      "loss: 1.606949  [32064/74412]\n",
      "loss: 1.311657  [38464/74412]\n",
      "loss: 1.157594  [44864/74412]\n",
      "loss: 1.262550  [51264/74412]\n",
      "loss: 1.098652  [57664/74412]\n",
      "loss: 1.373423  [64064/74412]\n",
      "loss: 1.040763  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 1.380826 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 1.747318  [   64/74412]\n",
      "loss: 1.200064  [ 6464/74412]\n",
      "loss: 1.083090  [12864/74412]\n",
      "loss: 1.151432  [19264/74412]\n",
      "loss: 1.299009  [25664/74412]\n",
      "loss: 1.604994  [32064/74412]\n",
      "loss: 1.310402  [38464/74412]\n",
      "loss: 1.156504  [44864/74412]\n",
      "loss: 1.260822  [51264/74412]\n",
      "loss: 1.096927  [57664/74412]\n",
      "loss: 1.372713  [64064/74412]\n",
      "loss: 1.038991  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.379117 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 1.745558  [   64/74412]\n",
      "loss: 1.198614  [ 6464/74412]\n",
      "loss: 1.082149  [12864/74412]\n",
      "loss: 1.150206  [19264/74412]\n",
      "loss: 1.298307  [25664/74412]\n",
      "loss: 1.602054  [32064/74412]\n",
      "loss: 1.309265  [38464/74412]\n",
      "loss: 1.155634  [44864/74412]\n",
      "loss: 1.259009  [51264/74412]\n",
      "loss: 1.095316  [57664/74412]\n",
      "loss: 1.371126  [64064/74412]\n",
      "loss: 1.036991  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.378151 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 1.744745  [   64/74412]\n",
      "loss: 1.196495  [ 6464/74412]\n",
      "loss: 1.080811  [12864/74412]\n",
      "loss: 1.149147  [19264/74412]\n",
      "loss: 1.297128  [25664/74412]\n",
      "loss: 1.599972  [32064/74412]\n",
      "loss: 1.308720  [38464/74412]\n",
      "loss: 1.154145  [44864/74412]\n",
      "loss: 1.257497  [51264/74412]\n",
      "loss: 1.093822  [57664/74412]\n",
      "loss: 1.370350  [64064/74412]\n",
      "loss: 1.035433  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 1.376325 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 1.743868  [   64/74412]\n",
      "loss: 1.194597  [ 6464/74412]\n",
      "loss: 1.079730  [12864/74412]\n",
      "loss: 1.148072  [19264/74412]\n",
      "loss: 1.296443  [25664/74412]\n",
      "loss: 1.597347  [32064/74412]\n",
      "loss: 1.307602  [38464/74412]\n",
      "loss: 1.153410  [44864/74412]\n",
      "loss: 1.256214  [51264/74412]\n",
      "loss: 1.092058  [57664/74412]\n",
      "loss: 1.369494  [64064/74412]\n",
      "loss: 1.033426  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.375226 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 1.743204  [   64/74412]\n",
      "loss: 1.193346  [ 6464/74412]\n",
      "loss: 1.078546  [12864/74412]\n",
      "loss: 1.147362  [19264/74412]\n",
      "loss: 1.295389  [25664/74412]\n",
      "loss: 1.595850  [32064/74412]\n",
      "loss: 1.306364  [38464/74412]\n",
      "loss: 1.152543  [44864/74412]\n",
      "loss: 1.254761  [51264/74412]\n",
      "loss: 1.090548  [57664/74412]\n",
      "loss: 1.369071  [64064/74412]\n",
      "loss: 1.031764  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 1.374267 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 1.742290  [   64/74412]\n",
      "loss: 1.191446  [ 6464/74412]\n",
      "loss: 1.077606  [12864/74412]\n",
      "loss: 1.146310  [19264/74412]\n",
      "loss: 1.294862  [25664/74412]\n",
      "loss: 1.592997  [32064/74412]\n",
      "loss: 1.305282  [38464/74412]\n",
      "loss: 1.151352  [44864/74412]\n",
      "loss: 1.253240  [51264/74412]\n",
      "loss: 1.088598  [57664/74412]\n",
      "loss: 1.368085  [64064/74412]\n",
      "loss: 1.029341  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 1.372898 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 1.740683  [   64/74412]\n",
      "loss: 1.190019  [ 6464/74412]\n",
      "loss: 1.076448  [12864/74412]\n",
      "loss: 1.145526  [19264/74412]\n",
      "loss: 1.294345  [25664/74412]\n",
      "loss: 1.590660  [32064/74412]\n",
      "loss: 1.303454  [38464/74412]\n",
      "loss: 1.149551  [44864/74412]\n",
      "loss: 1.251502  [51264/74412]\n",
      "loss: 1.087020  [57664/74412]\n",
      "loss: 1.367401  [64064/74412]\n",
      "loss: 1.027888  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 1.371444 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 1.739897  [   64/74412]\n",
      "loss: 1.188401  [ 6464/74412]\n",
      "loss: 1.074721  [12864/74412]\n",
      "loss: 1.144526  [19264/74412]\n",
      "loss: 1.293874  [25664/74412]\n",
      "loss: 1.587784  [32064/74412]\n",
      "loss: 1.303123  [38464/74412]\n",
      "loss: 1.148525  [44864/74412]\n",
      "loss: 1.250571  [51264/74412]\n",
      "loss: 1.085133  [57664/74412]\n",
      "loss: 1.366495  [64064/74412]\n",
      "loss: 1.025452  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 1.370833 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 1.740245  [   64/74412]\n",
      "loss: 1.186726  [ 6464/74412]\n",
      "loss: 1.073649  [12864/74412]\n",
      "loss: 1.143378  [19264/74412]\n",
      "loss: 1.292821  [25664/74412]\n",
      "loss: 1.585455  [32064/74412]\n",
      "loss: 1.302347  [38464/74412]\n",
      "loss: 1.147320  [44864/74412]\n",
      "loss: 1.249017  [51264/74412]\n",
      "loss: 1.083382  [57664/74412]\n",
      "loss: 1.365814  [64064/74412]\n",
      "loss: 1.024951  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 1.369125 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 1.739087  [   64/74412]\n",
      "loss: 1.185017  [ 6464/74412]\n",
      "loss: 1.072850  [12864/74412]\n",
      "loss: 1.142629  [19264/74412]\n",
      "loss: 1.291866  [25664/74412]\n",
      "loss: 1.582343  [32064/74412]\n",
      "loss: 1.301300  [38464/74412]\n",
      "loss: 1.146791  [44864/74412]\n",
      "loss: 1.247452  [51264/74412]\n",
      "loss: 1.082351  [57664/74412]\n",
      "loss: 1.363778  [64064/74412]\n",
      "loss: 1.022918  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.367395 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 1.737727  [   64/74412]\n",
      "loss: 1.182030  [ 6464/74412]\n",
      "loss: 1.071700  [12864/74412]\n",
      "loss: 1.141449  [19264/74412]\n",
      "loss: 1.290805  [25664/74412]\n",
      "loss: 1.579884  [32064/74412]\n",
      "loss: 1.300193  [38464/74412]\n",
      "loss: 1.145334  [44864/74412]\n",
      "loss: 1.246227  [51264/74412]\n",
      "loss: 1.080219  [57664/74412]\n",
      "loss: 1.363085  [64064/74412]\n",
      "loss: 1.021500  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.366412 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 1.737391  [   64/74412]\n",
      "loss: 1.181162  [ 6464/74412]\n",
      "loss: 1.070866  [12864/74412]\n",
      "loss: 1.140223  [19264/74412]\n",
      "loss: 1.290179  [25664/74412]\n",
      "loss: 1.577778  [32064/74412]\n",
      "loss: 1.299345  [38464/74412]\n",
      "loss: 1.144444  [44864/74412]\n",
      "loss: 1.244802  [51264/74412]\n",
      "loss: 1.078224  [57664/74412]\n",
      "loss: 1.362832  [64064/74412]\n",
      "loss: 1.019723  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.364974 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 1.736158  [   64/74412]\n",
      "loss: 1.178656  [ 6464/74412]\n",
      "loss: 1.069803  [12864/74412]\n",
      "loss: 1.139059  [19264/74412]\n",
      "loss: 1.289208  [25664/74412]\n",
      "loss: 1.575149  [32064/74412]\n",
      "loss: 1.298581  [38464/74412]\n",
      "loss: 1.143093  [44864/74412]\n",
      "loss: 1.243683  [51264/74412]\n",
      "loss: 1.076548  [57664/74412]\n",
      "loss: 1.362092  [64064/74412]\n",
      "loss: 1.018074  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.363861 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 1.735346  [   64/74412]\n",
      "loss: 1.177422  [ 6464/74412]\n",
      "loss: 1.068646  [12864/74412]\n",
      "loss: 1.138702  [19264/74412]\n",
      "loss: 1.287897  [25664/74412]\n",
      "loss: 1.572270  [32064/74412]\n",
      "loss: 1.297438  [38464/74412]\n",
      "loss: 1.141494  [44864/74412]\n",
      "loss: 1.242894  [51264/74412]\n",
      "loss: 1.075038  [57664/74412]\n",
      "loss: 1.360141  [64064/74412]\n",
      "loss: 1.016912  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.362765 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 1.734313  [   64/74412]\n",
      "loss: 1.176467  [ 6464/74412]\n",
      "loss: 1.067466  [12864/74412]\n",
      "loss: 1.137177  [19264/74412]\n",
      "loss: 1.286831  [25664/74412]\n",
      "loss: 1.570689  [32064/74412]\n",
      "loss: 1.296112  [38464/74412]\n",
      "loss: 1.140682  [44864/74412]\n",
      "loss: 1.241442  [51264/74412]\n",
      "loss: 1.073237  [57664/74412]\n",
      "loss: 1.359558  [64064/74412]\n",
      "loss: 1.015274  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.361157 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 1.732892  [   64/74412]\n",
      "loss: 1.174500  [ 6464/74412]\n",
      "loss: 1.066260  [12864/74412]\n",
      "loss: 1.136966  [19264/74412]\n",
      "loss: 1.286086  [25664/74412]\n",
      "loss: 1.567593  [32064/74412]\n",
      "loss: 1.295445  [38464/74412]\n",
      "loss: 1.139293  [44864/74412]\n",
      "loss: 1.239651  [51264/74412]\n",
      "loss: 1.071154  [57664/74412]\n",
      "loss: 1.358792  [64064/74412]\n",
      "loss: 1.013567  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.359669 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 1.731870  [   64/74412]\n",
      "loss: 1.173311  [ 6464/74412]\n",
      "loss: 1.064978  [12864/74412]\n",
      "loss: 1.135827  [19264/74412]\n",
      "loss: 1.285104  [25664/74412]\n",
      "loss: 1.565361  [32064/74412]\n",
      "loss: 1.294857  [38464/74412]\n",
      "loss: 1.138374  [44864/74412]\n",
      "loss: 1.237949  [51264/74412]\n",
      "loss: 1.069000  [57664/74412]\n",
      "loss: 1.357627  [64064/74412]\n",
      "loss: 1.011889  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.358506 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 1.731012  [   64/74412]\n",
      "loss: 1.171428  [ 6464/74412]\n",
      "loss: 1.064178  [12864/74412]\n",
      "loss: 1.134729  [19264/74412]\n",
      "loss: 1.284846  [25664/74412]\n",
      "loss: 1.563023  [32064/74412]\n",
      "loss: 1.292994  [38464/74412]\n",
      "loss: 1.137310  [44864/74412]\n",
      "loss: 1.236855  [51264/74412]\n",
      "loss: 1.067376  [57664/74412]\n",
      "loss: 1.356659  [64064/74412]\n",
      "loss: 1.010103  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.357105 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 1.729311  [   64/74412]\n",
      "loss: 1.169618  [ 6464/74412]\n",
      "loss: 1.063116  [12864/74412]\n",
      "loss: 1.133992  [19264/74412]\n",
      "loss: 1.283334  [25664/74412]\n",
      "loss: 1.560846  [32064/74412]\n",
      "loss: 1.291907  [38464/74412]\n",
      "loss: 1.135431  [44864/74412]\n",
      "loss: 1.235047  [51264/74412]\n",
      "loss: 1.065637  [57664/74412]\n",
      "loss: 1.355078  [64064/74412]\n",
      "loss: 1.008783  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 1.355702 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 1.728740  [   64/74412]\n",
      "loss: 1.168437  [ 6464/74412]\n",
      "loss: 1.062286  [12864/74412]\n",
      "loss: 1.132241  [19264/74412]\n",
      "loss: 1.282660  [25664/74412]\n",
      "loss: 1.557987  [32064/74412]\n",
      "loss: 1.290018  [38464/74412]\n",
      "loss: 1.134438  [44864/74412]\n",
      "loss: 1.233765  [51264/74412]\n",
      "loss: 1.063800  [57664/74412]\n",
      "loss: 1.353867  [64064/74412]\n",
      "loss: 1.007509  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 1.354277 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 1.727803  [   64/74412]\n",
      "loss: 1.166977  [ 6464/74412]\n",
      "loss: 1.060903  [12864/74412]\n",
      "loss: 1.132053  [19264/74412]\n",
      "loss: 1.282159  [25664/74412]\n",
      "loss: 1.555262  [32064/74412]\n",
      "loss: 1.289163  [38464/74412]\n",
      "loss: 1.133621  [44864/74412]\n",
      "loss: 1.232126  [51264/74412]\n",
      "loss: 1.062735  [57664/74412]\n",
      "loss: 1.352929  [64064/74412]\n",
      "loss: 1.006015  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 1.352653 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 1.726459  [   64/74412]\n",
      "loss: 1.165761  [ 6464/74412]\n",
      "loss: 1.060147  [12864/74412]\n",
      "loss: 1.130533  [19264/74412]\n",
      "loss: 1.281179  [25664/74412]\n",
      "loss: 1.552658  [32064/74412]\n",
      "loss: 1.287546  [38464/74412]\n",
      "loss: 1.132689  [44864/74412]\n",
      "loss: 1.229745  [51264/74412]\n",
      "loss: 1.060826  [57664/74412]\n",
      "loss: 1.352165  [64064/74412]\n",
      "loss: 1.004805  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 1.351764 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 1.726049  [   64/74412]\n",
      "loss: 1.164640  [ 6464/74412]\n",
      "loss: 1.059622  [12864/74412]\n",
      "loss: 1.130145  [19264/74412]\n",
      "loss: 1.279127  [25664/74412]\n",
      "loss: 1.550004  [32064/74412]\n",
      "loss: 1.286834  [38464/74412]\n",
      "loss: 1.131810  [44864/74412]\n",
      "loss: 1.228031  [51264/74412]\n",
      "loss: 1.059567  [57664/74412]\n",
      "loss: 1.351035  [64064/74412]\n",
      "loss: 1.003382  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 1.350603 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 1.725375  [   64/74412]\n",
      "loss: 1.163116  [ 6464/74412]\n",
      "loss: 1.058816  [12864/74412]\n",
      "loss: 1.128650  [19264/74412]\n",
      "loss: 1.279144  [25664/74412]\n",
      "loss: 1.548304  [32064/74412]\n",
      "loss: 1.285032  [38464/74412]\n",
      "loss: 1.130950  [44864/74412]\n",
      "loss: 1.227067  [51264/74412]\n",
      "loss: 1.057882  [57664/74412]\n",
      "loss: 1.349537  [64064/74412]\n",
      "loss: 1.001557  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.349501 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 1.724435  [   64/74412]\n",
      "loss: 1.161680  [ 6464/74412]\n",
      "loss: 1.057824  [12864/74412]\n",
      "loss: 1.127853  [19264/74412]\n",
      "loss: 1.278564  [25664/74412]\n",
      "loss: 1.546034  [32064/74412]\n",
      "loss: 1.283754  [38464/74412]\n",
      "loss: 1.129844  [44864/74412]\n",
      "loss: 1.225342  [51264/74412]\n",
      "loss: 1.056484  [57664/74412]\n",
      "loss: 1.349407  [64064/74412]\n",
      "loss: 1.000443  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.348183 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 1.723409  [   64/74412]\n",
      "loss: 1.160789  [ 6464/74412]\n",
      "loss: 1.057306  [12864/74412]\n",
      "loss: 1.126925  [19264/74412]\n",
      "loss: 1.276614  [25664/74412]\n",
      "loss: 1.543070  [32064/74412]\n",
      "loss: 1.282338  [38464/74412]\n",
      "loss: 1.128977  [44864/74412]\n",
      "loss: 1.224659  [51264/74412]\n",
      "loss: 1.055120  [57664/74412]\n",
      "loss: 1.348082  [64064/74412]\n",
      "loss: 0.999195  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.346567 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 1.722234  [   64/74412]\n",
      "loss: 1.159333  [ 6464/74412]\n",
      "loss: 1.056676  [12864/74412]\n",
      "loss: 1.125689  [19264/74412]\n",
      "loss: 1.278413  [25664/74412]\n",
      "loss: 1.540203  [32064/74412]\n",
      "loss: 1.281070  [38464/74412]\n",
      "loss: 1.127947  [44864/74412]\n",
      "loss: 1.223365  [51264/74412]\n",
      "loss: 1.053239  [57664/74412]\n",
      "loss: 1.347088  [64064/74412]\n",
      "loss: 0.997937  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 1.345467 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 1.721076  [   64/74412]\n",
      "loss: 1.157814  [ 6464/74412]\n",
      "loss: 1.056008  [12864/74412]\n",
      "loss: 1.124825  [19264/74412]\n",
      "loss: 1.277614  [25664/74412]\n",
      "loss: 1.538179  [32064/74412]\n",
      "loss: 1.279601  [38464/74412]\n",
      "loss: 1.127649  [44864/74412]\n",
      "loss: 1.222117  [51264/74412]\n",
      "loss: 1.051649  [57664/74412]\n",
      "loss: 1.346329  [64064/74412]\n",
      "loss: 0.996411  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 1.344337 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 1.720118  [   64/74412]\n",
      "loss: 1.156799  [ 6464/74412]\n",
      "loss: 1.055107  [12864/74412]\n",
      "loss: 1.123721  [19264/74412]\n",
      "loss: 1.277186  [25664/74412]\n",
      "loss: 1.535536  [32064/74412]\n",
      "loss: 1.278223  [38464/74412]\n",
      "loss: 1.126634  [44864/74412]\n",
      "loss: 1.220862  [51264/74412]\n",
      "loss: 1.050235  [57664/74412]\n",
      "loss: 1.345929  [64064/74412]\n",
      "loss: 0.995333  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 1.343195 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 1.719182  [   64/74412]\n",
      "loss: 1.155444  [ 6464/74412]\n",
      "loss: 1.054052  [12864/74412]\n",
      "loss: 1.122938  [19264/74412]\n",
      "loss: 1.274701  [25664/74412]\n",
      "loss: 1.532924  [32064/74412]\n",
      "loss: 1.276965  [38464/74412]\n",
      "loss: 1.125690  [44864/74412]\n",
      "loss: 1.220181  [51264/74412]\n",
      "loss: 1.048535  [57664/74412]\n",
      "loss: 1.345771  [64064/74412]\n",
      "loss: 0.994125  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 1.341984 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 1.716925  [   64/74412]\n",
      "loss: 1.153627  [ 6464/74412]\n",
      "loss: 1.053836  [12864/74412]\n",
      "loss: 1.121343  [19264/74412]\n",
      "loss: 1.273727  [25664/74412]\n",
      "loss: 1.530325  [32064/74412]\n",
      "loss: 1.275616  [38464/74412]\n",
      "loss: 1.125023  [44864/74412]\n",
      "loss: 1.218604  [51264/74412]\n",
      "loss: 1.048263  [57664/74412]\n",
      "loss: 1.344686  [64064/74412]\n",
      "loss: 0.992293  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.340271 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 1.715087  [   64/74412]\n",
      "loss: 1.153150  [ 6464/74412]\n",
      "loss: 1.052775  [12864/74412]\n",
      "loss: 1.120406  [19264/74412]\n",
      "loss: 1.274126  [25664/74412]\n",
      "loss: 1.527797  [32064/74412]\n",
      "loss: 1.273977  [38464/74412]\n",
      "loss: 1.123969  [44864/74412]\n",
      "loss: 1.217071  [51264/74412]\n",
      "loss: 1.045865  [57664/74412]\n",
      "loss: 1.343185  [64064/74412]\n",
      "loss: 0.991140  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.339265 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 1.714147  [   64/74412]\n",
      "loss: 1.152534  [ 6464/74412]\n",
      "loss: 1.052156  [12864/74412]\n",
      "loss: 1.119246  [19264/74412]\n",
      "loss: 1.273245  [25664/74412]\n",
      "loss: 1.525519  [32064/74412]\n",
      "loss: 1.272995  [38464/74412]\n",
      "loss: 1.123108  [44864/74412]\n",
      "loss: 1.216060  [51264/74412]\n",
      "loss: 1.044192  [57664/74412]\n",
      "loss: 1.343746  [64064/74412]\n",
      "loss: 0.990007  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.338517 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 1.713715  [   64/74412]\n",
      "loss: 1.151379  [ 6464/74412]\n",
      "loss: 1.051537  [12864/74412]\n",
      "loss: 1.118655  [19264/74412]\n",
      "loss: 1.272330  [25664/74412]\n",
      "loss: 1.522914  [32064/74412]\n",
      "loss: 1.271249  [38464/74412]\n",
      "loss: 1.122628  [44864/74412]\n",
      "loss: 1.215494  [51264/74412]\n",
      "loss: 1.042961  [57664/74412]\n",
      "loss: 1.342378  [64064/74412]\n",
      "loss: 0.987983  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.337174 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 1.712426  [   64/74412]\n",
      "loss: 1.150409  [ 6464/74412]\n",
      "loss: 1.051290  [12864/74412]\n",
      "loss: 1.117495  [19264/74412]\n",
      "loss: 1.270713  [25664/74412]\n",
      "loss: 1.520548  [32064/74412]\n",
      "loss: 1.270291  [38464/74412]\n",
      "loss: 1.120936  [44864/74412]\n",
      "loss: 1.213538  [51264/74412]\n",
      "loss: 1.041070  [57664/74412]\n",
      "loss: 1.342181  [64064/74412]\n",
      "loss: 0.986317  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.335686 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 1.711164  [   64/74412]\n",
      "loss: 1.149327  [ 6464/74412]\n",
      "loss: 1.049797  [12864/74412]\n",
      "loss: 1.115971  [19264/74412]\n",
      "loss: 1.269395  [25664/74412]\n",
      "loss: 1.517870  [32064/74412]\n",
      "loss: 1.268529  [38464/74412]\n",
      "loss: 1.119861  [44864/74412]\n",
      "loss: 1.213203  [51264/74412]\n",
      "loss: 1.039624  [57664/74412]\n",
      "loss: 1.341265  [64064/74412]\n",
      "loss: 0.984919  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.335098 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 1.711419  [   64/74412]\n",
      "loss: 1.148060  [ 6464/74412]\n",
      "loss: 1.048588  [12864/74412]\n",
      "loss: 1.115386  [19264/74412]\n",
      "loss: 1.268488  [25664/74412]\n",
      "loss: 1.515185  [32064/74412]\n",
      "loss: 1.268060  [38464/74412]\n",
      "loss: 1.117985  [44864/74412]\n",
      "loss: 1.211675  [51264/74412]\n",
      "loss: 1.037924  [57664/74412]\n",
      "loss: 1.340200  [64064/74412]\n",
      "loss: 0.983586  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.334283 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 1.710704  [   64/74412]\n",
      "loss: 1.146959  [ 6464/74412]\n",
      "loss: 1.048646  [12864/74412]\n",
      "loss: 1.114256  [19264/74412]\n",
      "loss: 1.267241  [25664/74412]\n",
      "loss: 1.513091  [32064/74412]\n",
      "loss: 1.266139  [38464/74412]\n",
      "loss: 1.116869  [44864/74412]\n",
      "loss: 1.210898  [51264/74412]\n",
      "loss: 1.036555  [57664/74412]\n",
      "loss: 1.340216  [64064/74412]\n",
      "loss: 0.981972  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 1.332841 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 1.708533  [   64/74412]\n",
      "loss: 1.146327  [ 6464/74412]\n",
      "loss: 1.047881  [12864/74412]\n",
      "loss: 1.113495  [19264/74412]\n",
      "loss: 1.264953  [25664/74412]\n",
      "loss: 1.510886  [32064/74412]\n",
      "loss: 1.265107  [38464/74412]\n",
      "loss: 1.115753  [44864/74412]\n",
      "loss: 1.209425  [51264/74412]\n",
      "loss: 1.034970  [57664/74412]\n",
      "loss: 1.338951  [64064/74412]\n",
      "loss: 0.980629  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 1.331209 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 1.707053  [   64/74412]\n",
      "loss: 1.144762  [ 6464/74412]\n",
      "loss: 1.047245  [12864/74412]\n",
      "loss: 1.112359  [19264/74412]\n",
      "loss: 1.265325  [25664/74412]\n",
      "loss: 1.509040  [32064/74412]\n",
      "loss: 1.263419  [38464/74412]\n",
      "loss: 1.114599  [44864/74412]\n",
      "loss: 1.209410  [51264/74412]\n",
      "loss: 1.033408  [57664/74412]\n",
      "loss: 1.337954  [64064/74412]\n",
      "loss: 0.979180  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 1.331339 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 1.707763  [   64/74412]\n",
      "loss: 1.143534  [ 6464/74412]\n",
      "loss: 1.046213  [12864/74412]\n",
      "loss: 1.111728  [19264/74412]\n",
      "loss: 1.263040  [25664/74412]\n",
      "loss: 1.506465  [32064/74412]\n",
      "loss: 1.261961  [38464/74412]\n",
      "loss: 1.113468  [44864/74412]\n",
      "loss: 1.207762  [51264/74412]\n",
      "loss: 1.031767  [57664/74412]\n",
      "loss: 1.337971  [64064/74412]\n",
      "loss: 0.978128  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.330175 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 1.706666  [   64/74412]\n",
      "loss: 1.142422  [ 6464/74412]\n",
      "loss: 1.045661  [12864/74412]\n",
      "loss: 1.110650  [19264/74412]\n",
      "loss: 1.261672  [25664/74412]\n",
      "loss: 1.504279  [32064/74412]\n",
      "loss: 1.261130  [38464/74412]\n",
      "loss: 1.112651  [44864/74412]\n",
      "loss: 1.206637  [51264/74412]\n",
      "loss: 1.030090  [57664/74412]\n",
      "loss: 1.337493  [64064/74412]\n",
      "loss: 0.976399  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.329181 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 1.705720  [   64/74412]\n",
      "loss: 1.141578  [ 6464/74412]\n",
      "loss: 1.044919  [12864/74412]\n",
      "loss: 1.109730  [19264/74412]\n",
      "loss: 1.261035  [25664/74412]\n",
      "loss: 1.501917  [32064/74412]\n",
      "loss: 1.259667  [38464/74412]\n",
      "loss: 1.111839  [44864/74412]\n",
      "loss: 1.205113  [51264/74412]\n",
      "loss: 1.029011  [57664/74412]\n",
      "loss: 1.336207  [64064/74412]\n",
      "loss: 0.975015  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.327843 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 1.704298  [   64/74412]\n",
      "loss: 1.140617  [ 6464/74412]\n",
      "loss: 1.044233  [12864/74412]\n",
      "loss: 1.108616  [19264/74412]\n",
      "loss: 1.260473  [25664/74412]\n",
      "loss: 1.499630  [32064/74412]\n",
      "loss: 1.258532  [38464/74412]\n",
      "loss: 1.110915  [44864/74412]\n",
      "loss: 1.204004  [51264/74412]\n",
      "loss: 1.027168  [57664/74412]\n",
      "loss: 1.335213  [64064/74412]\n",
      "loss: 0.973226  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.326910 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 1.703424  [   64/74412]\n",
      "loss: 1.139464  [ 6464/74412]\n",
      "loss: 1.043549  [12864/74412]\n",
      "loss: 1.107653  [19264/74412]\n",
      "loss: 1.259461  [25664/74412]\n",
      "loss: 1.497530  [32064/74412]\n",
      "loss: 1.257728  [38464/74412]\n",
      "loss: 1.109771  [44864/74412]\n",
      "loss: 1.202884  [51264/74412]\n",
      "loss: 1.025638  [57664/74412]\n",
      "loss: 1.334273  [64064/74412]\n",
      "loss: 0.971951  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 1.325121 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 1.701667  [   64/74412]\n",
      "loss: 1.138153  [ 6464/74412]\n",
      "loss: 1.042760  [12864/74412]\n",
      "loss: 1.106632  [19264/74412]\n",
      "loss: 1.258417  [25664/74412]\n",
      "loss: 1.495400  [32064/74412]\n",
      "loss: 1.256360  [38464/74412]\n",
      "loss: 1.108403  [44864/74412]\n",
      "loss: 1.201577  [51264/74412]\n",
      "loss: 1.024303  [57664/74412]\n",
      "loss: 1.333674  [64064/74412]\n",
      "loss: 0.971006  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 1.324171 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 1.700225  [   64/74412]\n",
      "loss: 1.137081  [ 6464/74412]\n",
      "loss: 1.042322  [12864/74412]\n",
      "loss: 1.105601  [19264/74412]\n",
      "loss: 1.258931  [25664/74412]\n",
      "loss: 1.493155  [32064/74412]\n",
      "loss: 1.255730  [38464/74412]\n",
      "loss: 1.107592  [44864/74412]\n",
      "loss: 1.200459  [51264/74412]\n",
      "loss: 1.022772  [57664/74412]\n",
      "loss: 1.332752  [64064/74412]\n",
      "loss: 0.969254  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.323042 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 1.699329  [   64/74412]\n",
      "loss: 1.135829  [ 6464/74412]\n",
      "loss: 1.041640  [12864/74412]\n",
      "loss: 1.104195  [19264/74412]\n",
      "loss: 1.258506  [25664/74412]\n",
      "loss: 1.491185  [32064/74412]\n",
      "loss: 1.254044  [38464/74412]\n",
      "loss: 1.106219  [44864/74412]\n",
      "loss: 1.199390  [51264/74412]\n",
      "loss: 1.021137  [57664/74412]\n",
      "loss: 1.331893  [64064/74412]\n",
      "loss: 0.967911  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.321905 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 1.698203  [   64/74412]\n",
      "loss: 1.135081  [ 6464/74412]\n",
      "loss: 1.041010  [12864/74412]\n",
      "loss: 1.103714  [19264/74412]\n",
      "loss: 1.257273  [25664/74412]\n",
      "loss: 1.489092  [32064/74412]\n",
      "loss: 1.252884  [38464/74412]\n",
      "loss: 1.105866  [44864/74412]\n",
      "loss: 1.197546  [51264/74412]\n",
      "loss: 1.019062  [57664/74412]\n",
      "loss: 1.331356  [64064/74412]\n",
      "loss: 0.966523  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.320803 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 1.697258  [   64/74412]\n",
      "loss: 1.134012  [ 6464/74412]\n",
      "loss: 1.040614  [12864/74412]\n",
      "loss: 1.103179  [19264/74412]\n",
      "loss: 1.256569  [25664/74412]\n",
      "loss: 1.486605  [32064/74412]\n",
      "loss: 1.251338  [38464/74412]\n",
      "loss: 1.104326  [44864/74412]\n",
      "loss: 1.196845  [51264/74412]\n",
      "loss: 1.015618  [57664/74412]\n",
      "loss: 1.330383  [64064/74412]\n",
      "loss: 0.965055  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.319691 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 1.695777  [   64/74412]\n",
      "loss: 1.132790  [ 6464/74412]\n",
      "loss: 1.039799  [12864/74412]\n",
      "loss: 1.101515  [19264/74412]\n",
      "loss: 1.255214  [25664/74412]\n",
      "loss: 1.484387  [32064/74412]\n",
      "loss: 1.250575  [38464/74412]\n",
      "loss: 1.103256  [44864/74412]\n",
      "loss: 1.196817  [51264/74412]\n",
      "loss: 1.014196  [57664/74412]\n",
      "loss: 1.329199  [64064/74412]\n",
      "loss: 0.963644  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.317933 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 1.694421  [   64/74412]\n",
      "loss: 1.131623  [ 6464/74412]\n",
      "loss: 1.038971  [12864/74412]\n",
      "loss: 1.100478  [19264/74412]\n",
      "loss: 1.254156  [25664/74412]\n",
      "loss: 1.482141  [32064/74412]\n",
      "loss: 1.250043  [38464/74412]\n",
      "loss: 1.102264  [44864/74412]\n",
      "loss: 1.195931  [51264/74412]\n",
      "loss: 1.012313  [57664/74412]\n",
      "loss: 1.327221  [64064/74412]\n",
      "loss: 0.962592  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.318000 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 1.694958  [   64/74412]\n",
      "loss: 1.130791  [ 6464/74412]\n",
      "loss: 1.037809  [12864/74412]\n",
      "loss: 1.099836  [19264/74412]\n",
      "loss: 1.253007  [25664/74412]\n",
      "loss: 1.480162  [32064/74412]\n",
      "loss: 1.248313  [38464/74412]\n",
      "loss: 1.101181  [44864/74412]\n",
      "loss: 1.194975  [51264/74412]\n",
      "loss: 1.010837  [57664/74412]\n",
      "loss: 1.327532  [64064/74412]\n",
      "loss: 0.961127  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.316713 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 1.693259  [   64/74412]\n",
      "loss: 1.129521  [ 6464/74412]\n",
      "loss: 1.037139  [12864/74412]\n",
      "loss: 1.098892  [19264/74412]\n",
      "loss: 1.251744  [25664/74412]\n",
      "loss: 1.477440  [32064/74412]\n",
      "loss: 1.247492  [38464/74412]\n",
      "loss: 1.100484  [44864/74412]\n",
      "loss: 1.194334  [51264/74412]\n",
      "loss: 1.008765  [57664/74412]\n",
      "loss: 1.325912  [64064/74412]\n",
      "loss: 0.959162  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 1.315204 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 1.691631  [   64/74412]\n",
      "loss: 1.128691  [ 6464/74412]\n",
      "loss: 1.036032  [12864/74412]\n",
      "loss: 1.097282  [19264/74412]\n",
      "loss: 1.250838  [25664/74412]\n",
      "loss: 1.475270  [32064/74412]\n",
      "loss: 1.246262  [38464/74412]\n",
      "loss: 1.099809  [44864/74412]\n",
      "loss: 1.193572  [51264/74412]\n",
      "loss: 1.007640  [57664/74412]\n",
      "loss: 1.325452  [64064/74412]\n",
      "loss: 0.957763  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 1.314294 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 1.691308  [   64/74412]\n",
      "loss: 1.127605  [ 6464/74412]\n",
      "loss: 1.035479  [12864/74412]\n",
      "loss: 1.096649  [19264/74412]\n",
      "loss: 1.249521  [25664/74412]\n",
      "loss: 1.473349  [32064/74412]\n",
      "loss: 1.245593  [38464/74412]\n",
      "loss: 1.099338  [44864/74412]\n",
      "loss: 1.192552  [51264/74412]\n",
      "loss: 1.006660  [57664/74412]\n",
      "loss: 1.324485  [64064/74412]\n",
      "loss: 0.956579  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 1.313271 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 1.690281  [   64/74412]\n",
      "loss: 1.126598  [ 6464/74412]\n",
      "loss: 1.034418  [12864/74412]\n",
      "loss: 1.095659  [19264/74412]\n",
      "loss: 1.248512  [25664/74412]\n",
      "loss: 1.471164  [32064/74412]\n",
      "loss: 1.243993  [38464/74412]\n",
      "loss: 1.098504  [44864/74412]\n",
      "loss: 1.191314  [51264/74412]\n",
      "loss: 1.005023  [57664/74412]\n",
      "loss: 1.323527  [64064/74412]\n",
      "loss: 0.956052  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 1.312215 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 1.689724  [   64/74412]\n",
      "loss: 1.125829  [ 6464/74412]\n",
      "loss: 1.032968  [12864/74412]\n",
      "loss: 1.095004  [19264/74412]\n",
      "loss: 1.247391  [25664/74412]\n",
      "loss: 1.468890  [32064/74412]\n",
      "loss: 1.243556  [38464/74412]\n",
      "loss: 1.096888  [44864/74412]\n",
      "loss: 1.190160  [51264/74412]\n",
      "loss: 1.003896  [57664/74412]\n",
      "loss: 1.323395  [64064/74412]\n",
      "loss: 0.954237  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.309998 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 1.687984  [   64/74412]\n",
      "loss: 1.125390  [ 6464/74412]\n",
      "loss: 1.031805  [12864/74412]\n",
      "loss: 1.093904  [19264/74412]\n",
      "loss: 1.246840  [25664/74412]\n",
      "loss: 1.467106  [32064/74412]\n",
      "loss: 1.242217  [38464/74412]\n",
      "loss: 1.095499  [44864/74412]\n",
      "loss: 1.189444  [51264/74412]\n",
      "loss: 1.002413  [57664/74412]\n",
      "loss: 1.323022  [64064/74412]\n",
      "loss: 0.953012  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.308953 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 1.686873  [   64/74412]\n",
      "loss: 1.124503  [ 6464/74412]\n",
      "loss: 1.031102  [12864/74412]\n",
      "loss: 1.092893  [19264/74412]\n",
      "loss: 1.245720  [25664/74412]\n",
      "loss: 1.464101  [32064/74412]\n",
      "loss: 1.241491  [38464/74412]\n",
      "loss: 1.095017  [44864/74412]\n",
      "loss: 1.188137  [51264/74412]\n",
      "loss: 1.000744  [57664/74412]\n",
      "loss: 1.322551  [64064/74412]\n",
      "loss: 0.951575  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.307578 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 1.685820  [   64/74412]\n",
      "loss: 1.123388  [ 6464/74412]\n",
      "loss: 1.031003  [12864/74412]\n",
      "loss: 1.092757  [19264/74412]\n",
      "loss: 1.244374  [25664/74412]\n",
      "loss: 1.462641  [32064/74412]\n",
      "loss: 1.240455  [38464/74412]\n",
      "loss: 1.093606  [44864/74412]\n",
      "loss: 1.187701  [51264/74412]\n",
      "loss: 0.999424  [57664/74412]\n",
      "loss: 1.322211  [64064/74412]\n",
      "loss: 0.950723  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.306623 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 1.684918  [   64/74412]\n",
      "loss: 1.122242  [ 6464/74412]\n",
      "loss: 1.029941  [12864/74412]\n",
      "loss: 1.091177  [19264/74412]\n",
      "loss: 1.243202  [25664/74412]\n",
      "loss: 1.460145  [32064/74412]\n",
      "loss: 1.240013  [38464/74412]\n",
      "loss: 1.092709  [44864/74412]\n",
      "loss: 1.186575  [51264/74412]\n",
      "loss: 0.997619  [57664/74412]\n",
      "loss: 1.321043  [64064/74412]\n",
      "loss: 0.949478  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.305421 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 1.682933  [   64/74412]\n",
      "loss: 1.120907  [ 6464/74412]\n",
      "loss: 1.029801  [12864/74412]\n",
      "loss: 1.091002  [19264/74412]\n",
      "loss: 1.242249  [25664/74412]\n",
      "loss: 1.457453  [32064/74412]\n",
      "loss: 1.238815  [38464/74412]\n",
      "loss: 1.091671  [44864/74412]\n",
      "loss: 1.185975  [51264/74412]\n",
      "loss: 0.996362  [57664/74412]\n",
      "loss: 1.320575  [64064/74412]\n",
      "loss: 0.948352  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 1.304466 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 1.682123  [   64/74412]\n",
      "loss: 1.119200  [ 6464/74412]\n",
      "loss: 1.029252  [12864/74412]\n",
      "loss: 1.090016  [19264/74412]\n",
      "loss: 1.240436  [25664/74412]\n",
      "loss: 1.455141  [32064/74412]\n",
      "loss: 1.238258  [38464/74412]\n",
      "loss: 1.091193  [44864/74412]\n",
      "loss: 1.185354  [51264/74412]\n",
      "loss: 0.994718  [57664/74412]\n",
      "loss: 1.319754  [64064/74412]\n",
      "loss: 0.947548  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 1.302838 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 1.680350  [   64/74412]\n",
      "loss: 1.118161  [ 6464/74412]\n",
      "loss: 1.028760  [12864/74412]\n",
      "loss: 1.088832  [19264/74412]\n",
      "loss: 1.239555  [25664/74412]\n",
      "loss: 1.453009  [32064/74412]\n",
      "loss: 1.237113  [38464/74412]\n",
      "loss: 1.089808  [44864/74412]\n",
      "loss: 1.184549  [51264/74412]\n",
      "loss: 0.992868  [57664/74412]\n",
      "loss: 1.319078  [64064/74412]\n",
      "loss: 0.945791  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 1.301792 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 1.678672  [   64/74412]\n",
      "loss: 1.117718  [ 6464/74412]\n",
      "loss: 1.027243  [12864/74412]\n",
      "loss: 1.088475  [19264/74412]\n",
      "loss: 1.238039  [25664/74412]\n",
      "loss: 1.451101  [32064/74412]\n",
      "loss: 1.236216  [38464/74412]\n",
      "loss: 1.088667  [44864/74412]\n",
      "loss: 1.184413  [51264/74412]\n",
      "loss: 0.991981  [57664/74412]\n",
      "loss: 1.318095  [64064/74412]\n",
      "loss: 0.944641  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.300777 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 1.677550  [   64/74412]\n",
      "loss: 1.116178  [ 6464/74412]\n",
      "loss: 1.026517  [12864/74412]\n",
      "loss: 1.087882  [19264/74412]\n",
      "loss: 1.236943  [25664/74412]\n",
      "loss: 1.449189  [32064/74412]\n",
      "loss: 1.234660  [38464/74412]\n",
      "loss: 1.088301  [44864/74412]\n",
      "loss: 1.183549  [51264/74412]\n",
      "loss: 0.990074  [57664/74412]\n",
      "loss: 1.316821  [64064/74412]\n",
      "loss: 0.943810  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.299652 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 1.675787  [   64/74412]\n",
      "loss: 1.115025  [ 6464/74412]\n",
      "loss: 1.027052  [12864/74412]\n",
      "loss: 1.087018  [19264/74412]\n",
      "loss: 1.236224  [25664/74412]\n",
      "loss: 1.446857  [32064/74412]\n",
      "loss: 1.233747  [38464/74412]\n",
      "loss: 1.087436  [44864/74412]\n",
      "loss: 1.183070  [51264/74412]\n",
      "loss: 0.988862  [57664/74412]\n",
      "loss: 1.315982  [64064/74412]\n",
      "loss: 0.942454  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.298408 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 1.674378  [   64/74412]\n",
      "loss: 1.113949  [ 6464/74412]\n",
      "loss: 1.026401  [12864/74412]\n",
      "loss: 1.086038  [19264/74412]\n",
      "loss: 1.234910  [25664/74412]\n",
      "loss: 1.444631  [32064/74412]\n",
      "loss: 1.233005  [38464/74412]\n",
      "loss: 1.086774  [44864/74412]\n",
      "loss: 1.181677  [51264/74412]\n",
      "loss: 0.987557  [57664/74412]\n",
      "loss: 1.315136  [64064/74412]\n",
      "loss: 0.941310  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 1.297415 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 1.673490  [   64/74412]\n",
      "loss: 1.113030  [ 6464/74412]\n",
      "loss: 1.025620  [12864/74412]\n",
      "loss: 1.085500  [19264/74412]\n",
      "loss: 1.233866  [25664/74412]\n",
      "loss: 1.442493  [32064/74412]\n",
      "loss: 1.231773  [38464/74412]\n",
      "loss: 1.085845  [44864/74412]\n",
      "loss: 1.181219  [51264/74412]\n",
      "loss: 0.986266  [57664/74412]\n",
      "loss: 1.314308  [64064/74412]\n",
      "loss: 0.940130  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 1.296088 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 1.672184  [   64/74412]\n",
      "loss: 1.111939  [ 6464/74412]\n",
      "loss: 1.025759  [12864/74412]\n",
      "loss: 1.084118  [19264/74412]\n",
      "loss: 1.233189  [25664/74412]\n",
      "loss: 1.440856  [32064/74412]\n",
      "loss: 1.231426  [38464/74412]\n",
      "loss: 1.085007  [44864/74412]\n",
      "loss: 1.180095  [51264/74412]\n",
      "loss: 0.983999  [57664/74412]\n",
      "loss: 1.313523  [64064/74412]\n",
      "loss: 0.938899  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 1.295282 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 1.670651  [   64/74412]\n",
      "loss: 1.110621  [ 6464/74412]\n",
      "loss: 1.025341  [12864/74412]\n",
      "loss: 1.083689  [19264/74412]\n",
      "loss: 1.232180  [25664/74412]\n",
      "loss: 1.438854  [32064/74412]\n",
      "loss: 1.230851  [38464/74412]\n",
      "loss: 1.084018  [44864/74412]\n",
      "loss: 1.179473  [51264/74412]\n",
      "loss: 0.983064  [57664/74412]\n",
      "loss: 1.312621  [64064/74412]\n",
      "loss: 0.937875  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.293952 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 1.669646  [   64/74412]\n",
      "loss: 1.109625  [ 6464/74412]\n",
      "loss: 1.024645  [12864/74412]\n",
      "loss: 1.082608  [19264/74412]\n",
      "loss: 1.231285  [25664/74412]\n",
      "loss: 1.437115  [32064/74412]\n",
      "loss: 1.229423  [38464/74412]\n",
      "loss: 1.082449  [44864/74412]\n",
      "loss: 1.178408  [51264/74412]\n",
      "loss: 0.981352  [57664/74412]\n",
      "loss: 1.312174  [64064/74412]\n",
      "loss: 0.936905  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.292781 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 1.668174  [   64/74412]\n",
      "loss: 1.108901  [ 6464/74412]\n",
      "loss: 1.024556  [12864/74412]\n",
      "loss: 1.081230  [19264/74412]\n",
      "loss: 1.230583  [25664/74412]\n",
      "loss: 1.434107  [32064/74412]\n",
      "loss: 1.227673  [38464/74412]\n",
      "loss: 1.081901  [44864/74412]\n",
      "loss: 1.177489  [51264/74412]\n",
      "loss: 0.980273  [57664/74412]\n",
      "loss: 1.311234  [64064/74412]\n",
      "loss: 0.935738  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.291690 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 1.667114  [   64/74412]\n",
      "loss: 1.107951  [ 6464/74412]\n",
      "loss: 1.024197  [12864/74412]\n",
      "loss: 1.080800  [19264/74412]\n",
      "loss: 1.229252  [25664/74412]\n",
      "loss: 1.432453  [32064/74412]\n",
      "loss: 1.226621  [38464/74412]\n",
      "loss: 1.080905  [44864/74412]\n",
      "loss: 1.176016  [51264/74412]\n",
      "loss: 0.978702  [57664/74412]\n",
      "loss: 1.310044  [64064/74412]\n",
      "loss: 0.934876  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.290626 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 1.665527  [   64/74412]\n",
      "loss: 1.107049  [ 6464/74412]\n",
      "loss: 1.023742  [12864/74412]\n",
      "loss: 1.080175  [19264/74412]\n",
      "loss: 1.227631  [25664/74412]\n",
      "loss: 1.430205  [32064/74412]\n",
      "loss: 1.225702  [38464/74412]\n",
      "loss: 1.080295  [44864/74412]\n",
      "loss: 1.174448  [51264/74412]\n",
      "loss: 0.977491  [57664/74412]\n",
      "loss: 1.309889  [64064/74412]\n",
      "loss: 0.933208  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.289585 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 1.664077  [   64/74412]\n",
      "loss: 1.105917  [ 6464/74412]\n",
      "loss: 1.023214  [12864/74412]\n",
      "loss: 1.079823  [19264/74412]\n",
      "loss: 1.226825  [25664/74412]\n",
      "loss: 1.428635  [32064/74412]\n",
      "loss: 1.224305  [38464/74412]\n",
      "loss: 1.079507  [44864/74412]\n",
      "loss: 1.173919  [51264/74412]\n",
      "loss: 0.976428  [57664/74412]\n",
      "loss: 1.308978  [64064/74412]\n",
      "loss: 0.932526  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.288812 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 1.663774  [   64/74412]\n",
      "loss: 1.104160  [ 6464/74412]\n",
      "loss: 1.022608  [12864/74412]\n",
      "loss: 1.079563  [19264/74412]\n",
      "loss: 1.225765  [25664/74412]\n",
      "loss: 1.426913  [32064/74412]\n",
      "loss: 1.224040  [38464/74412]\n",
      "loss: 1.079251  [44864/74412]\n",
      "loss: 1.173098  [51264/74412]\n",
      "loss: 0.975263  [57664/74412]\n",
      "loss: 1.307905  [64064/74412]\n",
      "loss: 0.931320  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.288035 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 1.662692  [   64/74412]\n",
      "loss: 1.103544  [ 6464/74412]\n",
      "loss: 1.022131  [12864/74412]\n",
      "loss: 1.078024  [19264/74412]\n",
      "loss: 1.224909  [25664/74412]\n",
      "loss: 1.425726  [32064/74412]\n",
      "loss: 1.223009  [38464/74412]\n",
      "loss: 1.078112  [44864/74412]\n",
      "loss: 1.171511  [51264/74412]\n",
      "loss: 0.974300  [57664/74412]\n",
      "loss: 1.307221  [64064/74412]\n",
      "loss: 0.930081  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 1.286630 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 1.661748  [   64/74412]\n",
      "loss: 1.102369  [ 6464/74412]\n",
      "loss: 1.021827  [12864/74412]\n",
      "loss: 1.076863  [19264/74412]\n",
      "loss: 1.223615  [25664/74412]\n",
      "loss: 1.423553  [32064/74412]\n",
      "loss: 1.222450  [38464/74412]\n",
      "loss: 1.076699  [44864/74412]\n",
      "loss: 1.170445  [51264/74412]\n",
      "loss: 0.972750  [57664/74412]\n",
      "loss: 1.306536  [64064/74412]\n",
      "loss: 0.929130  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 1.285721 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 1.660331  [   64/74412]\n",
      "loss: 1.102144  [ 6464/74412]\n",
      "loss: 1.021550  [12864/74412]\n",
      "loss: 1.076083  [19264/74412]\n",
      "loss: 1.222418  [25664/74412]\n",
      "loss: 1.421825  [32064/74412]\n",
      "loss: 1.221619  [38464/74412]\n",
      "loss: 1.075691  [44864/74412]\n",
      "loss: 1.169744  [51264/74412]\n",
      "loss: 0.971636  [57664/74412]\n",
      "loss: 1.305451  [64064/74412]\n",
      "loss: 0.928085  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 1.285087 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 1.659940  [   64/74412]\n",
      "loss: 1.100950  [ 6464/74412]\n",
      "loss: 1.020809  [12864/74412]\n",
      "loss: 1.074664  [19264/74412]\n",
      "loss: 1.221186  [25664/74412]\n",
      "loss: 1.419569  [32064/74412]\n",
      "loss: 1.220933  [38464/74412]\n",
      "loss: 1.075555  [44864/74412]\n",
      "loss: 1.168849  [51264/74412]\n",
      "loss: 0.971209  [57664/74412]\n",
      "loss: 1.304149  [64064/74412]\n",
      "loss: 0.926846  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 1.283944 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 1.658392  [   64/74412]\n",
      "loss: 1.100187  [ 6464/74412]\n",
      "loss: 1.020200  [12864/74412]\n",
      "loss: 1.073766  [19264/74412]\n",
      "loss: 1.220648  [25664/74412]\n",
      "loss: 1.418247  [32064/74412]\n",
      "loss: 1.220300  [38464/74412]\n",
      "loss: 1.074067  [44864/74412]\n",
      "loss: 1.167759  [51264/74412]\n",
      "loss: 0.970280  [57664/74412]\n",
      "loss: 1.303468  [64064/74412]\n",
      "loss: 0.925747  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.283147 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 1.657982  [   64/74412]\n",
      "loss: 1.099589  [ 6464/74412]\n",
      "loss: 1.019639  [12864/74412]\n",
      "loss: 1.073500  [19264/74412]\n",
      "loss: 1.218404  [25664/74412]\n",
      "loss: 1.416027  [32064/74412]\n",
      "loss: 1.219246  [38464/74412]\n",
      "loss: 1.073255  [44864/74412]\n",
      "loss: 1.167319  [51264/74412]\n",
      "loss: 0.968674  [57664/74412]\n",
      "loss: 1.302656  [64064/74412]\n",
      "loss: 0.924896  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.282052 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 1.656043  [   64/74412]\n",
      "loss: 1.098105  [ 6464/74412]\n",
      "loss: 1.019415  [12864/74412]\n",
      "loss: 1.072639  [19264/74412]\n",
      "loss: 1.219201  [25664/74412]\n",
      "loss: 1.414236  [32064/74412]\n",
      "loss: 1.218759  [38464/74412]\n",
      "loss: 1.072169  [44864/74412]\n",
      "loss: 1.165905  [51264/74412]\n",
      "loss: 0.968330  [57664/74412]\n",
      "loss: 1.302119  [64064/74412]\n",
      "loss: 0.923803  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.281233 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 1.655310  [   64/74412]\n",
      "loss: 1.097204  [ 6464/74412]\n",
      "loss: 1.019135  [12864/74412]\n",
      "loss: 1.071532  [19264/74412]\n",
      "loss: 1.218251  [25664/74412]\n",
      "loss: 1.412364  [32064/74412]\n",
      "loss: 1.218323  [38464/74412]\n",
      "loss: 1.071481  [44864/74412]\n",
      "loss: 1.165498  [51264/74412]\n",
      "loss: 0.967379  [57664/74412]\n",
      "loss: 1.301434  [64064/74412]\n",
      "loss: 0.922730  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.279596 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 1.653878  [   64/74412]\n",
      "loss: 1.096535  [ 6464/74412]\n",
      "loss: 1.018707  [12864/74412]\n",
      "loss: 1.070645  [19264/74412]\n",
      "loss: 1.216597  [25664/74412]\n",
      "loss: 1.410803  [32064/74412]\n",
      "loss: 1.217353  [38464/74412]\n",
      "loss: 1.070921  [44864/74412]\n",
      "loss: 1.164169  [51264/74412]\n",
      "loss: 0.966011  [57664/74412]\n",
      "loss: 1.301098  [64064/74412]\n",
      "loss: 0.921898  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 1.278600 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 1.652117  [   64/74412]\n",
      "loss: 1.095228  [ 6464/74412]\n",
      "loss: 1.018975  [12864/74412]\n",
      "loss: 1.069766  [19264/74412]\n",
      "loss: 1.215695  [25664/74412]\n",
      "loss: 1.408697  [32064/74412]\n",
      "loss: 1.216223  [38464/74412]\n",
      "loss: 1.069465  [44864/74412]\n",
      "loss: 1.162815  [51264/74412]\n",
      "loss: 0.964548  [57664/74412]\n",
      "loss: 1.300323  [64064/74412]\n",
      "loss: 0.920957  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 1.277618 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 1.651433  [   64/74412]\n",
      "loss: 1.094774  [ 6464/74412]\n",
      "loss: 1.018587  [12864/74412]\n",
      "loss: 1.068959  [19264/74412]\n",
      "loss: 1.215305  [25664/74412]\n",
      "loss: 1.407186  [32064/74412]\n",
      "loss: 1.216202  [38464/74412]\n",
      "loss: 1.068816  [44864/74412]\n",
      "loss: 1.162254  [51264/74412]\n",
      "loss: 0.963005  [57664/74412]\n",
      "loss: 1.299700  [64064/74412]\n",
      "loss: 0.919672  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 1.276737 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 1.649989  [   64/74412]\n",
      "loss: 1.094214  [ 6464/74412]\n",
      "loss: 1.018582  [12864/74412]\n",
      "loss: 1.068481  [19264/74412]\n",
      "loss: 1.213957  [25664/74412]\n",
      "loss: 1.404844  [32064/74412]\n",
      "loss: 1.215664  [38464/74412]\n",
      "loss: 1.067911  [44864/74412]\n",
      "loss: 1.161434  [51264/74412]\n",
      "loss: 0.962684  [57664/74412]\n",
      "loss: 1.299521  [64064/74412]\n",
      "loss: 0.918493  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 1.275659 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 1.649442  [   64/74412]\n",
      "loss: 1.093173  [ 6464/74412]\n",
      "loss: 1.018229  [12864/74412]\n",
      "loss: 1.067016  [19264/74412]\n",
      "loss: 1.213318  [25664/74412]\n",
      "loss: 1.403270  [32064/74412]\n",
      "loss: 1.214933  [38464/74412]\n",
      "loss: 1.067026  [44864/74412]\n",
      "loss: 1.160687  [51264/74412]\n",
      "loss: 0.961645  [57664/74412]\n",
      "loss: 1.298793  [64064/74412]\n",
      "loss: 0.917484  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 1.274905 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 1.648449  [   64/74412]\n",
      "loss: 1.092862  [ 6464/74412]\n",
      "loss: 1.017405  [12864/74412]\n",
      "loss: 1.066099  [19264/74412]\n",
      "loss: 1.210978  [25664/74412]\n",
      "loss: 1.401200  [32064/74412]\n",
      "loss: 1.214140  [38464/74412]\n",
      "loss: 1.066015  [44864/74412]\n",
      "loss: 1.160103  [51264/74412]\n",
      "loss: 0.960425  [57664/74412]\n",
      "loss: 1.297886  [64064/74412]\n",
      "loss: 0.916842  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 1.273856 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 1.647006  [   64/74412]\n",
      "loss: 1.091232  [ 6464/74412]\n",
      "loss: 1.017322  [12864/74412]\n",
      "loss: 1.065919  [19264/74412]\n",
      "loss: 1.211079  [25664/74412]\n",
      "loss: 1.399637  [32064/74412]\n",
      "loss: 1.213317  [38464/74412]\n",
      "loss: 1.065555  [44864/74412]\n",
      "loss: 1.159292  [51264/74412]\n",
      "loss: 0.959766  [57664/74412]\n",
      "loss: 1.297914  [64064/74412]\n",
      "loss: 0.915720  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.272724 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 1.645477  [   64/74412]\n",
      "loss: 1.091015  [ 6464/74412]\n",
      "loss: 1.016897  [12864/74412]\n",
      "loss: 1.064443  [19264/74412]\n",
      "loss: 1.210047  [25664/74412]\n",
      "loss: 1.397037  [32064/74412]\n",
      "loss: 1.212111  [38464/74412]\n",
      "loss: 1.064481  [44864/74412]\n",
      "loss: 1.158430  [51264/74412]\n",
      "loss: 0.958635  [57664/74412]\n",
      "loss: 1.297025  [64064/74412]\n",
      "loss: 0.914701  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.271438 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 1.644348  [   64/74412]\n",
      "loss: 1.089479  [ 6464/74412]\n",
      "loss: 1.016654  [12864/74412]\n",
      "loss: 1.063692  [19264/74412]\n",
      "loss: 1.207645  [25664/74412]\n",
      "loss: 1.394832  [32064/74412]\n",
      "loss: 1.211588  [38464/74412]\n",
      "loss: 1.063215  [44864/74412]\n",
      "loss: 1.157613  [51264/74412]\n",
      "loss: 0.957780  [57664/74412]\n",
      "loss: 1.296273  [64064/74412]\n",
      "loss: 0.912543  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.270862 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 1.643072  [   64/74412]\n",
      "loss: 1.089016  [ 6464/74412]\n",
      "loss: 1.015805  [12864/74412]\n",
      "loss: 1.062208  [19264/74412]\n",
      "loss: 1.208109  [25664/74412]\n",
      "loss: 1.393113  [32064/74412]\n",
      "loss: 1.211323  [38464/74412]\n",
      "loss: 1.062959  [44864/74412]\n",
      "loss: 1.157226  [51264/74412]\n",
      "loss: 0.956099  [57664/74412]\n",
      "loss: 1.295572  [64064/74412]\n",
      "loss: 0.911817  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 1.269921 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 1.642038  [   64/74412]\n",
      "loss: 1.087574  [ 6464/74412]\n",
      "loss: 1.015213  [12864/74412]\n",
      "loss: 1.060994  [19264/74412]\n",
      "loss: 1.205924  [25664/74412]\n",
      "loss: 1.390681  [32064/74412]\n",
      "loss: 1.210600  [38464/74412]\n",
      "loss: 1.061459  [44864/74412]\n",
      "loss: 1.157548  [51264/74412]\n",
      "loss: 0.955519  [57664/74412]\n",
      "loss: 1.295085  [64064/74412]\n",
      "loss: 0.910912  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 1.268709 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 1.641210  [   64/74412]\n",
      "loss: 1.086401  [ 6464/74412]\n",
      "loss: 1.014588  [12864/74412]\n",
      "loss: 1.060790  [19264/74412]\n",
      "loss: 1.204544  [25664/74412]\n",
      "loss: 1.388627  [32064/74412]\n",
      "loss: 1.209036  [38464/74412]\n",
      "loss: 1.060533  [44864/74412]\n",
      "loss: 1.156046  [51264/74412]\n",
      "loss: 0.954485  [57664/74412]\n",
      "loss: 1.294698  [64064/74412]\n",
      "loss: 0.909758  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.267522 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 1.640297  [   64/74412]\n",
      "loss: 1.085393  [ 6464/74412]\n",
      "loss: 1.014550  [12864/74412]\n",
      "loss: 1.059934  [19264/74412]\n",
      "loss: 1.203562  [25664/74412]\n",
      "loss: 1.386886  [32064/74412]\n",
      "loss: 1.209288  [38464/74412]\n",
      "loss: 1.059737  [44864/74412]\n",
      "loss: 1.155268  [51264/74412]\n",
      "loss: 0.953403  [57664/74412]\n",
      "loss: 1.293658  [64064/74412]\n",
      "loss: 0.909171  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.266513 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "loss: 1.638461  [   64/74412]\n",
      "loss: 1.084748  [ 6464/74412]\n",
      "loss: 1.014772  [12864/74412]\n",
      "loss: 1.058957  [19264/74412]\n",
      "loss: 1.203627  [25664/74412]\n",
      "loss: 1.384927  [32064/74412]\n",
      "loss: 1.208502  [38464/74412]\n",
      "loss: 1.059081  [44864/74412]\n",
      "loss: 1.154111  [51264/74412]\n",
      "loss: 0.951817  [57664/74412]\n",
      "loss: 1.292573  [64064/74412]\n",
      "loss: 0.908568  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.265362 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "loss: 1.637846  [   64/74412]\n",
      "loss: 1.083702  [ 6464/74412]\n",
      "loss: 1.014976  [12864/74412]\n",
      "loss: 1.058036  [19264/74412]\n",
      "loss: 1.202110  [25664/74412]\n",
      "loss: 1.383022  [32064/74412]\n",
      "loss: 1.207563  [38464/74412]\n",
      "loss: 1.057946  [44864/74412]\n",
      "loss: 1.153529  [51264/74412]\n",
      "loss: 0.951124  [57664/74412]\n",
      "loss: 1.292085  [64064/74412]\n",
      "loss: 0.907949  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.264596 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "loss: 1.636003  [   64/74412]\n",
      "loss: 1.082574  [ 6464/74412]\n",
      "loss: 1.014805  [12864/74412]\n",
      "loss: 1.057278  [19264/74412]\n",
      "loss: 1.201453  [25664/74412]\n",
      "loss: 1.381493  [32064/74412]\n",
      "loss: 1.207063  [38464/74412]\n",
      "loss: 1.057282  [44864/74412]\n",
      "loss: 1.152957  [51264/74412]\n",
      "loss: 0.950425  [57664/74412]\n",
      "loss: 1.291471  [64064/74412]\n",
      "loss: 0.906881  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 1.263459 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "loss: 1.634243  [   64/74412]\n",
      "loss: 1.081890  [ 6464/74412]\n",
      "loss: 1.014152  [12864/74412]\n",
      "loss: 1.056343  [19264/74412]\n",
      "loss: 1.200250  [25664/74412]\n",
      "loss: 1.380059  [32064/74412]\n",
      "loss: 1.206938  [38464/74412]\n",
      "loss: 1.056161  [44864/74412]\n",
      "loss: 1.152299  [51264/74412]\n",
      "loss: 0.949526  [57664/74412]\n",
      "loss: 1.290504  [64064/74412]\n",
      "loss: 0.905878  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 1.262857 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "loss: 1.633885  [   64/74412]\n",
      "loss: 1.080701  [ 6464/74412]\n",
      "loss: 1.013779  [12864/74412]\n",
      "loss: 1.055516  [19264/74412]\n",
      "loss: 1.199270  [25664/74412]\n",
      "loss: 1.378303  [32064/74412]\n",
      "loss: 1.206583  [38464/74412]\n",
      "loss: 1.055477  [44864/74412]\n",
      "loss: 1.150927  [51264/74412]\n",
      "loss: 0.948370  [57664/74412]\n",
      "loss: 1.289053  [64064/74412]\n",
      "loss: 0.905283  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 1.259572 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "loss: 1.631210  [   64/74412]\n",
      "loss: 1.080118  [ 6464/74412]\n",
      "loss: 1.008335  [12864/74412]\n",
      "loss: 1.054751  [19264/74412]\n",
      "loss: 1.197761  [25664/74412]\n",
      "loss: 1.376710  [32064/74412]\n",
      "loss: 1.205568  [38464/74412]\n",
      "loss: 1.054581  [44864/74412]\n",
      "loss: 1.150281  [51264/74412]\n",
      "loss: 0.947890  [57664/74412]\n",
      "loss: 1.287618  [64064/74412]\n",
      "loss: 0.904139  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 1.260749 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "loss: 1.630718  [   64/74412]\n",
      "loss: 1.078855  [ 6464/74412]\n",
      "loss: 1.012728  [12864/74412]\n",
      "loss: 1.052860  [19264/74412]\n",
      "loss: 1.197344  [25664/74412]\n",
      "loss: 1.375190  [32064/74412]\n",
      "loss: 1.204303  [38464/74412]\n",
      "loss: 1.052775  [44864/74412]\n",
      "loss: 1.149473  [51264/74412]\n",
      "loss: 0.946657  [57664/74412]\n",
      "loss: 1.288053  [64064/74412]\n",
      "loss: 0.903294  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 1.257645 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "loss: 1.628760  [   64/74412]\n",
      "loss: 1.078161  [ 6464/74412]\n",
      "loss: 1.011775  [12864/74412]\n",
      "loss: 1.051840  [19264/74412]\n",
      "loss: 1.196192  [25664/74412]\n",
      "loss: 1.372442  [32064/74412]\n",
      "loss: 1.203484  [38464/74412]\n",
      "loss: 1.051871  [44864/74412]\n",
      "loss: 1.148527  [51264/74412]\n",
      "loss: 0.945597  [57664/74412]\n",
      "loss: 1.287127  [64064/74412]\n",
      "loss: 0.902515  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 1.256706 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "loss: 1.628013  [   64/74412]\n",
      "loss: 1.077749  [ 6464/74412]\n",
      "loss: 1.012129  [12864/74412]\n",
      "loss: 1.051218  [19264/74412]\n",
      "loss: 1.195201  [25664/74412]\n",
      "loss: 1.370528  [32064/74412]\n",
      "loss: 1.203329  [38464/74412]\n",
      "loss: 1.050073  [44864/74412]\n",
      "loss: 1.147469  [51264/74412]\n",
      "loss: 0.944308  [57664/74412]\n",
      "loss: 1.286088  [64064/74412]\n",
      "loss: 0.901743  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 1.255719 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "loss: 1.625696  [   64/74412]\n",
      "loss: 1.076127  [ 6464/74412]\n",
      "loss: 1.007093  [12864/74412]\n",
      "loss: 1.050730  [19264/74412]\n",
      "loss: 1.194947  [25664/74412]\n",
      "loss: 1.369184  [32064/74412]\n",
      "loss: 1.202608  [38464/74412]\n",
      "loss: 1.050125  [44864/74412]\n",
      "loss: 1.146881  [51264/74412]\n",
      "loss: 0.943030  [57664/74412]\n",
      "loss: 1.285013  [64064/74412]\n",
      "loss: 0.900800  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 1.254809 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "loss: 1.624857  [   64/74412]\n",
      "loss: 1.075457  [ 6464/74412]\n",
      "loss: 1.006715  [12864/74412]\n",
      "loss: 1.050117  [19264/74412]\n",
      "loss: 1.194160  [25664/74412]\n",
      "loss: 1.367788  [32064/74412]\n",
      "loss: 1.201128  [38464/74412]\n",
      "loss: 1.048556  [44864/74412]\n",
      "loss: 1.145949  [51264/74412]\n",
      "loss: 0.941941  [57664/74412]\n",
      "loss: 1.285039  [64064/74412]\n",
      "loss: 0.900398  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.253965 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "loss: 1.623221  [   64/74412]\n",
      "loss: 1.075054  [ 6464/74412]\n",
      "loss: 1.005811  [12864/74412]\n",
      "loss: 1.049507  [19264/74412]\n",
      "loss: 1.193084  [25664/74412]\n",
      "loss: 1.365663  [32064/74412]\n",
      "loss: 1.200565  [38464/74412]\n",
      "loss: 1.048432  [44864/74412]\n",
      "loss: 1.145187  [51264/74412]\n",
      "loss: 0.941407  [57664/74412]\n",
      "loss: 1.284305  [64064/74412]\n",
      "loss: 0.899708  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.253285 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "loss: 1.622571  [   64/74412]\n",
      "loss: 1.074157  [ 6464/74412]\n",
      "loss: 1.005626  [12864/74412]\n",
      "loss: 1.048306  [19264/74412]\n",
      "loss: 1.192722  [25664/74412]\n",
      "loss: 1.363721  [32064/74412]\n",
      "loss: 1.199991  [38464/74412]\n",
      "loss: 1.047621  [44864/74412]\n",
      "loss: 1.145142  [51264/74412]\n",
      "loss: 0.940489  [57664/74412]\n",
      "loss: 1.282977  [64064/74412]\n",
      "loss: 0.898380  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.251652 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "loss: 1.620756  [   64/74412]\n",
      "loss: 1.073118  [ 6464/74412]\n",
      "loss: 1.005831  [12864/74412]\n",
      "loss: 1.047590  [19264/74412]\n",
      "loss: 1.191436  [25664/74412]\n",
      "loss: 1.361913  [32064/74412]\n",
      "loss: 1.200035  [38464/74412]\n",
      "loss: 1.046705  [44864/74412]\n",
      "loss: 1.144868  [51264/74412]\n",
      "loss: 0.939287  [57664/74412]\n",
      "loss: 1.282269  [64064/74412]\n",
      "loss: 0.897819  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 1.250650 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "loss: 1.619026  [   64/74412]\n",
      "loss: 1.072402  [ 6464/74412]\n",
      "loss: 1.004988  [12864/74412]\n",
      "loss: 1.046513  [19264/74412]\n",
      "loss: 1.190703  [25664/74412]\n",
      "loss: 1.359886  [32064/74412]\n",
      "loss: 1.199038  [38464/74412]\n",
      "loss: 1.045614  [44864/74412]\n",
      "loss: 1.143696  [51264/74412]\n",
      "loss: 0.937827  [57664/74412]\n",
      "loss: 1.281296  [64064/74412]\n",
      "loss: 0.896610  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 1.250193 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "loss: 1.618659  [   64/74412]\n",
      "loss: 1.072267  [ 6464/74412]\n",
      "loss: 1.004705  [12864/74412]\n",
      "loss: 1.045641  [19264/74412]\n",
      "loss: 1.189430  [25664/74412]\n",
      "loss: 1.358236  [32064/74412]\n",
      "loss: 1.197857  [38464/74412]\n",
      "loss: 1.044258  [44864/74412]\n",
      "loss: 1.143089  [51264/74412]\n",
      "loss: 0.936659  [57664/74412]\n",
      "loss: 1.280730  [64064/74412]\n",
      "loss: 0.895586  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 1.249035 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "loss: 1.616935  [   64/74412]\n",
      "loss: 1.071181  [ 6464/74412]\n",
      "loss: 1.005145  [12864/74412]\n",
      "loss: 1.045295  [19264/74412]\n",
      "loss: 1.188704  [25664/74412]\n",
      "loss: 1.356460  [32064/74412]\n",
      "loss: 1.197223  [38464/74412]\n",
      "loss: 1.044242  [44864/74412]\n",
      "loss: 1.141427  [51264/74412]\n",
      "loss: 0.935441  [57664/74412]\n",
      "loss: 1.280342  [64064/74412]\n",
      "loss: 0.894622  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 1.248158 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "loss: 1.616067  [   64/74412]\n",
      "loss: 1.070502  [ 6464/74412]\n",
      "loss: 1.004370  [12864/74412]\n",
      "loss: 1.044530  [19264/74412]\n",
      "loss: 1.187749  [25664/74412]\n",
      "loss: 1.355101  [32064/74412]\n",
      "loss: 1.196329  [38464/74412]\n",
      "loss: 1.043342  [44864/74412]\n",
      "loss: 1.140540  [51264/74412]\n",
      "loss: 0.934015  [57664/74412]\n",
      "loss: 1.279884  [64064/74412]\n",
      "loss: 0.893776  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.246761 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "loss: 1.614822  [   64/74412]\n",
      "loss: 1.069521  [ 6464/74412]\n",
      "loss: 1.003775  [12864/74412]\n",
      "loss: 1.044012  [19264/74412]\n",
      "loss: 1.187163  [25664/74412]\n",
      "loss: 1.353528  [32064/74412]\n",
      "loss: 1.195198  [38464/74412]\n",
      "loss: 1.042501  [44864/74412]\n",
      "loss: 1.139787  [51264/74412]\n",
      "loss: 0.932770  [57664/74412]\n",
      "loss: 1.279609  [64064/74412]\n",
      "loss: 0.893261  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.245626 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "loss: 1.613520  [   64/74412]\n",
      "loss: 1.068132  [ 6464/74412]\n",
      "loss: 1.003178  [12864/74412]\n",
      "loss: 1.043863  [19264/74412]\n",
      "loss: 1.185997  [25664/74412]\n",
      "loss: 1.351670  [32064/74412]\n",
      "loss: 1.194329  [38464/74412]\n",
      "loss: 1.041445  [44864/74412]\n",
      "loss: 1.139415  [51264/74412]\n",
      "loss: 0.931868  [57664/74412]\n",
      "loss: 1.278232  [64064/74412]\n",
      "loss: 0.892515  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.245420 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "loss: 1.613154  [   64/74412]\n",
      "loss: 1.067542  [ 6464/74412]\n",
      "loss: 1.002024  [12864/74412]\n",
      "loss: 1.043622  [19264/74412]\n",
      "loss: 1.184964  [25664/74412]\n",
      "loss: 1.350402  [32064/74412]\n",
      "loss: 1.193715  [38464/74412]\n",
      "loss: 1.040715  [44864/74412]\n",
      "loss: 1.137787  [51264/74412]\n",
      "loss: 0.930880  [57664/74412]\n",
      "loss: 1.277152  [64064/74412]\n",
      "loss: 0.891636  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.243849 \n",
      "\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "loss: 1.610987  [   64/74412]\n",
      "loss: 1.066018  [ 6464/74412]\n",
      "loss: 1.001858  [12864/74412]\n",
      "loss: 1.042912  [19264/74412]\n",
      "loss: 1.184090  [25664/74412]\n",
      "loss: 1.348029  [32064/74412]\n",
      "loss: 1.192993  [38464/74412]\n",
      "loss: 1.040276  [44864/74412]\n",
      "loss: 1.137136  [51264/74412]\n",
      "loss: 0.930192  [57664/74412]\n",
      "loss: 1.276659  [64064/74412]\n",
      "loss: 0.890603  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.243256 \n",
      "\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "loss: 1.610513  [   64/74412]\n",
      "loss: 1.064737  [ 6464/74412]\n",
      "loss: 1.001796  [12864/74412]\n",
      "loss: 1.042579  [19264/74412]\n",
      "loss: 1.183713  [25664/74412]\n",
      "loss: 1.345893  [32064/74412]\n",
      "loss: 1.191800  [38464/74412]\n",
      "loss: 1.038988  [44864/74412]\n",
      "loss: 1.137050  [51264/74412]\n",
      "loss: 0.929207  [57664/74412]\n",
      "loss: 1.275514  [64064/74412]\n",
      "loss: 0.890100  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.242422 \n",
      "\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "loss: 1.608645  [   64/74412]\n",
      "loss: 1.063756  [ 6464/74412]\n",
      "loss: 1.001147  [12864/74412]\n",
      "loss: 1.042392  [19264/74412]\n",
      "loss: 1.182694  [25664/74412]\n",
      "loss: 1.344618  [32064/74412]\n",
      "loss: 1.191171  [38464/74412]\n",
      "loss: 1.038388  [44864/74412]\n",
      "loss: 1.135922  [51264/74412]\n",
      "loss: 0.928254  [57664/74412]\n",
      "loss: 1.275316  [64064/74412]\n",
      "loss: 0.889060  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.241856 \n",
      "\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "loss: 1.608393  [   64/74412]\n",
      "loss: 1.062831  [ 6464/74412]\n",
      "loss: 1.000372  [12864/74412]\n",
      "loss: 1.041994  [19264/74412]\n",
      "loss: 1.180564  [25664/74412]\n",
      "loss: 1.343038  [32064/74412]\n",
      "loss: 1.190271  [38464/74412]\n",
      "loss: 1.037331  [44864/74412]\n",
      "loss: 1.135538  [51264/74412]\n",
      "loss: 0.927046  [57664/74412]\n",
      "loss: 1.273790  [64064/74412]\n",
      "loss: 0.887949  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 1.240643 \n",
      "\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "loss: 1.607054  [   64/74412]\n",
      "loss: 1.061807  [ 6464/74412]\n",
      "loss: 0.999930  [12864/74412]\n",
      "loss: 1.041250  [19264/74412]\n",
      "loss: 1.179530  [25664/74412]\n",
      "loss: 1.341769  [32064/74412]\n",
      "loss: 1.189359  [38464/74412]\n",
      "loss: 1.037612  [44864/74412]\n",
      "loss: 1.135117  [51264/74412]\n",
      "loss: 0.925778  [57664/74412]\n",
      "loss: 1.273938  [64064/74412]\n",
      "loss: 0.887091  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 1.239861 \n",
      "\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "loss: 1.605665  [   64/74412]\n",
      "loss: 1.061342  [ 6464/74412]\n",
      "loss: 0.999825  [12864/74412]\n",
      "loss: 1.040263  [19264/74412]\n",
      "loss: 1.179047  [25664/74412]\n",
      "loss: 1.339781  [32064/74412]\n",
      "loss: 1.188860  [38464/74412]\n",
      "loss: 1.036051  [44864/74412]\n",
      "loss: 1.134270  [51264/74412]\n",
      "loss: 0.924171  [57664/74412]\n",
      "loss: 1.273262  [64064/74412]\n",
      "loss: 0.885569  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 1.239331 \n",
      "\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "loss: 1.604849  [   64/74412]\n",
      "loss: 1.060332  [ 6464/74412]\n",
      "loss: 0.999230  [12864/74412]\n",
      "loss: 1.039945  [19264/74412]\n",
      "loss: 1.178463  [25664/74412]\n",
      "loss: 1.338390  [32064/74412]\n",
      "loss: 1.187765  [38464/74412]\n",
      "loss: 1.035187  [44864/74412]\n",
      "loss: 1.133717  [51264/74412]\n",
      "loss: 0.923561  [57664/74412]\n",
      "loss: 1.272893  [64064/74412]\n",
      "loss: 0.885238  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 1.238317 \n",
      "\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "loss: 1.603944  [   64/74412]\n",
      "loss: 1.059733  [ 6464/74412]\n",
      "loss: 0.999017  [12864/74412]\n",
      "loss: 1.039150  [19264/74412]\n",
      "loss: 1.177685  [25664/74412]\n",
      "loss: 1.336642  [32064/74412]\n",
      "loss: 1.186572  [38464/74412]\n",
      "loss: 1.034099  [44864/74412]\n",
      "loss: 1.133225  [51264/74412]\n",
      "loss: 0.922413  [57664/74412]\n",
      "loss: 1.272094  [64064/74412]\n",
      "loss: 0.884590  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 1.237189 \n",
      "\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "loss: 1.602947  [   64/74412]\n",
      "loss: 1.058933  [ 6464/74412]\n",
      "loss: 0.998386  [12864/74412]\n",
      "loss: 1.038786  [19264/74412]\n",
      "loss: 1.176972  [25664/74412]\n",
      "loss: 1.334720  [32064/74412]\n",
      "loss: 1.185716  [38464/74412]\n",
      "loss: 1.033989  [44864/74412]\n",
      "loss: 1.132719  [51264/74412]\n",
      "loss: 0.921411  [57664/74412]\n",
      "loss: 1.271721  [64064/74412]\n",
      "loss: 0.883884  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 1.236429 \n",
      "\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "loss: 1.601677  [   64/74412]\n",
      "loss: 1.057615  [ 6464/74412]\n",
      "loss: 0.996998  [12864/74412]\n",
      "loss: 1.037959  [19264/74412]\n",
      "loss: 1.175626  [25664/74412]\n",
      "loss: 1.333231  [32064/74412]\n",
      "loss: 1.184735  [38464/74412]\n",
      "loss: 1.032398  [44864/74412]\n",
      "loss: 1.132260  [51264/74412]\n",
      "loss: 0.920463  [57664/74412]\n",
      "loss: 1.270969  [64064/74412]\n",
      "loss: 0.882629  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 1.235363 \n",
      "\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "loss: 1.600440  [   64/74412]\n",
      "loss: 1.056721  [ 6464/74412]\n",
      "loss: 0.997453  [12864/74412]\n",
      "loss: 1.037356  [19264/74412]\n",
      "loss: 1.174983  [25664/74412]\n",
      "loss: 1.331441  [32064/74412]\n",
      "loss: 1.183986  [38464/74412]\n",
      "loss: 1.031740  [44864/74412]\n",
      "loss: 1.131041  [51264/74412]\n",
      "loss: 0.919367  [57664/74412]\n",
      "loss: 1.270431  [64064/74412]\n",
      "loss: 0.882172  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 1.234320 \n",
      "\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "loss: 1.599498  [   64/74412]\n",
      "loss: 1.056198  [ 6464/74412]\n",
      "loss: 0.997513  [12864/74412]\n",
      "loss: 1.036699  [19264/74412]\n",
      "loss: 1.173543  [25664/74412]\n",
      "loss: 1.330277  [32064/74412]\n",
      "loss: 1.182958  [38464/74412]\n",
      "loss: 1.030830  [44864/74412]\n",
      "loss: 1.130872  [51264/74412]\n",
      "loss: 0.918096  [57664/74412]\n",
      "loss: 1.269256  [64064/74412]\n",
      "loss: 0.881394  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 1.233888 \n",
      "\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "loss: 1.598545  [   64/74412]\n",
      "loss: 1.055578  [ 6464/74412]\n",
      "loss: 0.996651  [12864/74412]\n",
      "loss: 1.036291  [19264/74412]\n",
      "loss: 1.172218  [25664/74412]\n",
      "loss: 1.328598  [32064/74412]\n",
      "loss: 1.181889  [38464/74412]\n",
      "loss: 1.029829  [44864/74412]\n",
      "loss: 1.130533  [51264/74412]\n",
      "loss: 0.917733  [57664/74412]\n",
      "loss: 1.269451  [64064/74412]\n",
      "loss: 0.880613  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 1.233134 \n",
      "\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "loss: 1.597691  [   64/74412]\n",
      "loss: 1.055048  [ 6464/74412]\n",
      "loss: 0.996211  [12864/74412]\n",
      "loss: 1.035565  [19264/74412]\n",
      "loss: 1.171215  [25664/74412]\n",
      "loss: 1.326618  [32064/74412]\n",
      "loss: 1.181148  [38464/74412]\n",
      "loss: 1.028802  [44864/74412]\n",
      "loss: 1.129879  [51264/74412]\n",
      "loss: 0.916559  [57664/74412]\n",
      "loss: 1.268437  [64064/74412]\n",
      "loss: 0.879783  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 1.232265 \n",
      "\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "loss: 1.596741  [   64/74412]\n",
      "loss: 1.053368  [ 6464/74412]\n",
      "loss: 0.995984  [12864/74412]\n",
      "loss: 1.033995  [19264/74412]\n",
      "loss: 1.169175  [25664/74412]\n",
      "loss: 1.325401  [32064/74412]\n",
      "loss: 1.179838  [38464/74412]\n",
      "loss: 1.027495  [44864/74412]\n",
      "loss: 1.128848  [51264/74412]\n",
      "loss: 0.915996  [57664/74412]\n",
      "loss: 1.267895  [64064/74412]\n",
      "loss: 0.878909  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 1.231496 \n",
      "\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "loss: 1.594998  [   64/74412]\n",
      "loss: 1.053079  [ 6464/74412]\n",
      "loss: 0.996058  [12864/74412]\n",
      "loss: 1.034729  [19264/74412]\n",
      "loss: 1.167672  [25664/74412]\n",
      "loss: 1.323995  [32064/74412]\n",
      "loss: 1.178519  [38464/74412]\n",
      "loss: 1.026247  [44864/74412]\n",
      "loss: 1.128128  [51264/74412]\n",
      "loss: 0.914793  [57664/74412]\n",
      "loss: 1.267779  [64064/74412]\n",
      "loss: 0.878483  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 1.230756 \n",
      "\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "loss: 1.593765  [   64/74412]\n",
      "loss: 1.052421  [ 6464/74412]\n",
      "loss: 0.994906  [12864/74412]\n",
      "loss: 1.034149  [19264/74412]\n",
      "loss: 1.166701  [25664/74412]\n",
      "loss: 1.322423  [32064/74412]\n",
      "loss: 1.178057  [38464/74412]\n",
      "loss: 1.025057  [44864/74412]\n",
      "loss: 1.127700  [51264/74412]\n",
      "loss: 0.913568  [57664/74412]\n",
      "loss: 1.266979  [64064/74412]\n",
      "loss: 0.877972  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 1.229645 \n",
      "\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "loss: 1.592129  [   64/74412]\n",
      "loss: 1.051024  [ 6464/74412]\n",
      "loss: 0.994887  [12864/74412]\n",
      "loss: 1.033237  [19264/74412]\n",
      "loss: 1.165507  [25664/74412]\n",
      "loss: 1.320560  [32064/74412]\n",
      "loss: 1.177824  [38464/74412]\n",
      "loss: 1.024466  [44864/74412]\n",
      "loss: 1.126501  [51264/74412]\n",
      "loss: 0.912846  [57664/74412]\n",
      "loss: 1.266184  [64064/74412]\n",
      "loss: 0.877315  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 1.228872 \n",
      "\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "loss: 1.591373  [   64/74412]\n",
      "loss: 1.050308  [ 6464/74412]\n",
      "loss: 0.994300  [12864/74412]\n",
      "loss: 1.032722  [19264/74412]\n",
      "loss: 1.164183  [25664/74412]\n",
      "loss: 1.318564  [32064/74412]\n",
      "loss: 1.176788  [38464/74412]\n",
      "loss: 1.023114  [44864/74412]\n",
      "loss: 1.126185  [51264/74412]\n",
      "loss: 0.911962  [57664/74412]\n",
      "loss: 1.265805  [64064/74412]\n",
      "loss: 0.876358  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 1.227919 \n",
      "\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "loss: 1.590063  [   64/74412]\n",
      "loss: 1.049252  [ 6464/74412]\n",
      "loss: 0.994215  [12864/74412]\n",
      "loss: 1.032565  [19264/74412]\n",
      "loss: 1.163077  [25664/74412]\n",
      "loss: 1.317196  [32064/74412]\n",
      "loss: 1.175707  [38464/74412]\n",
      "loss: 1.022290  [44864/74412]\n",
      "loss: 1.125369  [51264/74412]\n",
      "loss: 0.911393  [57664/74412]\n",
      "loss: 1.265408  [64064/74412]\n",
      "loss: 0.876069  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 1.226488 \n",
      "\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "loss: 1.588246  [   64/74412]\n",
      "loss: 1.048768  [ 6464/74412]\n",
      "loss: 0.993779  [12864/74412]\n",
      "loss: 1.031536  [19264/74412]\n",
      "loss: 1.162612  [25664/74412]\n",
      "loss: 1.315476  [32064/74412]\n",
      "loss: 1.174874  [38464/74412]\n",
      "loss: 1.021517  [44864/74412]\n",
      "loss: 1.124310  [51264/74412]\n",
      "loss: 0.909617  [57664/74412]\n",
      "loss: 1.264438  [64064/74412]\n",
      "loss: 0.875493  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 1.225943 \n",
      "\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "loss: 1.587006  [   64/74412]\n",
      "loss: 1.047706  [ 6464/74412]\n",
      "loss: 0.993008  [12864/74412]\n",
      "loss: 1.030571  [19264/74412]\n",
      "loss: 1.161722  [25664/74412]\n",
      "loss: 1.313670  [32064/74412]\n",
      "loss: 1.174051  [38464/74412]\n",
      "loss: 1.020860  [44864/74412]\n",
      "loss: 1.123551  [51264/74412]\n",
      "loss: 0.908823  [57664/74412]\n",
      "loss: 1.264084  [64064/74412]\n",
      "loss: 0.875346  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 1.224480 \n",
      "\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "loss: 1.585659  [   64/74412]\n",
      "loss: 1.046757  [ 6464/74412]\n",
      "loss: 0.992850  [12864/74412]\n",
      "loss: 1.030002  [19264/74412]\n",
      "loss: 1.160638  [25664/74412]\n",
      "loss: 1.312079  [32064/74412]\n",
      "loss: 1.172113  [38464/74412]\n",
      "loss: 1.019901  [44864/74412]\n",
      "loss: 1.123095  [51264/74412]\n",
      "loss: 0.907670  [57664/74412]\n",
      "loss: 1.263062  [64064/74412]\n",
      "loss: 0.874521  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 1.223841 \n",
      "\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "loss: 1.584602  [   64/74412]\n",
      "loss: 1.045454  [ 6464/74412]\n",
      "loss: 0.992495  [12864/74412]\n",
      "loss: 1.029175  [19264/74412]\n",
      "loss: 1.159505  [25664/74412]\n",
      "loss: 1.310212  [32064/74412]\n",
      "loss: 1.171685  [38464/74412]\n",
      "loss: 1.019155  [44864/74412]\n",
      "loss: 1.122817  [51264/74412]\n",
      "loss: 0.906790  [57664/74412]\n",
      "loss: 1.262843  [64064/74412]\n",
      "loss: 0.874176  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 1.223157 \n",
      "\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "loss: 1.583622  [   64/74412]\n",
      "loss: 1.044615  [ 6464/74412]\n",
      "loss: 0.992109  [12864/74412]\n",
      "loss: 1.028092  [19264/74412]\n",
      "loss: 1.158518  [25664/74412]\n",
      "loss: 1.309083  [32064/74412]\n",
      "loss: 1.170959  [38464/74412]\n",
      "loss: 1.018075  [44864/74412]\n",
      "loss: 1.122215  [51264/74412]\n",
      "loss: 0.905702  [57664/74412]\n",
      "loss: 1.262479  [64064/74412]\n",
      "loss: 0.873396  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 1.222364 \n",
      "\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "loss: 1.582076  [   64/74412]\n",
      "loss: 1.044023  [ 6464/74412]\n",
      "loss: 0.991377  [12864/74412]\n",
      "loss: 1.027908  [19264/74412]\n",
      "loss: 1.157417  [25664/74412]\n",
      "loss: 1.307249  [32064/74412]\n",
      "loss: 1.169661  [38464/74412]\n",
      "loss: 1.016450  [44864/74412]\n",
      "loss: 1.121473  [51264/74412]\n",
      "loss: 0.904101  [57664/74412]\n",
      "loss: 1.261352  [64064/74412]\n",
      "loss: 0.873290  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 1.221672 \n",
      "\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "loss: 1.581105  [   64/74412]\n",
      "loss: 1.043478  [ 6464/74412]\n",
      "loss: 0.990988  [12864/74412]\n",
      "loss: 1.027403  [19264/74412]\n",
      "loss: 1.156969  [25664/74412]\n",
      "loss: 1.305688  [32064/74412]\n",
      "loss: 1.169144  [38464/74412]\n",
      "loss: 1.016159  [44864/74412]\n",
      "loss: 1.120648  [51264/74412]\n",
      "loss: 0.902701  [57664/74412]\n",
      "loss: 1.261419  [64064/74412]\n",
      "loss: 0.872189  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 1.220970 \n",
      "\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "loss: 1.579587  [   64/74412]\n",
      "loss: 1.042588  [ 6464/74412]\n",
      "loss: 0.991289  [12864/74412]\n",
      "loss: 1.026299  [19264/74412]\n",
      "loss: 1.155722  [25664/74412]\n",
      "loss: 1.304134  [32064/74412]\n",
      "loss: 1.168695  [38464/74412]\n",
      "loss: 1.014904  [44864/74412]\n",
      "loss: 1.120179  [51264/74412]\n",
      "loss: 0.901606  [57664/74412]\n",
      "loss: 1.260420  [64064/74412]\n",
      "loss: 0.871673  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 1.220220 \n",
      "\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "loss: 1.578544  [   64/74412]\n",
      "loss: 1.041756  [ 6464/74412]\n",
      "loss: 0.990897  [12864/74412]\n",
      "loss: 1.025173  [19264/74412]\n",
      "loss: 1.154244  [25664/74412]\n",
      "loss: 1.302684  [32064/74412]\n",
      "loss: 1.167516  [38464/74412]\n",
      "loss: 1.013912  [44864/74412]\n",
      "loss: 1.119133  [51264/74412]\n",
      "loss: 0.901353  [57664/74412]\n",
      "loss: 1.259539  [64064/74412]\n",
      "loss: 0.870544  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 1.219264 \n",
      "\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "loss: 1.577339  [   64/74412]\n",
      "loss: 1.041246  [ 6464/74412]\n",
      "loss: 0.990716  [12864/74412]\n",
      "loss: 1.024618  [19264/74412]\n",
      "loss: 1.153444  [25664/74412]\n",
      "loss: 1.301286  [32064/74412]\n",
      "loss: 1.166287  [38464/74412]\n",
      "loss: 1.013501  [44864/74412]\n",
      "loss: 1.118759  [51264/74412]\n",
      "loss: 0.900448  [57664/74412]\n",
      "loss: 1.258682  [64064/74412]\n",
      "loss: 0.869690  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 1.218487 \n",
      "\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "loss: 1.576491  [   64/74412]\n",
      "loss: 1.040559  [ 6464/74412]\n",
      "loss: 0.990201  [12864/74412]\n",
      "loss: 1.024671  [19264/74412]\n",
      "loss: 1.152631  [25664/74412]\n",
      "loss: 1.299449  [32064/74412]\n",
      "loss: 1.165379  [38464/74412]\n",
      "loss: 1.012572  [44864/74412]\n",
      "loss: 1.117285  [51264/74412]\n",
      "loss: 0.899547  [57664/74412]\n",
      "loss: 1.258100  [64064/74412]\n",
      "loss: 0.869081  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.217964 \n",
      "\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "loss: 1.574959  [   64/74412]\n",
      "loss: 1.039377  [ 6464/74412]\n",
      "loss: 0.989820  [12864/74412]\n",
      "loss: 1.023634  [19264/74412]\n",
      "loss: 1.151673  [25664/74412]\n",
      "loss: 1.298332  [32064/74412]\n",
      "loss: 1.164182  [38464/74412]\n",
      "loss: 1.012188  [44864/74412]\n",
      "loss: 1.116866  [51264/74412]\n",
      "loss: 0.898361  [57664/74412]\n",
      "loss: 1.257746  [64064/74412]\n",
      "loss: 0.868151  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.217112 \n",
      "\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "loss: 1.574103  [   64/74412]\n",
      "loss: 1.038646  [ 6464/74412]\n",
      "loss: 0.989649  [12864/74412]\n",
      "loss: 1.023131  [19264/74412]\n",
      "loss: 1.151169  [25664/74412]\n",
      "loss: 1.296423  [32064/74412]\n",
      "loss: 1.163419  [38464/74412]\n",
      "loss: 1.011579  [44864/74412]\n",
      "loss: 1.117197  [51264/74412]\n",
      "loss: 0.897545  [57664/74412]\n",
      "loss: 1.256921  [64064/74412]\n",
      "loss: 0.867963  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.216261 \n",
      "\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "loss: 1.572802  [   64/74412]\n",
      "loss: 1.038127  [ 6464/74412]\n",
      "loss: 0.989548  [12864/74412]\n",
      "loss: 1.023108  [19264/74412]\n",
      "loss: 1.150339  [25664/74412]\n",
      "loss: 1.295177  [32064/74412]\n",
      "loss: 1.162230  [38464/74412]\n",
      "loss: 1.010383  [44864/74412]\n",
      "loss: 1.115969  [51264/74412]\n",
      "loss: 0.896704  [57664/74412]\n",
      "loss: 1.256192  [64064/74412]\n",
      "loss: 0.867318  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.215516 \n",
      "\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "loss: 1.571647  [   64/74412]\n",
      "loss: 1.037674  [ 6464/74412]\n",
      "loss: 0.989247  [12864/74412]\n",
      "loss: 1.021675  [19264/74412]\n",
      "loss: 1.149153  [25664/74412]\n",
      "loss: 1.292659  [32064/74412]\n",
      "loss: 1.161121  [38464/74412]\n",
      "loss: 1.009352  [44864/74412]\n",
      "loss: 1.115386  [51264/74412]\n",
      "loss: 0.895866  [57664/74412]\n",
      "loss: 1.255634  [64064/74412]\n",
      "loss: 0.866926  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.214788 \n",
      "\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "loss: 1.570203  [   64/74412]\n",
      "loss: 1.036671  [ 6464/74412]\n",
      "loss: 0.989237  [12864/74412]\n",
      "loss: 1.021294  [19264/74412]\n",
      "loss: 1.147369  [25664/74412]\n",
      "loss: 1.292159  [32064/74412]\n",
      "loss: 1.160480  [38464/74412]\n",
      "loss: 1.008439  [44864/74412]\n",
      "loss: 1.114744  [51264/74412]\n",
      "loss: 0.895359  [57664/74412]\n",
      "loss: 1.255084  [64064/74412]\n",
      "loss: 0.866256  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.214027 \n",
      "\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "loss: 1.569328  [   64/74412]\n",
      "loss: 1.036184  [ 6464/74412]\n",
      "loss: 0.988165  [12864/74412]\n",
      "loss: 1.021145  [19264/74412]\n",
      "loss: 1.146404  [25664/74412]\n",
      "loss: 1.289804  [32064/74412]\n",
      "loss: 1.159764  [38464/74412]\n",
      "loss: 1.007607  [44864/74412]\n",
      "loss: 1.114262  [51264/74412]\n",
      "loss: 0.894456  [57664/74412]\n",
      "loss: 1.254749  [64064/74412]\n",
      "loss: 0.865512  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.213472 \n",
      "\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "loss: 1.568405  [   64/74412]\n",
      "loss: 1.035314  [ 6464/74412]\n",
      "loss: 0.987228  [12864/74412]\n",
      "loss: 1.020146  [19264/74412]\n",
      "loss: 1.145128  [25664/74412]\n",
      "loss: 1.288820  [32064/74412]\n",
      "loss: 1.158647  [38464/74412]\n",
      "loss: 1.006906  [44864/74412]\n",
      "loss: 1.113962  [51264/74412]\n",
      "loss: 0.892490  [57664/74412]\n",
      "loss: 1.254025  [64064/74412]\n",
      "loss: 0.864781  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.212796 \n",
      "\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "loss: 1.567307  [   64/74412]\n",
      "loss: 1.034413  [ 6464/74412]\n",
      "loss: 0.986878  [12864/74412]\n",
      "loss: 1.018394  [19264/74412]\n",
      "loss: 1.144603  [25664/74412]\n",
      "loss: 1.287205  [32064/74412]\n",
      "loss: 1.157186  [38464/74412]\n",
      "loss: 1.005721  [44864/74412]\n",
      "loss: 1.112446  [51264/74412]\n",
      "loss: 0.891715  [57664/74412]\n",
      "loss: 1.253742  [64064/74412]\n",
      "loss: 0.863921  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.211926 \n",
      "\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "loss: 1.566058  [   64/74412]\n",
      "loss: 1.033338  [ 6464/74412]\n",
      "loss: 0.986770  [12864/74412]\n",
      "loss: 1.015443  [19264/74412]\n",
      "loss: 1.143488  [25664/74412]\n",
      "loss: 1.285682  [32064/74412]\n",
      "loss: 1.156101  [38464/74412]\n",
      "loss: 1.005327  [44864/74412]\n",
      "loss: 1.111894  [51264/74412]\n",
      "loss: 0.890315  [57664/74412]\n",
      "loss: 1.252324  [64064/74412]\n",
      "loss: 0.863334  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.211063 \n",
      "\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "loss: 1.565635  [   64/74412]\n",
      "loss: 1.032246  [ 6464/74412]\n",
      "loss: 0.986150  [12864/74412]\n",
      "loss: 1.016074  [19264/74412]\n",
      "loss: 1.142887  [25664/74412]\n",
      "loss: 1.284869  [32064/74412]\n",
      "loss: 1.155403  [38464/74412]\n",
      "loss: 1.004423  [44864/74412]\n",
      "loss: 1.111311  [51264/74412]\n",
      "loss: 0.889784  [57664/74412]\n",
      "loss: 1.252215  [64064/74412]\n",
      "loss: 0.862360  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.209860 \n",
      "\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "loss: 1.564721  [   64/74412]\n",
      "loss: 1.031640  [ 6464/74412]\n",
      "loss: 0.985452  [12864/74412]\n",
      "loss: 1.014709  [19264/74412]\n",
      "loss: 1.141494  [25664/74412]\n",
      "loss: 1.283262  [32064/74412]\n",
      "loss: 1.154691  [38464/74412]\n",
      "loss: 1.002904  [44864/74412]\n",
      "loss: 1.110246  [51264/74412]\n",
      "loss: 0.889380  [57664/74412]\n",
      "loss: 1.251857  [64064/74412]\n",
      "loss: 0.861738  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.209265 \n",
      "\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "loss: 1.563109  [   64/74412]\n",
      "loss: 1.031271  [ 6464/74412]\n",
      "loss: 0.985181  [12864/74412]\n",
      "loss: 1.014180  [19264/74412]\n",
      "loss: 1.140323  [25664/74412]\n",
      "loss: 1.280775  [32064/74412]\n",
      "loss: 1.153762  [38464/74412]\n",
      "loss: 1.002255  [44864/74412]\n",
      "loss: 1.109972  [51264/74412]\n",
      "loss: 0.888183  [57664/74412]\n",
      "loss: 1.250921  [64064/74412]\n",
      "loss: 0.861086  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.208873 \n",
      "\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "loss: 1.562414  [   64/74412]\n",
      "loss: 1.030016  [ 6464/74412]\n",
      "loss: 0.985317  [12864/74412]\n",
      "loss: 1.013798  [19264/74412]\n",
      "loss: 1.140329  [25664/74412]\n",
      "loss: 1.280410  [32064/74412]\n",
      "loss: 1.153349  [38464/74412]\n",
      "loss: 1.000716  [44864/74412]\n",
      "loss: 1.109209  [51264/74412]\n",
      "loss: 0.886649  [57664/74412]\n",
      "loss: 1.249862  [64064/74412]\n",
      "loss: 0.860141  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.208552 \n",
      "\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "loss: 1.563161  [   64/74412]\n",
      "loss: 1.029937  [ 6464/74412]\n",
      "loss: 0.984767  [12864/74412]\n",
      "loss: 1.012560  [19264/74412]\n",
      "loss: 1.138890  [25664/74412]\n",
      "loss: 1.278681  [32064/74412]\n",
      "loss: 1.152452  [38464/74412]\n",
      "loss: 0.999495  [44864/74412]\n",
      "loss: 1.108709  [51264/74412]\n",
      "loss: 0.885414  [57664/74412]\n",
      "loss: 1.249985  [64064/74412]\n",
      "loss: 0.859524  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.207818 \n",
      "\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "loss: 1.562160  [   64/74412]\n",
      "loss: 1.028344  [ 6464/74412]\n",
      "loss: 0.984078  [12864/74412]\n",
      "loss: 1.011874  [19264/74412]\n",
      "loss: 1.137990  [25664/74412]\n",
      "loss: 1.277334  [32064/74412]\n",
      "loss: 1.151929  [38464/74412]\n",
      "loss: 0.998784  [44864/74412]\n",
      "loss: 1.108005  [51264/74412]\n",
      "loss: 0.884809  [57664/74412]\n",
      "loss: 1.248481  [64064/74412]\n",
      "loss: 0.859296  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.207359 \n",
      "\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "loss: 1.561106  [   64/74412]\n",
      "loss: 1.027593  [ 6464/74412]\n",
      "loss: 0.983438  [12864/74412]\n",
      "loss: 1.011104  [19264/74412]\n",
      "loss: 1.136653  [25664/74412]\n",
      "loss: 1.275907  [32064/74412]\n",
      "loss: 1.151505  [38464/74412]\n",
      "loss: 0.997007  [44864/74412]\n",
      "loss: 1.107446  [51264/74412]\n",
      "loss: 0.883837  [57664/74412]\n",
      "loss: 1.248348  [64064/74412]\n",
      "loss: 0.858296  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.206557 \n",
      "\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "loss: 1.561003  [   64/74412]\n",
      "loss: 1.026853  [ 6464/74412]\n",
      "loss: 0.983062  [12864/74412]\n",
      "loss: 1.010984  [19264/74412]\n",
      "loss: 1.135767  [25664/74412]\n",
      "loss: 1.274645  [32064/74412]\n",
      "loss: 1.150233  [38464/74412]\n",
      "loss: 0.996484  [44864/74412]\n",
      "loss: 1.106322  [51264/74412]\n",
      "loss: 0.882684  [57664/74412]\n",
      "loss: 1.248242  [64064/74412]\n",
      "loss: 0.857717  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.205690 \n",
      "\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "loss: 1.559961  [   64/74412]\n",
      "loss: 1.025836  [ 6464/74412]\n",
      "loss: 0.982749  [12864/74412]\n",
      "loss: 1.010780  [19264/74412]\n",
      "loss: 1.133947  [25664/74412]\n",
      "loss: 1.273026  [32064/74412]\n",
      "loss: 1.149406  [38464/74412]\n",
      "loss: 0.996132  [44864/74412]\n",
      "loss: 1.106717  [51264/74412]\n",
      "loss: 0.881674  [57664/74412]\n",
      "loss: 1.246672  [64064/74412]\n",
      "loss: 0.856802  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 1.204755 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "loss: 1.557923  [   64/74412]\n",
      "loss: 1.024879  [ 6464/74412]\n",
      "loss: 0.982206  [12864/74412]\n",
      "loss: 1.010488  [19264/74412]\n",
      "loss: 1.134027  [25664/74412]\n",
      "loss: 1.271397  [32064/74412]\n",
      "loss: 1.149150  [38464/74412]\n",
      "loss: 0.994941  [44864/74412]\n",
      "loss: 1.105342  [51264/74412]\n",
      "loss: 0.880253  [57664/74412]\n",
      "loss: 1.247290  [64064/74412]\n",
      "loss: 0.856748  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 1.204145 \n",
      "\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "loss: 1.557464  [   64/74412]\n",
      "loss: 1.024579  [ 6464/74412]\n",
      "loss: 0.981760  [12864/74412]\n",
      "loss: 1.010368  [19264/74412]\n",
      "loss: 1.133186  [25664/74412]\n",
      "loss: 1.270897  [32064/74412]\n",
      "loss: 1.148065  [38464/74412]\n",
      "loss: 0.994690  [44864/74412]\n",
      "loss: 1.105826  [51264/74412]\n",
      "loss: 0.879291  [57664/74412]\n",
      "loss: 1.246644  [64064/74412]\n",
      "loss: 0.855684  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 1.203353 \n",
      "\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "loss: 1.556240  [   64/74412]\n",
      "loss: 1.023609  [ 6464/74412]\n",
      "loss: 0.981239  [12864/74412]\n",
      "loss: 1.009825  [19264/74412]\n",
      "loss: 1.131420  [25664/74412]\n",
      "loss: 1.268404  [32064/74412]\n",
      "loss: 1.147249  [38464/74412]\n",
      "loss: 0.993957  [44864/74412]\n",
      "loss: 1.105312  [51264/74412]\n",
      "loss: 0.878296  [57664/74412]\n",
      "loss: 1.246580  [64064/74412]\n",
      "loss: 0.855337  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 1.202632 \n",
      "\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "loss: 1.555684  [   64/74412]\n",
      "loss: 1.022478  [ 6464/74412]\n",
      "loss: 0.980223  [12864/74412]\n",
      "loss: 1.009303  [19264/74412]\n",
      "loss: 1.130646  [25664/74412]\n",
      "loss: 1.268098  [32064/74412]\n",
      "loss: 1.146928  [38464/74412]\n",
      "loss: 0.993209  [44864/74412]\n",
      "loss: 1.104929  [51264/74412]\n",
      "loss: 0.877596  [57664/74412]\n",
      "loss: 1.245366  [64064/74412]\n",
      "loss: 0.854258  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 1.202048 \n",
      "\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "loss: 1.553797  [   64/74412]\n",
      "loss: 1.021861  [ 6464/74412]\n",
      "loss: 0.979399  [12864/74412]\n",
      "loss: 1.008316  [19264/74412]\n",
      "loss: 1.130495  [25664/74412]\n",
      "loss: 1.266612  [32064/74412]\n",
      "loss: 1.145796  [38464/74412]\n",
      "loss: 0.992514  [44864/74412]\n",
      "loss: 1.103544  [51264/74412]\n",
      "loss: 0.876503  [57664/74412]\n",
      "loss: 1.245231  [64064/74412]\n",
      "loss: 0.852852  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 1.201343 \n",
      "\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "loss: 1.552925  [   64/74412]\n",
      "loss: 1.020560  [ 6464/74412]\n",
      "loss: 0.978304  [12864/74412]\n",
      "loss: 1.007780  [19264/74412]\n",
      "loss: 1.129097  [25664/74412]\n",
      "loss: 1.265709  [32064/74412]\n",
      "loss: 1.144459  [38464/74412]\n",
      "loss: 0.991437  [44864/74412]\n",
      "loss: 1.103125  [51264/74412]\n",
      "loss: 0.875626  [57664/74412]\n",
      "loss: 1.244120  [64064/74412]\n",
      "loss: 0.852414  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 1.200593 \n",
      "\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "loss: 1.552978  [   64/74412]\n",
      "loss: 1.020158  [ 6464/74412]\n",
      "loss: 0.978623  [12864/74412]\n",
      "loss: 1.006836  [19264/74412]\n",
      "loss: 1.127833  [25664/74412]\n",
      "loss: 1.264137  [32064/74412]\n",
      "loss: 1.143908  [38464/74412]\n",
      "loss: 0.990083  [44864/74412]\n",
      "loss: 1.102448  [51264/74412]\n",
      "loss: 0.874741  [57664/74412]\n",
      "loss: 1.244704  [64064/74412]\n",
      "loss: 0.852038  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 1.200186 \n",
      "\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "loss: 1.551518  [   64/74412]\n",
      "loss: 1.019143  [ 6464/74412]\n",
      "loss: 0.977598  [12864/74412]\n",
      "loss: 1.005803  [19264/74412]\n",
      "loss: 1.126914  [25664/74412]\n",
      "loss: 1.262589  [32064/74412]\n",
      "loss: 1.143431  [38464/74412]\n",
      "loss: 0.989576  [44864/74412]\n",
      "loss: 1.101476  [51264/74412]\n",
      "loss: 0.873178  [57664/74412]\n",
      "loss: 1.244058  [64064/74412]\n",
      "loss: 0.850602  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 1.198645 \n",
      "\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "loss: 1.549703  [   64/74412]\n",
      "loss: 1.019149  [ 6464/74412]\n",
      "loss: 0.977319  [12864/74412]\n",
      "loss: 1.005650  [19264/74412]\n",
      "loss: 1.125835  [25664/74412]\n",
      "loss: 1.261398  [32064/74412]\n",
      "loss: 1.142204  [38464/74412]\n",
      "loss: 0.988756  [44864/74412]\n",
      "loss: 1.100935  [51264/74412]\n",
      "loss: 0.872396  [57664/74412]\n",
      "loss: 1.243129  [64064/74412]\n",
      "loss: 0.851320  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 1.198320 \n",
      "\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "loss: 1.549160  [   64/74412]\n",
      "loss: 1.017950  [ 6464/74412]\n",
      "loss: 0.976527  [12864/74412]\n",
      "loss: 1.004152  [19264/74412]\n",
      "loss: 1.125471  [25664/74412]\n",
      "loss: 1.260350  [32064/74412]\n",
      "loss: 1.141633  [38464/74412]\n",
      "loss: 0.987721  [44864/74412]\n",
      "loss: 1.100566  [51264/74412]\n",
      "loss: 0.871578  [57664/74412]\n",
      "loss: 1.242769  [64064/74412]\n",
      "loss: 0.849644  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 1.197836 \n",
      "\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "loss: 1.548136  [   64/74412]\n",
      "loss: 1.017592  [ 6464/74412]\n",
      "loss: 0.976401  [12864/74412]\n",
      "loss: 1.003270  [19264/74412]\n",
      "loss: 1.124568  [25664/74412]\n",
      "loss: 1.258279  [32064/74412]\n",
      "loss: 1.140792  [38464/74412]\n",
      "loss: 0.987018  [44864/74412]\n",
      "loss: 1.100081  [51264/74412]\n",
      "loss: 0.870294  [57664/74412]\n",
      "loss: 1.241751  [64064/74412]\n",
      "loss: 0.849261  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 1.196746 \n",
      "\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "loss: 1.547099  [   64/74412]\n",
      "loss: 1.016775  [ 6464/74412]\n",
      "loss: 0.976208  [12864/74412]\n",
      "loss: 1.002527  [19264/74412]\n",
      "loss: 1.124030  [25664/74412]\n",
      "loss: 1.257434  [32064/74412]\n",
      "loss: 1.139756  [38464/74412]\n",
      "loss: 0.986628  [44864/74412]\n",
      "loss: 1.099254  [51264/74412]\n",
      "loss: 0.868872  [57664/74412]\n",
      "loss: 1.240539  [64064/74412]\n",
      "loss: 0.849368  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 1.195965 \n",
      "\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "loss: 1.545706  [   64/74412]\n",
      "loss: 1.016511  [ 6464/74412]\n",
      "loss: 0.975808  [12864/74412]\n",
      "loss: 1.002437  [19264/74412]\n",
      "loss: 1.123611  [25664/74412]\n",
      "loss: 1.256200  [32064/74412]\n",
      "loss: 1.139163  [38464/74412]\n",
      "loss: 0.986374  [44864/74412]\n",
      "loss: 1.098355  [51264/74412]\n",
      "loss: 0.867829  [57664/74412]\n",
      "loss: 1.240743  [64064/74412]\n",
      "loss: 0.847251  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 1.195218 \n",
      "\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "loss: 1.544584  [   64/74412]\n",
      "loss: 1.015991  [ 6464/74412]\n",
      "loss: 0.974725  [12864/74412]\n",
      "loss: 1.001556  [19264/74412]\n",
      "loss: 1.122270  [25664/74412]\n",
      "loss: 1.255157  [32064/74412]\n",
      "loss: 1.138081  [38464/74412]\n",
      "loss: 0.985222  [44864/74412]\n",
      "loss: 1.097437  [51264/74412]\n",
      "loss: 0.866673  [57664/74412]\n",
      "loss: 1.240103  [64064/74412]\n",
      "loss: 0.846972  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 1.195156 \n",
      "\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "loss: 1.544061  [   64/74412]\n",
      "loss: 1.015230  [ 6464/74412]\n",
      "loss: 0.974523  [12864/74412]\n",
      "loss: 1.000743  [19264/74412]\n",
      "loss: 1.121495  [25664/74412]\n",
      "loss: 1.253277  [32064/74412]\n",
      "loss: 1.137219  [38464/74412]\n",
      "loss: 0.984216  [44864/74412]\n",
      "loss: 1.096990  [51264/74412]\n",
      "loss: 0.865586  [57664/74412]\n",
      "loss: 1.239219  [64064/74412]\n",
      "loss: 0.846461  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 1.194293 \n",
      "\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "loss: 1.542380  [   64/74412]\n",
      "loss: 1.014867  [ 6464/74412]\n",
      "loss: 0.974491  [12864/74412]\n",
      "loss: 0.999957  [19264/74412]\n",
      "loss: 1.120379  [25664/74412]\n",
      "loss: 1.252045  [32064/74412]\n",
      "loss: 1.136598  [38464/74412]\n",
      "loss: 0.983959  [44864/74412]\n",
      "loss: 1.096060  [51264/74412]\n",
      "loss: 0.863843  [57664/74412]\n",
      "loss: 1.238872  [64064/74412]\n",
      "loss: 0.846010  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 1.193442 \n",
      "\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "loss: 1.540078  [   64/74412]\n",
      "loss: 1.013675  [ 6464/74412]\n",
      "loss: 0.973691  [12864/74412]\n",
      "loss: 0.999371  [19264/74412]\n",
      "loss: 1.119441  [25664/74412]\n",
      "loss: 1.250546  [32064/74412]\n",
      "loss: 1.135771  [38464/74412]\n",
      "loss: 0.982760  [44864/74412]\n",
      "loss: 1.095827  [51264/74412]\n",
      "loss: 0.863028  [57664/74412]\n",
      "loss: 1.238341  [64064/74412]\n",
      "loss: 0.845002  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.192625 \n",
      "\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "loss: 1.538970  [   64/74412]\n",
      "loss: 1.014015  [ 6464/74412]\n",
      "loss: 0.973014  [12864/74412]\n",
      "loss: 0.998605  [19264/74412]\n",
      "loss: 1.118016  [25664/74412]\n",
      "loss: 1.249399  [32064/74412]\n",
      "loss: 1.135424  [38464/74412]\n",
      "loss: 0.982054  [44864/74412]\n",
      "loss: 1.094393  [51264/74412]\n",
      "loss: 0.861602  [57664/74412]\n",
      "loss: 1.236939  [64064/74412]\n",
      "loss: 0.844413  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.191898 \n",
      "\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "loss: 1.537873  [   64/74412]\n",
      "loss: 1.013203  [ 6464/74412]\n",
      "loss: 0.972104  [12864/74412]\n",
      "loss: 0.998568  [19264/74412]\n",
      "loss: 1.117244  [25664/74412]\n",
      "loss: 1.248239  [32064/74412]\n",
      "loss: 1.134274  [38464/74412]\n",
      "loss: 0.980585  [44864/74412]\n",
      "loss: 1.093951  [51264/74412]\n",
      "loss: 0.860478  [57664/74412]\n",
      "loss: 1.236648  [64064/74412]\n",
      "loss: 0.843917  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.191477 \n",
      "\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "loss: 1.538211  [   64/74412]\n",
      "loss: 1.013009  [ 6464/74412]\n",
      "loss: 0.972439  [12864/74412]\n",
      "loss: 0.997546  [19264/74412]\n",
      "loss: 1.116726  [25664/74412]\n",
      "loss: 1.246907  [32064/74412]\n",
      "loss: 1.133719  [38464/74412]\n",
      "loss: 0.979983  [44864/74412]\n",
      "loss: 1.093586  [51264/74412]\n",
      "loss: 0.859128  [57664/74412]\n",
      "loss: 1.235699  [64064/74412]\n",
      "loss: 0.843107  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.190511 \n",
      "\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "loss: 1.536562  [   64/74412]\n",
      "loss: 1.011953  [ 6464/74412]\n",
      "loss: 0.971231  [12864/74412]\n",
      "loss: 0.997108  [19264/74412]\n",
      "loss: 1.115259  [25664/74412]\n",
      "loss: 1.245696  [32064/74412]\n",
      "loss: 1.132927  [38464/74412]\n",
      "loss: 0.978951  [44864/74412]\n",
      "loss: 1.092668  [51264/74412]\n",
      "loss: 0.858789  [57664/74412]\n",
      "loss: 1.235609  [64064/74412]\n",
      "loss: 0.842565  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.189885 \n",
      "\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "loss: 1.535527  [   64/74412]\n",
      "loss: 1.011556  [ 6464/74412]\n",
      "loss: 0.971132  [12864/74412]\n",
      "loss: 0.996131  [19264/74412]\n",
      "loss: 1.114532  [25664/74412]\n",
      "loss: 1.244411  [32064/74412]\n",
      "loss: 1.132019  [38464/74412]\n",
      "loss: 0.978481  [44864/74412]\n",
      "loss: 1.092140  [51264/74412]\n",
      "loss: 0.858177  [57664/74412]\n",
      "loss: 1.235697  [64064/74412]\n",
      "loss: 0.842325  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.189363 \n",
      "\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "loss: 1.535029  [   64/74412]\n",
      "loss: 1.011022  [ 6464/74412]\n",
      "loss: 0.971825  [12864/74412]\n",
      "loss: 0.995844  [19264/74412]\n",
      "loss: 1.113347  [25664/74412]\n",
      "loss: 1.243052  [32064/74412]\n",
      "loss: 1.131094  [38464/74412]\n",
      "loss: 0.977100  [44864/74412]\n",
      "loss: 1.091840  [51264/74412]\n",
      "loss: 0.857179  [57664/74412]\n",
      "loss: 1.235091  [64064/74412]\n",
      "loss: 0.842364  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 1.188726 \n",
      "\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "loss: 1.534539  [   64/74412]\n",
      "loss: 1.010012  [ 6464/74412]\n",
      "loss: 0.970776  [12864/74412]\n",
      "loss: 0.995387  [19264/74412]\n",
      "loss: 1.112660  [25664/74412]\n",
      "loss: 1.241862  [32064/74412]\n",
      "loss: 1.131041  [38464/74412]\n",
      "loss: 0.975979  [44864/74412]\n",
      "loss: 1.091299  [51264/74412]\n",
      "loss: 0.855841  [57664/74412]\n",
      "loss: 1.234888  [64064/74412]\n",
      "loss: 0.841253  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 1.187932 \n",
      "\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "loss: 1.533614  [   64/74412]\n",
      "loss: 1.009328  [ 6464/74412]\n",
      "loss: 0.971239  [12864/74412]\n",
      "loss: 0.994887  [19264/74412]\n",
      "loss: 1.111659  [25664/74412]\n",
      "loss: 1.240988  [32064/74412]\n",
      "loss: 1.129543  [38464/74412]\n",
      "loss: 0.974343  [44864/74412]\n",
      "loss: 1.090603  [51264/74412]\n",
      "loss: 0.854316  [57664/74412]\n",
      "loss: 1.233769  [64064/74412]\n",
      "loss: 0.841210  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 1.187206 \n",
      "\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "loss: 1.532398  [   64/74412]\n",
      "loss: 1.009375  [ 6464/74412]\n",
      "loss: 0.970898  [12864/74412]\n",
      "loss: 0.993978  [19264/74412]\n",
      "loss: 1.110445  [25664/74412]\n",
      "loss: 1.239890  [32064/74412]\n",
      "loss: 1.129263  [38464/74412]\n",
      "loss: 0.973242  [44864/74412]\n",
      "loss: 1.090020  [51264/74412]\n",
      "loss: 0.853081  [57664/74412]\n",
      "loss: 1.232827  [64064/74412]\n",
      "loss: 0.840756  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 1.186382 \n",
      "\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "loss: 1.530955  [   64/74412]\n",
      "loss: 1.008540  [ 6464/74412]\n",
      "loss: 0.970445  [12864/74412]\n",
      "loss: 0.993353  [19264/74412]\n",
      "loss: 1.109519  [25664/74412]\n",
      "loss: 1.237438  [32064/74412]\n",
      "loss: 1.128153  [38464/74412]\n",
      "loss: 0.972324  [44864/74412]\n",
      "loss: 1.089699  [51264/74412]\n",
      "loss: 0.851613  [57664/74412]\n",
      "loss: 1.232762  [64064/74412]\n",
      "loss: 0.839920  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 1.185529 \n",
      "\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "loss: 1.529143  [   64/74412]\n",
      "loss: 1.008054  [ 6464/74412]\n",
      "loss: 0.970441  [12864/74412]\n",
      "loss: 0.992566  [19264/74412]\n",
      "loss: 1.109419  [25664/74412]\n",
      "loss: 1.237441  [32064/74412]\n",
      "loss: 1.127176  [38464/74412]\n",
      "loss: 0.971763  [44864/74412]\n",
      "loss: 1.088705  [51264/74412]\n",
      "loss: 0.850383  [57664/74412]\n",
      "loss: 1.231447  [64064/74412]\n",
      "loss: 0.839378  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 1.184883 \n",
      "\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "loss: 1.528464  [   64/74412]\n",
      "loss: 1.007941  [ 6464/74412]\n",
      "loss: 0.969285  [12864/74412]\n",
      "loss: 0.992281  [19264/74412]\n",
      "loss: 1.107977  [25664/74412]\n",
      "loss: 1.236319  [32064/74412]\n",
      "loss: 1.126466  [38464/74412]\n",
      "loss: 0.970056  [44864/74412]\n",
      "loss: 1.088257  [51264/74412]\n",
      "loss: 0.849546  [57664/74412]\n",
      "loss: 1.231651  [64064/74412]\n",
      "loss: 0.838472  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 1.184411 \n",
      "\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "loss: 1.527686  [   64/74412]\n",
      "loss: 1.007458  [ 6464/74412]\n",
      "loss: 0.968478  [12864/74412]\n",
      "loss: 0.991642  [19264/74412]\n",
      "loss: 1.107856  [25664/74412]\n",
      "loss: 1.235092  [32064/74412]\n",
      "loss: 1.125271  [38464/74412]\n",
      "loss: 0.969584  [44864/74412]\n",
      "loss: 1.087351  [51264/74412]\n",
      "loss: 0.848605  [57664/74412]\n",
      "loss: 1.231297  [64064/74412]\n",
      "loss: 0.838647  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 1.183730 \n",
      "\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "loss: 1.527002  [   64/74412]\n",
      "loss: 1.006289  [ 6464/74412]\n",
      "loss: 0.968731  [12864/74412]\n",
      "loss: 0.991537  [19264/74412]\n",
      "loss: 1.106645  [25664/74412]\n",
      "loss: 1.233792  [32064/74412]\n",
      "loss: 1.125352  [38464/74412]\n",
      "loss: 0.968081  [44864/74412]\n",
      "loss: 1.086915  [51264/74412]\n",
      "loss: 0.847568  [57664/74412]\n",
      "loss: 1.231175  [64064/74412]\n",
      "loss: 0.837690  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 1.183167 \n",
      "\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "loss: 1.525956  [   64/74412]\n",
      "loss: 1.005592  [ 6464/74412]\n",
      "loss: 0.967956  [12864/74412]\n",
      "loss: 0.990218  [19264/74412]\n",
      "loss: 1.105610  [25664/74412]\n",
      "loss: 1.232834  [32064/74412]\n",
      "loss: 1.124086  [38464/74412]\n",
      "loss: 0.966766  [44864/74412]\n",
      "loss: 1.086481  [51264/74412]\n",
      "loss: 0.846555  [57664/74412]\n",
      "loss: 1.229899  [64064/74412]\n",
      "loss: 0.837169  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 1.182804 \n",
      "\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "loss: 1.526355  [   64/74412]\n",
      "loss: 1.005528  [ 6464/74412]\n",
      "loss: 0.968042  [12864/74412]\n",
      "loss: 0.990419  [19264/74412]\n",
      "loss: 1.105440  [25664/74412]\n",
      "loss: 1.232325  [32064/74412]\n",
      "loss: 1.123098  [38464/74412]\n",
      "loss: 0.966530  [44864/74412]\n",
      "loss: 1.085824  [51264/74412]\n",
      "loss: 0.845866  [57664/74412]\n",
      "loss: 1.229571  [64064/74412]\n",
      "loss: 0.836350  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 1.182031 \n",
      "\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "loss: 1.525223  [   64/74412]\n",
      "loss: 1.004265  [ 6464/74412]\n",
      "loss: 0.966736  [12864/74412]\n",
      "loss: 0.990124  [19264/74412]\n",
      "loss: 1.104432  [25664/74412]\n",
      "loss: 1.231109  [32064/74412]\n",
      "loss: 1.122453  [38464/74412]\n",
      "loss: 0.965819  [44864/74412]\n",
      "loss: 1.084748  [51264/74412]\n",
      "loss: 0.845018  [57664/74412]\n",
      "loss: 1.229156  [64064/74412]\n",
      "loss: 0.835926  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 1.182042 \n",
      "\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "loss: 1.525655  [   64/74412]\n",
      "loss: 1.003322  [ 6464/74412]\n",
      "loss: 0.967602  [12864/74412]\n",
      "loss: 0.988936  [19264/74412]\n",
      "loss: 1.103877  [25664/74412]\n",
      "loss: 1.229932  [32064/74412]\n",
      "loss: 1.121723  [38464/74412]\n",
      "loss: 0.964620  [44864/74412]\n",
      "loss: 1.084926  [51264/74412]\n",
      "loss: 0.843756  [57664/74412]\n",
      "loss: 1.228952  [64064/74412]\n",
      "loss: 0.834992  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 1.181593 \n",
      "\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "loss: 1.524083  [   64/74412]\n",
      "loss: 1.003632  [ 6464/74412]\n",
      "loss: 0.966631  [12864/74412]\n",
      "loss: 0.988813  [19264/74412]\n",
      "loss: 1.103658  [25664/74412]\n",
      "loss: 1.229215  [32064/74412]\n",
      "loss: 1.120968  [38464/74412]\n",
      "loss: 0.963661  [44864/74412]\n",
      "loss: 1.084295  [51264/74412]\n",
      "loss: 0.842802  [57664/74412]\n",
      "loss: 1.227614  [64064/74412]\n",
      "loss: 0.834860  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 1.180734 \n",
      "\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "loss: 1.523023  [   64/74412]\n",
      "loss: 1.002920  [ 6464/74412]\n",
      "loss: 0.966869  [12864/74412]\n",
      "loss: 0.988833  [19264/74412]\n",
      "loss: 1.102444  [25664/74412]\n",
      "loss: 1.227793  [32064/74412]\n",
      "loss: 1.120320  [38464/74412]\n",
      "loss: 0.961968  [44864/74412]\n",
      "loss: 1.083720  [51264/74412]\n",
      "loss: 0.841668  [57664/74412]\n",
      "loss: 1.227769  [64064/74412]\n",
      "loss: 0.834222  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 1.179960 \n",
      "\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "loss: 1.522030  [   64/74412]\n",
      "loss: 1.002363  [ 6464/74412]\n",
      "loss: 0.966065  [12864/74412]\n",
      "loss: 0.987379  [19264/74412]\n",
      "loss: 1.102037  [25664/74412]\n",
      "loss: 1.227832  [32064/74412]\n",
      "loss: 1.119148  [38464/74412]\n",
      "loss: 0.960976  [44864/74412]\n",
      "loss: 1.083084  [51264/74412]\n",
      "loss: 0.840303  [57664/74412]\n",
      "loss: 1.226345  [64064/74412]\n",
      "loss: 0.833930  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 1.179435 \n",
      "\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "loss: 1.520751  [   64/74412]\n",
      "loss: 1.002363  [ 6464/74412]\n",
      "loss: 0.964575  [12864/74412]\n",
      "loss: 0.987170  [19264/74412]\n",
      "loss: 1.100779  [25664/74412]\n",
      "loss: 1.226900  [32064/74412]\n",
      "loss: 1.118046  [38464/74412]\n",
      "loss: 0.960385  [44864/74412]\n",
      "loss: 1.082712  [51264/74412]\n",
      "loss: 0.839971  [57664/74412]\n",
      "loss: 1.226725  [64064/74412]\n",
      "loss: 0.833614  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 1.178778 \n",
      "\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "loss: 1.519901  [   64/74412]\n",
      "loss: 1.001158  [ 6464/74412]\n",
      "loss: 0.965349  [12864/74412]\n",
      "loss: 0.987198  [19264/74412]\n",
      "loss: 1.100344  [25664/74412]\n",
      "loss: 1.225331  [32064/74412]\n",
      "loss: 1.117477  [38464/74412]\n",
      "loss: 0.959310  [44864/74412]\n",
      "loss: 1.081870  [51264/74412]\n",
      "loss: 0.838693  [57664/74412]\n",
      "loss: 1.224897  [64064/74412]\n",
      "loss: 0.832806  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.178010 \n",
      "\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "loss: 1.518622  [   64/74412]\n",
      "loss: 1.000601  [ 6464/74412]\n",
      "loss: 0.964727  [12864/74412]\n",
      "loss: 0.986714  [19264/74412]\n",
      "loss: 1.099127  [25664/74412]\n",
      "loss: 1.224826  [32064/74412]\n",
      "loss: 1.116359  [38464/74412]\n",
      "loss: 0.958354  [44864/74412]\n",
      "loss: 1.081574  [51264/74412]\n",
      "loss: 0.837982  [57664/74412]\n",
      "loss: 1.224885  [64064/74412]\n",
      "loss: 0.832470  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.177408 \n",
      "\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "loss: 1.518401  [   64/74412]\n",
      "loss: 1.000054  [ 6464/74412]\n",
      "loss: 0.964919  [12864/74412]\n",
      "loss: 0.986082  [19264/74412]\n",
      "loss: 1.098261  [25664/74412]\n",
      "loss: 1.222557  [32064/74412]\n",
      "loss: 1.115886  [38464/74412]\n",
      "loss: 0.956654  [44864/74412]\n",
      "loss: 1.080632  [51264/74412]\n",
      "loss: 0.837145  [57664/74412]\n",
      "loss: 1.224664  [64064/74412]\n",
      "loss: 0.832062  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.176675 \n",
      "\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "loss: 1.516989  [   64/74412]\n",
      "loss: 1.000090  [ 6464/74412]\n",
      "loss: 0.964024  [12864/74412]\n",
      "loss: 0.984913  [19264/74412]\n",
      "loss: 1.097398  [25664/74412]\n",
      "loss: 1.222026  [32064/74412]\n",
      "loss: 1.115016  [38464/74412]\n",
      "loss: 0.955900  [44864/74412]\n",
      "loss: 1.080992  [51264/74412]\n",
      "loss: 0.836028  [57664/74412]\n",
      "loss: 1.223419  [64064/74412]\n",
      "loss: 0.831531  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.176069 \n",
      "\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "loss: 1.516027  [   64/74412]\n",
      "loss: 0.999461  [ 6464/74412]\n",
      "loss: 0.963353  [12864/74412]\n",
      "loss: 0.984157  [19264/74412]\n",
      "loss: 1.097333  [25664/74412]\n",
      "loss: 1.221573  [32064/74412]\n",
      "loss: 1.114323  [38464/74412]\n",
      "loss: 0.955211  [44864/74412]\n",
      "loss: 1.079785  [51264/74412]\n",
      "loss: 0.835335  [57664/74412]\n",
      "loss: 1.223145  [64064/74412]\n",
      "loss: 0.831980  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.175401 \n",
      "\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "loss: 1.514877  [   64/74412]\n",
      "loss: 0.998447  [ 6464/74412]\n",
      "loss: 0.962923  [12864/74412]\n",
      "loss: 0.983668  [19264/74412]\n",
      "loss: 1.096907  [25664/74412]\n",
      "loss: 1.219524  [32064/74412]\n",
      "loss: 1.113233  [38464/74412]\n",
      "loss: 0.954712  [44864/74412]\n",
      "loss: 1.079115  [51264/74412]\n",
      "loss: 0.834701  [57664/74412]\n",
      "loss: 1.221915  [64064/74412]\n",
      "loss: 0.830995  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 1.174552 \n",
      "\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "loss: 1.513598  [   64/74412]\n",
      "loss: 0.997565  [ 6464/74412]\n",
      "loss: 0.962104  [12864/74412]\n",
      "loss: 0.982994  [19264/74412]\n",
      "loss: 1.095213  [25664/74412]\n",
      "loss: 1.219022  [32064/74412]\n",
      "loss: 1.112593  [38464/74412]\n",
      "loss: 0.953497  [44864/74412]\n",
      "loss: 1.078755  [51264/74412]\n",
      "loss: 0.833638  [57664/74412]\n",
      "loss: 1.220624  [64064/74412]\n",
      "loss: 0.830330  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.174235 \n",
      "\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "loss: 1.514086  [   64/74412]\n",
      "loss: 0.997382  [ 6464/74412]\n",
      "loss: 0.961183  [12864/74412]\n",
      "loss: 0.982454  [19264/74412]\n",
      "loss: 1.094213  [25664/74412]\n",
      "loss: 1.217208  [32064/74412]\n",
      "loss: 1.111660  [38464/74412]\n",
      "loss: 0.952477  [44864/74412]\n",
      "loss: 1.078303  [51264/74412]\n",
      "loss: 0.832807  [57664/74412]\n",
      "loss: 1.220515  [64064/74412]\n",
      "loss: 0.829493  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 1.173598 \n",
      "\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "loss: 1.512745  [   64/74412]\n",
      "loss: 0.996326  [ 6464/74412]\n",
      "loss: 0.960762  [12864/74412]\n",
      "loss: 0.982457  [19264/74412]\n",
      "loss: 1.094564  [25664/74412]\n",
      "loss: 1.216434  [32064/74412]\n",
      "loss: 1.111045  [38464/74412]\n",
      "loss: 0.951330  [44864/74412]\n",
      "loss: 1.077042  [51264/74412]\n",
      "loss: 0.831654  [57664/74412]\n",
      "loss: 1.220176  [64064/74412]\n",
      "loss: 0.829209  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 1.172614 \n",
      "\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "loss: 1.511097  [   64/74412]\n",
      "loss: 0.995398  [ 6464/74412]\n",
      "loss: 0.960450  [12864/74412]\n",
      "loss: 0.981607  [19264/74412]\n",
      "loss: 1.094634  [25664/74412]\n",
      "loss: 1.215502  [32064/74412]\n",
      "loss: 1.110375  [38464/74412]\n",
      "loss: 0.950739  [44864/74412]\n",
      "loss: 1.077537  [51264/74412]\n",
      "loss: 0.830522  [57664/74412]\n",
      "loss: 1.219936  [64064/74412]\n",
      "loss: 0.828748  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 1.171926 \n",
      "\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "loss: 1.509634  [   64/74412]\n",
      "loss: 0.994793  [ 6464/74412]\n",
      "loss: 0.960664  [12864/74412]\n",
      "loss: 0.981545  [19264/74412]\n",
      "loss: 1.093026  [25664/74412]\n",
      "loss: 1.214515  [32064/74412]\n",
      "loss: 1.109293  [38464/74412]\n",
      "loss: 0.949788  [44864/74412]\n",
      "loss: 1.076429  [51264/74412]\n",
      "loss: 0.830046  [57664/74412]\n",
      "loss: 1.219073  [64064/74412]\n",
      "loss: 0.828373  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 1.171091 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "loss: 1.508713  [   64/74412]\n",
      "loss: 0.994305  [ 6464/74412]\n",
      "loss: 0.959660  [12864/74412]\n",
      "loss: 0.980609  [19264/74412]\n",
      "loss: 1.092012  [25664/74412]\n",
      "loss: 1.213650  [32064/74412]\n",
      "loss: 1.108924  [38464/74412]\n",
      "loss: 0.949287  [44864/74412]\n",
      "loss: 1.075923  [51264/74412]\n",
      "loss: 0.829307  [57664/74412]\n",
      "loss: 1.219459  [64064/74412]\n",
      "loss: 0.828103  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 1.170544 \n",
      "\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "loss: 1.507408  [   64/74412]\n",
      "loss: 0.994716  [ 6464/74412]\n",
      "loss: 0.959403  [12864/74412]\n",
      "loss: 0.979632  [19264/74412]\n",
      "loss: 1.090576  [25664/74412]\n",
      "loss: 1.212516  [32064/74412]\n",
      "loss: 1.107805  [38464/74412]\n",
      "loss: 0.948300  [44864/74412]\n",
      "loss: 1.075214  [51264/74412]\n",
      "loss: 0.828452  [57664/74412]\n",
      "loss: 1.218573  [64064/74412]\n",
      "loss: 0.826897  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 1.170387 \n",
      "\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "loss: 1.507411  [   64/74412]\n",
      "loss: 0.992802  [ 6464/74412]\n",
      "loss: 0.958997  [12864/74412]\n",
      "loss: 0.979624  [19264/74412]\n",
      "loss: 1.090468  [25664/74412]\n",
      "loss: 1.211229  [32064/74412]\n",
      "loss: 1.107401  [38464/74412]\n",
      "loss: 0.947357  [44864/74412]\n",
      "loss: 1.075163  [51264/74412]\n",
      "loss: 0.826960  [57664/74412]\n",
      "loss: 1.217921  [64064/74412]\n",
      "loss: 0.826946  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 1.169525 \n",
      "\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "loss: 1.505777  [   64/74412]\n",
      "loss: 0.992688  [ 6464/74412]\n",
      "loss: 0.958810  [12864/74412]\n",
      "loss: 0.978386  [19264/74412]\n",
      "loss: 1.089913  [25664/74412]\n",
      "loss: 1.210651  [32064/74412]\n",
      "loss: 1.106279  [38464/74412]\n",
      "loss: 0.946621  [44864/74412]\n",
      "loss: 1.074444  [51264/74412]\n",
      "loss: 0.826221  [57664/74412]\n",
      "loss: 1.218655  [64064/74412]\n",
      "loss: 0.826508  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 1.169347 \n",
      "\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "loss: 1.505018  [   64/74412]\n",
      "loss: 0.992955  [ 6464/74412]\n",
      "loss: 0.957916  [12864/74412]\n",
      "loss: 0.978060  [19264/74412]\n",
      "loss: 1.089389  [25664/74412]\n",
      "loss: 1.208873  [32064/74412]\n",
      "loss: 1.105229  [38464/74412]\n",
      "loss: 0.945688  [44864/74412]\n",
      "loss: 1.073166  [51264/74412]\n",
      "loss: 0.825495  [57664/74412]\n",
      "loss: 1.216449  [64064/74412]\n",
      "loss: 0.826382  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 1.168789 \n",
      "\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "loss: 1.504395  [   64/74412]\n",
      "loss: 0.992219  [ 6464/74412]\n",
      "loss: 0.957493  [12864/74412]\n",
      "loss: 0.977258  [19264/74412]\n",
      "loss: 1.088436  [25664/74412]\n",
      "loss: 1.207454  [32064/74412]\n",
      "loss: 1.105226  [38464/74412]\n",
      "loss: 0.944700  [44864/74412]\n",
      "loss: 1.073809  [51264/74412]\n",
      "loss: 0.824164  [57664/74412]\n",
      "loss: 1.216839  [64064/74412]\n",
      "loss: 0.826156  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 1.168087 \n",
      "\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "loss: 1.504058  [   64/74412]\n",
      "loss: 0.991674  [ 6464/74412]\n",
      "loss: 0.956595  [12864/74412]\n",
      "loss: 0.976249  [19264/74412]\n",
      "loss: 1.088277  [25664/74412]\n",
      "loss: 1.207415  [32064/74412]\n",
      "loss: 1.104185  [38464/74412]\n",
      "loss: 0.943488  [44864/74412]\n",
      "loss: 1.072090  [51264/74412]\n",
      "loss: 0.823611  [57664/74412]\n",
      "loss: 1.216563  [64064/74412]\n",
      "loss: 0.826186  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 1.167694 \n",
      "\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "loss: 1.502562  [   64/74412]\n",
      "loss: 0.991035  [ 6464/74412]\n",
      "loss: 0.957093  [12864/74412]\n",
      "loss: 0.975522  [19264/74412]\n",
      "loss: 1.087174  [25664/74412]\n",
      "loss: 1.206578  [32064/74412]\n",
      "loss: 1.103896  [38464/74412]\n",
      "loss: 0.942496  [44864/74412]\n",
      "loss: 1.072437  [51264/74412]\n",
      "loss: 0.822891  [57664/74412]\n",
      "loss: 1.216151  [64064/74412]\n",
      "loss: 0.825708  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 1.166766 \n",
      "\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "loss: 1.501454  [   64/74412]\n",
      "loss: 0.990203  [ 6464/74412]\n",
      "loss: 0.956625  [12864/74412]\n",
      "loss: 0.975620  [19264/74412]\n",
      "loss: 1.085877  [25664/74412]\n",
      "loss: 1.205272  [32064/74412]\n",
      "loss: 1.102628  [38464/74412]\n",
      "loss: 0.941416  [44864/74412]\n",
      "loss: 1.070778  [51264/74412]\n",
      "loss: 0.821306  [57664/74412]\n",
      "loss: 1.215555  [64064/74412]\n",
      "loss: 0.825065  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 1.166238 \n",
      "\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "loss: 1.501060  [   64/74412]\n",
      "loss: 0.988795  [ 6464/74412]\n",
      "loss: 0.955985  [12864/74412]\n",
      "loss: 0.975102  [19264/74412]\n",
      "loss: 1.084862  [25664/74412]\n",
      "loss: 1.204155  [32064/74412]\n",
      "loss: 1.101820  [38464/74412]\n",
      "loss: 0.940741  [44864/74412]\n",
      "loss: 1.069982  [51264/74412]\n",
      "loss: 0.820563  [57664/74412]\n",
      "loss: 1.215363  [64064/74412]\n",
      "loss: 0.824859  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 1.165836 \n",
      "\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "loss: 1.500740  [   64/74412]\n",
      "loss: 0.989328  [ 6464/74412]\n",
      "loss: 0.955571  [12864/74412]\n",
      "loss: 0.974233  [19264/74412]\n",
      "loss: 1.083734  [25664/74412]\n",
      "loss: 1.203473  [32064/74412]\n",
      "loss: 1.101527  [38464/74412]\n",
      "loss: 0.940018  [44864/74412]\n",
      "loss: 1.069459  [51264/74412]\n",
      "loss: 0.819769  [57664/74412]\n",
      "loss: 1.214924  [64064/74412]\n",
      "loss: 0.824459  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 1.165295 \n",
      "\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "loss: 1.499664  [   64/74412]\n",
      "loss: 0.988913  [ 6464/74412]\n",
      "loss: 0.954943  [12864/74412]\n",
      "loss: 0.974204  [19264/74412]\n",
      "loss: 1.082515  [25664/74412]\n",
      "loss: 1.201826  [32064/74412]\n",
      "loss: 1.100793  [38464/74412]\n",
      "loss: 0.939209  [44864/74412]\n",
      "loss: 1.068880  [51264/74412]\n",
      "loss: 0.819259  [57664/74412]\n",
      "loss: 1.213397  [64064/74412]\n",
      "loss: 0.824585  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 1.164765 \n",
      "\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "loss: 1.498557  [   64/74412]\n",
      "loss: 0.989240  [ 6464/74412]\n",
      "loss: 0.954755  [12864/74412]\n",
      "loss: 0.973429  [19264/74412]\n",
      "loss: 1.081562  [25664/74412]\n",
      "loss: 1.201158  [32064/74412]\n",
      "loss: 1.099895  [38464/74412]\n",
      "loss: 0.939044  [44864/74412]\n",
      "loss: 1.068476  [51264/74412]\n",
      "loss: 0.818610  [57664/74412]\n",
      "loss: 1.213068  [64064/74412]\n",
      "loss: 0.823924  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 1.164186 \n",
      "\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "loss: 1.496971  [   64/74412]\n",
      "loss: 0.987643  [ 6464/74412]\n",
      "loss: 0.954327  [12864/74412]\n",
      "loss: 0.972173  [19264/74412]\n",
      "loss: 1.080389  [25664/74412]\n",
      "loss: 1.200211  [32064/74412]\n",
      "loss: 1.099018  [38464/74412]\n",
      "loss: 0.937182  [44864/74412]\n",
      "loss: 1.067439  [51264/74412]\n",
      "loss: 0.817243  [57664/74412]\n",
      "loss: 1.213215  [64064/74412]\n",
      "loss: 0.823692  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 1.163328 \n",
      "\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "loss: 1.495525  [   64/74412]\n",
      "loss: 0.986708  [ 6464/74412]\n",
      "loss: 0.953949  [12864/74412]\n",
      "loss: 0.972404  [19264/74412]\n",
      "loss: 1.080401  [25664/74412]\n",
      "loss: 1.199246  [32064/74412]\n",
      "loss: 1.098354  [38464/74412]\n",
      "loss: 0.935866  [44864/74412]\n",
      "loss: 1.067515  [51264/74412]\n",
      "loss: 0.815943  [57664/74412]\n",
      "loss: 1.212223  [64064/74412]\n",
      "loss: 0.823355  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 1.162581 \n",
      "\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "loss: 1.495052  [   64/74412]\n",
      "loss: 0.986824  [ 6464/74412]\n",
      "loss: 0.953227  [12864/74412]\n",
      "loss: 0.971280  [19264/74412]\n",
      "loss: 1.079861  [25664/74412]\n",
      "loss: 1.198130  [32064/74412]\n",
      "loss: 1.097376  [38464/74412]\n",
      "loss: 0.935966  [44864/74412]\n",
      "loss: 1.066399  [51264/74412]\n",
      "loss: 0.814567  [57664/74412]\n",
      "loss: 1.212312  [64064/74412]\n",
      "loss: 0.822691  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 1.162212 \n",
      "\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "loss: 1.493910  [   64/74412]\n",
      "loss: 0.986646  [ 6464/74412]\n",
      "loss: 0.953330  [12864/74412]\n",
      "loss: 0.971559  [19264/74412]\n",
      "loss: 1.077807  [25664/74412]\n",
      "loss: 1.196650  [32064/74412]\n",
      "loss: 1.096669  [38464/74412]\n",
      "loss: 0.933331  [44864/74412]\n",
      "loss: 1.066153  [51264/74412]\n",
      "loss: 0.814064  [57664/74412]\n",
      "loss: 1.211788  [64064/74412]\n",
      "loss: 0.822341  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 1.161727 \n",
      "\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "loss: 1.493106  [   64/74412]\n",
      "loss: 0.985023  [ 6464/74412]\n",
      "loss: 0.952051  [12864/74412]\n",
      "loss: 0.970977  [19264/74412]\n",
      "loss: 1.077533  [25664/74412]\n",
      "loss: 1.196043  [32064/74412]\n",
      "loss: 1.096405  [38464/74412]\n",
      "loss: 0.932692  [44864/74412]\n",
      "loss: 1.065945  [51264/74412]\n",
      "loss: 0.813797  [57664/74412]\n",
      "loss: 1.210867  [64064/74412]\n",
      "loss: 0.822572  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 1.161182 \n",
      "\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "loss: 1.492633  [   64/74412]\n",
      "loss: 0.984268  [ 6464/74412]\n",
      "loss: 0.952177  [12864/74412]\n",
      "loss: 0.969644  [19264/74412]\n",
      "loss: 1.077291  [25664/74412]\n",
      "loss: 1.195197  [32064/74412]\n",
      "loss: 1.095691  [38464/74412]\n",
      "loss: 0.930731  [44864/74412]\n",
      "loss: 1.065008  [51264/74412]\n",
      "loss: 0.812273  [57664/74412]\n",
      "loss: 1.210546  [64064/74412]\n",
      "loss: 0.821902  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 1.160811 \n",
      "\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "loss: 1.491880  [   64/74412]\n",
      "loss: 0.983794  [ 6464/74412]\n",
      "loss: 0.951893  [12864/74412]\n",
      "loss: 0.968850  [19264/74412]\n",
      "loss: 1.076584  [25664/74412]\n",
      "loss: 1.194077  [32064/74412]\n",
      "loss: 1.094926  [38464/74412]\n",
      "loss: 0.930528  [44864/74412]\n",
      "loss: 1.064573  [51264/74412]\n",
      "loss: 0.810600  [57664/74412]\n",
      "loss: 1.209381  [64064/74412]\n",
      "loss: 0.821768  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 1.160347 \n",
      "\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "loss: 1.490708  [   64/74412]\n",
      "loss: 0.983150  [ 6464/74412]\n",
      "loss: 0.952283  [12864/74412]\n",
      "loss: 0.968022  [19264/74412]\n",
      "loss: 1.075115  [25664/74412]\n",
      "loss: 1.193004  [32064/74412]\n",
      "loss: 1.093354  [38464/74412]\n",
      "loss: 0.929223  [44864/74412]\n",
      "loss: 1.064559  [51264/74412]\n",
      "loss: 0.809383  [57664/74412]\n",
      "loss: 1.209359  [64064/74412]\n",
      "loss: 0.821426  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 1.159979 \n",
      "\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "loss: 1.489258  [   64/74412]\n",
      "loss: 0.982621  [ 6464/74412]\n",
      "loss: 0.951451  [12864/74412]\n",
      "loss: 0.967415  [19264/74412]\n",
      "loss: 1.074508  [25664/74412]\n",
      "loss: 1.192481  [32064/74412]\n",
      "loss: 1.093331  [38464/74412]\n",
      "loss: 0.928497  [44864/74412]\n",
      "loss: 1.063730  [51264/74412]\n",
      "loss: 0.808595  [57664/74412]\n",
      "loss: 1.207152  [64064/74412]\n",
      "loss: 0.821012  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 1.159129 \n",
      "\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "loss: 1.488302  [   64/74412]\n",
      "loss: 0.982437  [ 6464/74412]\n",
      "loss: 0.950868  [12864/74412]\n",
      "loss: 0.966594  [19264/74412]\n",
      "loss: 1.073688  [25664/74412]\n",
      "loss: 1.190711  [32064/74412]\n",
      "loss: 1.092291  [38464/74412]\n",
      "loss: 0.927819  [44864/74412]\n",
      "loss: 1.062823  [51264/74412]\n",
      "loss: 0.807970  [57664/74412]\n",
      "loss: 1.207637  [64064/74412]\n",
      "loss: 0.820037  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 1.158397 \n",
      "\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "loss: 1.486932  [   64/74412]\n",
      "loss: 0.981784  [ 6464/74412]\n",
      "loss: 0.950215  [12864/74412]\n",
      "loss: 0.966312  [19264/74412]\n",
      "loss: 1.072267  [25664/74412]\n",
      "loss: 1.189592  [32064/74412]\n",
      "loss: 1.091836  [38464/74412]\n",
      "loss: 0.926171  [44864/74412]\n",
      "loss: 1.062694  [51264/74412]\n",
      "loss: 0.806825  [57664/74412]\n",
      "loss: 1.208087  [64064/74412]\n",
      "loss: 0.819931  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 1.159301 \n",
      "\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "loss: 1.486946  [   64/74412]\n",
      "loss: 0.981309  [ 6464/74412]\n",
      "loss: 0.950030  [12864/74412]\n",
      "loss: 0.965826  [19264/74412]\n",
      "loss: 1.071773  [25664/74412]\n",
      "loss: 1.188375  [32064/74412]\n",
      "loss: 1.091447  [38464/74412]\n",
      "loss: 0.926221  [44864/74412]\n",
      "loss: 1.061715  [51264/74412]\n",
      "loss: 0.805897  [57664/74412]\n",
      "loss: 1.206903  [64064/74412]\n",
      "loss: 0.819538  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 1.156608 \n",
      "\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "loss: 1.484650  [   64/74412]\n",
      "loss: 0.979921  [ 6464/74412]\n",
      "loss: 0.949439  [12864/74412]\n",
      "loss: 0.965260  [19264/74412]\n",
      "loss: 1.070178  [25664/74412]\n",
      "loss: 1.187016  [32064/74412]\n",
      "loss: 1.090595  [38464/74412]\n",
      "loss: 0.925205  [44864/74412]\n",
      "loss: 1.061503  [51264/74412]\n",
      "loss: 0.804977  [57664/74412]\n",
      "loss: 1.206972  [64064/74412]\n",
      "loss: 0.819193  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 1.155851 \n",
      "\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "loss: 1.483404  [   64/74412]\n",
      "loss: 0.979665  [ 6464/74412]\n",
      "loss: 0.949158  [12864/74412]\n",
      "loss: 0.964639  [19264/74412]\n",
      "loss: 1.069207  [25664/74412]\n",
      "loss: 1.186008  [32064/74412]\n",
      "loss: 1.089790  [38464/74412]\n",
      "loss: 0.924261  [44864/74412]\n",
      "loss: 1.061062  [51264/74412]\n",
      "loss: 0.803707  [57664/74412]\n",
      "loss: 1.206245  [64064/74412]\n",
      "loss: 0.818835  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 1.156855 \n",
      "\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "loss: 1.483531  [   64/74412]\n",
      "loss: 0.979024  [ 6464/74412]\n",
      "loss: 0.948772  [12864/74412]\n",
      "loss: 0.963091  [19264/74412]\n",
      "loss: 1.067954  [25664/74412]\n",
      "loss: 1.185159  [32064/74412]\n",
      "loss: 1.088344  [38464/74412]\n",
      "loss: 0.923011  [44864/74412]\n",
      "loss: 1.060448  [51264/74412]\n",
      "loss: 0.802710  [57664/74412]\n",
      "loss: 1.204744  [64064/74412]\n",
      "loss: 0.818487  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 1.154973 \n",
      "\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "loss: 1.481798  [   64/74412]\n",
      "loss: 0.978600  [ 6464/74412]\n",
      "loss: 0.948441  [12864/74412]\n",
      "loss: 0.962462  [19264/74412]\n",
      "loss: 1.067764  [25664/74412]\n",
      "loss: 1.184567  [32064/74412]\n",
      "loss: 1.087887  [38464/74412]\n",
      "loss: 0.922065  [44864/74412]\n",
      "loss: 1.059419  [51264/74412]\n",
      "loss: 0.802186  [57664/74412]\n",
      "loss: 1.204970  [64064/74412]\n",
      "loss: 0.818241  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 1.154497 \n",
      "\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "loss: 1.480704  [   64/74412]\n",
      "loss: 0.978261  [ 6464/74412]\n",
      "loss: 0.948069  [12864/74412]\n",
      "loss: 0.962517  [19264/74412]\n",
      "loss: 1.067705  [25664/74412]\n",
      "loss: 1.183128  [32064/74412]\n",
      "loss: 1.087544  [38464/74412]\n",
      "loss: 0.921032  [44864/74412]\n",
      "loss: 1.058962  [51264/74412]\n",
      "loss: 0.801485  [57664/74412]\n",
      "loss: 1.203617  [64064/74412]\n",
      "loss: 0.817305  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 1.153801 \n",
      "\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "loss: 1.478939  [   64/74412]\n",
      "loss: 0.978440  [ 6464/74412]\n",
      "loss: 0.947540  [12864/74412]\n",
      "loss: 0.962241  [19264/74412]\n",
      "loss: 1.066721  [25664/74412]\n",
      "loss: 1.182071  [32064/74412]\n",
      "loss: 1.086403  [38464/74412]\n",
      "loss: 0.920743  [44864/74412]\n",
      "loss: 1.058418  [51264/74412]\n",
      "loss: 0.799702  [57664/74412]\n",
      "loss: 1.203719  [64064/74412]\n",
      "loss: 0.816969  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 1.153093 \n",
      "\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "loss: 1.477658  [   64/74412]\n",
      "loss: 0.977658  [ 6464/74412]\n",
      "loss: 0.947354  [12864/74412]\n",
      "loss: 0.960713  [19264/74412]\n",
      "loss: 1.065764  [25664/74412]\n",
      "loss: 1.180671  [32064/74412]\n",
      "loss: 1.085469  [38464/74412]\n",
      "loss: 0.919387  [44864/74412]\n",
      "loss: 1.058156  [51264/74412]\n",
      "loss: 0.799758  [57664/74412]\n",
      "loss: 1.202711  [64064/74412]\n",
      "loss: 0.816323  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 1.152100 \n",
      "\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "loss: 1.476976  [   64/74412]\n",
      "loss: 0.977156  [ 6464/74412]\n",
      "loss: 0.947185  [12864/74412]\n",
      "loss: 0.960618  [19264/74412]\n",
      "loss: 1.064597  [25664/74412]\n",
      "loss: 1.179934  [32064/74412]\n",
      "loss: 1.085737  [38464/74412]\n",
      "loss: 0.918621  [44864/74412]\n",
      "loss: 1.057144  [51264/74412]\n",
      "loss: 0.797809  [57664/74412]\n",
      "loss: 1.202489  [64064/74412]\n",
      "loss: 0.816054  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.151224 \n",
      "\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "loss: 1.475611  [   64/74412]\n",
      "loss: 0.976542  [ 6464/74412]\n",
      "loss: 0.946435  [12864/74412]\n",
      "loss: 0.959575  [19264/74412]\n",
      "loss: 1.063548  [25664/74412]\n",
      "loss: 1.178483  [32064/74412]\n",
      "loss: 1.084873  [38464/74412]\n",
      "loss: 0.917595  [44864/74412]\n",
      "loss: 1.056683  [51264/74412]\n",
      "loss: 0.797326  [57664/74412]\n",
      "loss: 1.202072  [64064/74412]\n",
      "loss: 0.815698  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.151148 \n",
      "\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "loss: 1.475177  [   64/74412]\n",
      "loss: 0.976276  [ 6464/74412]\n",
      "loss: 0.945580  [12864/74412]\n",
      "loss: 0.958528  [19264/74412]\n",
      "loss: 1.062979  [25664/74412]\n",
      "loss: 1.177111  [32064/74412]\n",
      "loss: 1.084315  [38464/74412]\n",
      "loss: 0.916594  [44864/74412]\n",
      "loss: 1.056615  [51264/74412]\n",
      "loss: 0.796335  [57664/74412]\n",
      "loss: 1.201350  [64064/74412]\n",
      "loss: 0.815046  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.150411 \n",
      "\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "loss: 1.473857  [   64/74412]\n",
      "loss: 0.975407  [ 6464/74412]\n",
      "loss: 0.945405  [12864/74412]\n",
      "loss: 0.958612  [19264/74412]\n",
      "loss: 1.062261  [25664/74412]\n",
      "loss: 1.176210  [32064/74412]\n",
      "loss: 1.083935  [38464/74412]\n",
      "loss: 0.916021  [44864/74412]\n",
      "loss: 1.055907  [51264/74412]\n",
      "loss: 0.795023  [57664/74412]\n",
      "loss: 1.200306  [64064/74412]\n",
      "loss: 0.814275  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 1.150524 \n",
      "\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "loss: 1.473165  [   64/74412]\n",
      "loss: 0.974823  [ 6464/74412]\n",
      "loss: 0.944629  [12864/74412]\n",
      "loss: 0.958104  [19264/74412]\n",
      "loss: 1.061112  [25664/74412]\n",
      "loss: 1.175876  [32064/74412]\n",
      "loss: 1.082181  [38464/74412]\n",
      "loss: 0.914787  [44864/74412]\n",
      "loss: 1.055439  [51264/74412]\n",
      "loss: 0.794508  [57664/74412]\n",
      "loss: 1.199745  [64064/74412]\n",
      "loss: 0.814773  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.149289 \n",
      "\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "loss: 1.472313  [   64/74412]\n",
      "loss: 0.974446  [ 6464/74412]\n",
      "loss: 0.944991  [12864/74412]\n",
      "loss: 0.957193  [19264/74412]\n",
      "loss: 1.060527  [25664/74412]\n",
      "loss: 1.174703  [32064/74412]\n",
      "loss: 1.082211  [38464/74412]\n",
      "loss: 0.914117  [44864/74412]\n",
      "loss: 1.055034  [51264/74412]\n",
      "loss: 0.793268  [57664/74412]\n",
      "loss: 1.199306  [64064/74412]\n",
      "loss: 0.813364  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.148789 \n",
      "\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "loss: 1.471120  [   64/74412]\n",
      "loss: 0.973959  [ 6464/74412]\n",
      "loss: 0.944618  [12864/74412]\n",
      "loss: 0.956680  [19264/74412]\n",
      "loss: 1.059963  [25664/74412]\n",
      "loss: 1.173613  [32064/74412]\n",
      "loss: 1.081373  [38464/74412]\n",
      "loss: 0.912980  [44864/74412]\n",
      "loss: 1.054760  [51264/74412]\n",
      "loss: 0.791675  [57664/74412]\n",
      "loss: 1.197721  [64064/74412]\n",
      "loss: 0.812688  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.148678 \n",
      "\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "loss: 1.471218  [   64/74412]\n",
      "loss: 0.973413  [ 6464/74412]\n",
      "loss: 0.944767  [12864/74412]\n",
      "loss: 0.956255  [19264/74412]\n",
      "loss: 1.059415  [25664/74412]\n",
      "loss: 1.171892  [32064/74412]\n",
      "loss: 1.080577  [38464/74412]\n",
      "loss: 0.912332  [44864/74412]\n",
      "loss: 1.054060  [51264/74412]\n",
      "loss: 0.791911  [57664/74412]\n",
      "loss: 1.197179  [64064/74412]\n",
      "loss: 0.813120  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.148134 \n",
      "\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "loss: 1.470154  [   64/74412]\n",
      "loss: 0.972554  [ 6464/74412]\n",
      "loss: 0.944406  [12864/74412]\n",
      "loss: 0.956253  [19264/74412]\n",
      "loss: 1.058146  [25664/74412]\n",
      "loss: 1.171824  [32064/74412]\n",
      "loss: 1.079924  [38464/74412]\n",
      "loss: 0.911520  [44864/74412]\n",
      "loss: 1.054058  [51264/74412]\n",
      "loss: 0.790448  [57664/74412]\n",
      "loss: 1.196537  [64064/74412]\n",
      "loss: 0.813472  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.147603 \n",
      "\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "loss: 1.468922  [   64/74412]\n",
      "loss: 0.971827  [ 6464/74412]\n",
      "loss: 0.944172  [12864/74412]\n",
      "loss: 0.955484  [19264/74412]\n",
      "loss: 1.057069  [25664/74412]\n",
      "loss: 1.170697  [32064/74412]\n",
      "loss: 1.078797  [38464/74412]\n",
      "loss: 0.911515  [44864/74412]\n",
      "loss: 1.053178  [51264/74412]\n",
      "loss: 0.788824  [57664/74412]\n",
      "loss: 1.196353  [64064/74412]\n",
      "loss: 0.811753  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.147061 \n",
      "\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "loss: 1.468176  [   64/74412]\n",
      "loss: 0.970868  [ 6464/74412]\n",
      "loss: 0.943589  [12864/74412]\n",
      "loss: 0.954661  [19264/74412]\n",
      "loss: 1.056529  [25664/74412]\n",
      "loss: 1.170469  [32064/74412]\n",
      "loss: 1.078736  [38464/74412]\n",
      "loss: 0.909906  [44864/74412]\n",
      "loss: 1.052389  [51264/74412]\n",
      "loss: 0.787913  [57664/74412]\n",
      "loss: 1.196467  [64064/74412]\n",
      "loss: 0.811795  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.146754 \n",
      "\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "loss: 1.466772  [   64/74412]\n",
      "loss: 0.970541  [ 6464/74412]\n",
      "loss: 0.943247  [12864/74412]\n",
      "loss: 0.953839  [19264/74412]\n",
      "loss: 1.054150  [25664/74412]\n",
      "loss: 1.169247  [32064/74412]\n",
      "loss: 1.078050  [38464/74412]\n",
      "loss: 0.908737  [44864/74412]\n",
      "loss: 1.051442  [51264/74412]\n",
      "loss: 0.787266  [57664/74412]\n",
      "loss: 1.196601  [64064/74412]\n",
      "loss: 0.812405  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.146316 \n",
      "\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "loss: 1.466240  [   64/74412]\n",
      "loss: 0.970282  [ 6464/74412]\n",
      "loss: 0.942580  [12864/74412]\n",
      "loss: 0.953548  [19264/74412]\n",
      "loss: 1.053625  [25664/74412]\n",
      "loss: 1.168000  [32064/74412]\n",
      "loss: 1.077219  [38464/74412]\n",
      "loss: 0.907657  [44864/74412]\n",
      "loss: 1.051126  [51264/74412]\n",
      "loss: 0.786372  [57664/74412]\n",
      "loss: 1.195644  [64064/74412]\n",
      "loss: 0.812750  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.146021 \n",
      "\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "loss: 1.465960  [   64/74412]\n",
      "loss: 0.970066  [ 6464/74412]\n",
      "loss: 0.942153  [12864/74412]\n",
      "loss: 0.953129  [19264/74412]\n",
      "loss: 1.052113  [25664/74412]\n",
      "loss: 1.168016  [32064/74412]\n",
      "loss: 1.076705  [38464/74412]\n",
      "loss: 0.906894  [44864/74412]\n",
      "loss: 1.050856  [51264/74412]\n",
      "loss: 0.784585  [57664/74412]\n",
      "loss: 1.195260  [64064/74412]\n",
      "loss: 0.811720  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.145544 \n",
      "\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "loss: 1.464817  [   64/74412]\n",
      "loss: 0.969388  [ 6464/74412]\n",
      "loss: 0.941979  [12864/74412]\n",
      "loss: 0.952545  [19264/74412]\n",
      "loss: 1.051856  [25664/74412]\n",
      "loss: 1.167219  [32064/74412]\n",
      "loss: 1.076504  [38464/74412]\n",
      "loss: 0.906173  [44864/74412]\n",
      "loss: 1.050736  [51264/74412]\n",
      "loss: 0.784187  [57664/74412]\n",
      "loss: 1.194159  [64064/74412]\n",
      "loss: 0.810854  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 1.145619 \n",
      "\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "loss: 1.464764  [   64/74412]\n",
      "loss: 0.968964  [ 6464/74412]\n",
      "loss: 0.941733  [12864/74412]\n",
      "loss: 0.952245  [19264/74412]\n",
      "loss: 1.050387  [25664/74412]\n",
      "loss: 1.165901  [32064/74412]\n",
      "loss: 1.075723  [38464/74412]\n",
      "loss: 0.905775  [44864/74412]\n",
      "loss: 1.050366  [51264/74412]\n",
      "loss: 0.783192  [57664/74412]\n",
      "loss: 1.193585  [64064/74412]\n",
      "loss: 0.810358  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 1.144582 \n",
      "\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "loss: 1.463875  [   64/74412]\n",
      "loss: 0.967760  [ 6464/74412]\n",
      "loss: 0.940657  [12864/74412]\n",
      "loss: 0.950737  [19264/74412]\n",
      "loss: 1.052351  [25664/74412]\n",
      "loss: 1.165039  [32064/74412]\n",
      "loss: 1.074983  [38464/74412]\n",
      "loss: 0.904374  [44864/74412]\n",
      "loss: 1.050579  [51264/74412]\n",
      "loss: 0.782398  [57664/74412]\n",
      "loss: 1.192051  [64064/74412]\n",
      "loss: 0.810452  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 1.143398 \n",
      "\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "loss: 1.462084  [   64/74412]\n",
      "loss: 0.967453  [ 6464/74412]\n",
      "loss: 0.940415  [12864/74412]\n",
      "loss: 0.950031  [19264/74412]\n",
      "loss: 1.050521  [25664/74412]\n",
      "loss: 1.164491  [32064/74412]\n",
      "loss: 1.074031  [38464/74412]\n",
      "loss: 0.902301  [44864/74412]\n",
      "loss: 1.049262  [51264/74412]\n",
      "loss: 0.781958  [57664/74412]\n",
      "loss: 1.191514  [64064/74412]\n",
      "loss: 0.809910  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 1.143403 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "loss: 1.461489  [   64/74412]\n",
      "loss: 0.967232  [ 6464/74412]\n",
      "loss: 0.939915  [12864/74412]\n",
      "loss: 0.948874  [19264/74412]\n",
      "loss: 1.048806  [25664/74412]\n",
      "loss: 1.163391  [32064/74412]\n",
      "loss: 1.073399  [38464/74412]\n",
      "loss: 0.900820  [44864/74412]\n",
      "loss: 1.049025  [51264/74412]\n",
      "loss: 0.779774  [57664/74412]\n",
      "loss: 1.190614  [64064/74412]\n",
      "loss: 0.809272  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 1.142384 \n",
      "\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "loss: 1.460934  [   64/74412]\n",
      "loss: 0.966264  [ 6464/74412]\n",
      "loss: 0.939798  [12864/74412]\n",
      "loss: 0.948973  [19264/74412]\n",
      "loss: 1.049611  [25664/74412]\n",
      "loss: 1.162424  [32064/74412]\n",
      "loss: 1.072299  [38464/74412]\n",
      "loss: 0.900013  [44864/74412]\n",
      "loss: 1.048505  [51264/74412]\n",
      "loss: 0.779534  [57664/74412]\n",
      "loss: 1.188936  [64064/74412]\n",
      "loss: 0.808976  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 1.142393 \n",
      "\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "loss: 1.460741  [   64/74412]\n",
      "loss: 0.965910  [ 6464/74412]\n",
      "loss: 0.939279  [12864/74412]\n",
      "loss: 0.948576  [19264/74412]\n",
      "loss: 1.048486  [25664/74412]\n",
      "loss: 1.160856  [32064/74412]\n",
      "loss: 1.071361  [38464/74412]\n",
      "loss: 0.898861  [44864/74412]\n",
      "loss: 1.048231  [51264/74412]\n",
      "loss: 0.778324  [57664/74412]\n",
      "loss: 1.187526  [64064/74412]\n",
      "loss: 0.809098  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 1.141884 \n",
      "\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "loss: 1.460037  [   64/74412]\n",
      "loss: 0.964422  [ 6464/74412]\n",
      "loss: 0.938605  [12864/74412]\n",
      "loss: 0.948591  [19264/74412]\n",
      "loss: 1.046586  [25664/74412]\n",
      "loss: 1.160147  [32064/74412]\n",
      "loss: 1.071307  [38464/74412]\n",
      "loss: 0.899299  [44864/74412]\n",
      "loss: 1.047863  [51264/74412]\n",
      "loss: 0.777682  [57664/74412]\n",
      "loss: 1.187530  [64064/74412]\n",
      "loss: 0.808630  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 1.141161 \n",
      "\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "loss: 1.459046  [   64/74412]\n",
      "loss: 0.963737  [ 6464/74412]\n",
      "loss: 0.938272  [12864/74412]\n",
      "loss: 0.948144  [19264/74412]\n",
      "loss: 1.046720  [25664/74412]\n",
      "loss: 1.159520  [32064/74412]\n",
      "loss: 1.070026  [38464/74412]\n",
      "loss: 0.897590  [44864/74412]\n",
      "loss: 1.046552  [51264/74412]\n",
      "loss: 0.775566  [57664/74412]\n",
      "loss: 1.186674  [64064/74412]\n",
      "loss: 0.808296  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.140239 \n",
      "\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "loss: 1.459185  [   64/74412]\n",
      "loss: 0.963111  [ 6464/74412]\n",
      "loss: 0.938229  [12864/74412]\n",
      "loss: 0.947722  [19264/74412]\n",
      "loss: 1.045341  [25664/74412]\n",
      "loss: 1.158465  [32064/74412]\n",
      "loss: 1.069859  [38464/74412]\n",
      "loss: 0.896432  [44864/74412]\n",
      "loss: 1.046094  [51264/74412]\n",
      "loss: 0.774723  [57664/74412]\n",
      "loss: 1.187534  [64064/74412]\n",
      "loss: 0.807122  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.139586 \n",
      "\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "loss: 1.456749  [   64/74412]\n",
      "loss: 0.962191  [ 6464/74412]\n",
      "loss: 0.937195  [12864/74412]\n",
      "loss: 0.947180  [19264/74412]\n",
      "loss: 1.046354  [25664/74412]\n",
      "loss: 1.157351  [32064/74412]\n",
      "loss: 1.069137  [38464/74412]\n",
      "loss: 0.895511  [44864/74412]\n",
      "loss: 1.046114  [51264/74412]\n",
      "loss: 0.773565  [57664/74412]\n",
      "loss: 1.187965  [64064/74412]\n",
      "loss: 0.807513  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.139503 \n",
      "\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "loss: 1.456713  [   64/74412]\n",
      "loss: 0.962188  [ 6464/74412]\n",
      "loss: 0.937161  [12864/74412]\n",
      "loss: 0.946930  [19264/74412]\n",
      "loss: 1.044393  [25664/74412]\n",
      "loss: 1.156631  [32064/74412]\n",
      "loss: 1.068366  [38464/74412]\n",
      "loss: 0.894768  [44864/74412]\n",
      "loss: 1.045016  [51264/74412]\n",
      "loss: 0.773133  [57664/74412]\n",
      "loss: 1.187395  [64064/74412]\n",
      "loss: 0.806756  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.138838 \n",
      "\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "loss: 1.455657  [   64/74412]\n",
      "loss: 0.960931  [ 6464/74412]\n",
      "loss: 0.936547  [12864/74412]\n",
      "loss: 0.946770  [19264/74412]\n",
      "loss: 1.044850  [25664/74412]\n",
      "loss: 1.156717  [32064/74412]\n",
      "loss: 1.067891  [38464/74412]\n",
      "loss: 0.893882  [44864/74412]\n",
      "loss: 1.045009  [51264/74412]\n",
      "loss: 0.771673  [57664/74412]\n",
      "loss: 1.186090  [64064/74412]\n",
      "loss: 0.806500  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.138257 \n",
      "\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "loss: 1.454548  [   64/74412]\n",
      "loss: 0.959992  [ 6464/74412]\n",
      "loss: 0.936290  [12864/74412]\n",
      "loss: 0.946133  [19264/74412]\n",
      "loss: 1.043559  [25664/74412]\n",
      "loss: 1.154568  [32064/74412]\n",
      "loss: 1.067055  [38464/74412]\n",
      "loss: 0.892582  [44864/74412]\n",
      "loss: 1.043652  [51264/74412]\n",
      "loss: 0.770858  [57664/74412]\n",
      "loss: 1.186029  [64064/74412]\n",
      "loss: 0.806204  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.137654 \n",
      "\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "loss: 1.453493  [   64/74412]\n",
      "loss: 0.958932  [ 6464/74412]\n",
      "loss: 0.935596  [12864/74412]\n",
      "loss: 0.945159  [19264/74412]\n",
      "loss: 1.042654  [25664/74412]\n",
      "loss: 1.154843  [32064/74412]\n",
      "loss: 1.066160  [38464/74412]\n",
      "loss: 0.891864  [44864/74412]\n",
      "loss: 1.043589  [51264/74412]\n",
      "loss: 0.769873  [57664/74412]\n",
      "loss: 1.184523  [64064/74412]\n",
      "loss: 0.806260  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.137325 \n",
      "\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "loss: 1.452588  [   64/74412]\n",
      "loss: 0.959080  [ 6464/74412]\n",
      "loss: 0.935288  [12864/74412]\n",
      "loss: 0.944289  [19264/74412]\n",
      "loss: 1.042017  [25664/74412]\n",
      "loss: 1.152547  [32064/74412]\n",
      "loss: 1.065051  [38464/74412]\n",
      "loss: 0.891017  [44864/74412]\n",
      "loss: 1.041876  [51264/74412]\n",
      "loss: 0.769198  [57664/74412]\n",
      "loss: 1.184003  [64064/74412]\n",
      "loss: 0.806027  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.137863 \n",
      "\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "loss: 1.453590  [   64/74412]\n",
      "loss: 0.958655  [ 6464/74412]\n",
      "loss: 0.934758  [12864/74412]\n",
      "loss: 0.944037  [19264/74412]\n",
      "loss: 1.043015  [25664/74412]\n",
      "loss: 1.151644  [32064/74412]\n",
      "loss: 1.063955  [38464/74412]\n",
      "loss: 0.889748  [44864/74412]\n",
      "loss: 1.041828  [51264/74412]\n",
      "loss: 0.768429  [57664/74412]\n",
      "loss: 1.183286  [64064/74412]\n",
      "loss: 0.805661  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.137128 \n",
      "\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "loss: 1.452717  [   64/74412]\n",
      "loss: 0.958443  [ 6464/74412]\n",
      "loss: 0.933595  [12864/74412]\n",
      "loss: 0.943362  [19264/74412]\n",
      "loss: 1.042630  [25664/74412]\n",
      "loss: 1.150979  [32064/74412]\n",
      "loss: 1.063752  [38464/74412]\n",
      "loss: 0.888258  [44864/74412]\n",
      "loss: 1.041313  [51264/74412]\n",
      "loss: 0.767806  [57664/74412]\n",
      "loss: 1.181690  [64064/74412]\n",
      "loss: 0.805923  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.136750 \n",
      "\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "loss: 1.451254  [   64/74412]\n",
      "loss: 0.957432  [ 6464/74412]\n",
      "loss: 0.933087  [12864/74412]\n",
      "loss: 0.942565  [19264/74412]\n",
      "loss: 1.041957  [25664/74412]\n",
      "loss: 1.149029  [32064/74412]\n",
      "loss: 1.063590  [38464/74412]\n",
      "loss: 0.887833  [44864/74412]\n",
      "loss: 1.040009  [51264/74412]\n",
      "loss: 0.766740  [57664/74412]\n",
      "loss: 1.180993  [64064/74412]\n",
      "loss: 0.804764  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.136083 \n",
      "\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "loss: 1.450184  [   64/74412]\n",
      "loss: 0.957108  [ 6464/74412]\n",
      "loss: 0.932265  [12864/74412]\n",
      "loss: 0.942369  [19264/74412]\n",
      "loss: 1.040431  [25664/74412]\n",
      "loss: 1.148645  [32064/74412]\n",
      "loss: 1.062364  [38464/74412]\n",
      "loss: 0.887343  [44864/74412]\n",
      "loss: 1.040086  [51264/74412]\n",
      "loss: 0.765925  [57664/74412]\n",
      "loss: 1.181019  [64064/74412]\n",
      "loss: 0.804525  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 1.135641 \n",
      "\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "loss: 1.449861  [   64/74412]\n",
      "loss: 0.955750  [ 6464/74412]\n",
      "loss: 0.931952  [12864/74412]\n",
      "loss: 0.942398  [19264/74412]\n",
      "loss: 1.040503  [25664/74412]\n",
      "loss: 1.147879  [32064/74412]\n",
      "loss: 1.061573  [38464/74412]\n",
      "loss: 0.887074  [44864/74412]\n",
      "loss: 1.038854  [51264/74412]\n",
      "loss: 0.765255  [57664/74412]\n",
      "loss: 1.179349  [64064/74412]\n",
      "loss: 0.803972  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 1.135031 \n",
      "\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "loss: 1.448973  [   64/74412]\n",
      "loss: 0.955872  [ 6464/74412]\n",
      "loss: 0.931188  [12864/74412]\n",
      "loss: 0.940700  [19264/74412]\n",
      "loss: 1.040010  [25664/74412]\n",
      "loss: 1.147641  [32064/74412]\n",
      "loss: 1.060838  [38464/74412]\n",
      "loss: 0.884917  [44864/74412]\n",
      "loss: 1.038323  [51264/74412]\n",
      "loss: 0.763939  [57664/74412]\n",
      "loss: 1.179493  [64064/74412]\n",
      "loss: 0.803846  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 1.134878 \n",
      "\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "loss: 1.447892  [   64/74412]\n",
      "loss: 0.954870  [ 6464/74412]\n",
      "loss: 0.930576  [12864/74412]\n",
      "loss: 0.941298  [19264/74412]\n",
      "loss: 1.039655  [25664/74412]\n",
      "loss: 1.146613  [32064/74412]\n",
      "loss: 1.059688  [38464/74412]\n",
      "loss: 0.884660  [44864/74412]\n",
      "loss: 1.037487  [51264/74412]\n",
      "loss: 0.763268  [57664/74412]\n",
      "loss: 1.179362  [64064/74412]\n",
      "loss: 0.803408  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 1.134583 \n",
      "\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "loss: 1.447540  [   64/74412]\n",
      "loss: 0.954483  [ 6464/74412]\n",
      "loss: 0.930596  [12864/74412]\n",
      "loss: 0.940543  [19264/74412]\n",
      "loss: 1.038972  [25664/74412]\n",
      "loss: 1.146375  [32064/74412]\n",
      "loss: 1.058701  [38464/74412]\n",
      "loss: 0.883687  [44864/74412]\n",
      "loss: 1.037160  [51264/74412]\n",
      "loss: 0.762462  [57664/74412]\n",
      "loss: 1.178187  [64064/74412]\n",
      "loss: 0.803424  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 1.133900 \n",
      "\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "loss: 1.446952  [   64/74412]\n",
      "loss: 0.953969  [ 6464/74412]\n",
      "loss: 0.929740  [12864/74412]\n",
      "loss: 0.939418  [19264/74412]\n",
      "loss: 1.038993  [25664/74412]\n",
      "loss: 1.144318  [32064/74412]\n",
      "loss: 1.058173  [38464/74412]\n",
      "loss: 0.883095  [44864/74412]\n",
      "loss: 1.037074  [51264/74412]\n",
      "loss: 0.761400  [57664/74412]\n",
      "loss: 1.178649  [64064/74412]\n",
      "loss: 0.803023  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 1.133608 \n",
      "\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "loss: 1.444731  [   64/74412]\n",
      "loss: 0.953610  [ 6464/74412]\n",
      "loss: 0.928931  [12864/74412]\n",
      "loss: 0.939571  [19264/74412]\n",
      "loss: 1.038136  [25664/74412]\n",
      "loss: 1.144292  [32064/74412]\n",
      "loss: 1.057272  [38464/74412]\n",
      "loss: 0.881782  [44864/74412]\n",
      "loss: 1.036001  [51264/74412]\n",
      "loss: 0.760301  [57664/74412]\n",
      "loss: 1.177743  [64064/74412]\n",
      "loss: 0.803054  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 1.133443 \n",
      "\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "loss: 1.444794  [   64/74412]\n",
      "loss: 0.952630  [ 6464/74412]\n",
      "loss: 0.928666  [12864/74412]\n",
      "loss: 0.938784  [19264/74412]\n",
      "loss: 1.037688  [25664/74412]\n",
      "loss: 1.143284  [32064/74412]\n",
      "loss: 1.056765  [38464/74412]\n",
      "loss: 0.880867  [44864/74412]\n",
      "loss: 1.036492  [51264/74412]\n",
      "loss: 0.758877  [57664/74412]\n",
      "loss: 1.176397  [64064/74412]\n",
      "loss: 0.802640  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 1.132721 \n",
      "\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "loss: 1.444094  [   64/74412]\n",
      "loss: 0.952267  [ 6464/74412]\n",
      "loss: 0.928277  [12864/74412]\n",
      "loss: 0.938010  [19264/74412]\n",
      "loss: 1.037152  [25664/74412]\n",
      "loss: 1.142412  [32064/74412]\n",
      "loss: 1.056051  [38464/74412]\n",
      "loss: 0.880282  [44864/74412]\n",
      "loss: 1.035740  [51264/74412]\n",
      "loss: 0.757123  [57664/74412]\n",
      "loss: 1.175593  [64064/74412]\n",
      "loss: 0.802208  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 1.131894 \n",
      "\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "loss: 1.442831  [   64/74412]\n",
      "loss: 0.951585  [ 6464/74412]\n",
      "loss: 0.927281  [12864/74412]\n",
      "loss: 0.937565  [19264/74412]\n",
      "loss: 1.036732  [25664/74412]\n",
      "loss: 1.141708  [32064/74412]\n",
      "loss: 1.054782  [38464/74412]\n",
      "loss: 0.879111  [44864/74412]\n",
      "loss: 1.035841  [51264/74412]\n",
      "loss: 0.756584  [57664/74412]\n",
      "loss: 1.174610  [64064/74412]\n",
      "loss: 0.801877  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 1.132339 \n",
      "\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "loss: 1.443317  [   64/74412]\n",
      "loss: 0.950979  [ 6464/74412]\n",
      "loss: 0.926177  [12864/74412]\n",
      "loss: 0.937137  [19264/74412]\n",
      "loss: 1.035336  [25664/74412]\n",
      "loss: 1.140441  [32064/74412]\n",
      "loss: 1.054704  [38464/74412]\n",
      "loss: 0.878722  [44864/74412]\n",
      "loss: 1.034762  [51264/74412]\n",
      "loss: 0.755632  [57664/74412]\n",
      "loss: 1.174051  [64064/74412]\n",
      "loss: 0.801829  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 1.131124 \n",
      "\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "loss: 1.441653  [   64/74412]\n",
      "loss: 0.950776  [ 6464/74412]\n",
      "loss: 0.926386  [12864/74412]\n",
      "loss: 0.937002  [19264/74412]\n",
      "loss: 1.034632  [25664/74412]\n",
      "loss: 1.139718  [32064/74412]\n",
      "loss: 1.054085  [38464/74412]\n",
      "loss: 0.877495  [44864/74412]\n",
      "loss: 1.033576  [51264/74412]\n",
      "loss: 0.754316  [57664/74412]\n",
      "loss: 1.173172  [64064/74412]\n",
      "loss: 0.801600  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 1.130631 \n",
      "\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "loss: 1.440389  [   64/74412]\n",
      "loss: 0.949728  [ 6464/74412]\n",
      "loss: 0.925885  [12864/74412]\n",
      "loss: 0.936737  [19264/74412]\n",
      "loss: 1.034178  [25664/74412]\n",
      "loss: 1.138681  [32064/74412]\n",
      "loss: 1.053424  [38464/74412]\n",
      "loss: 0.876396  [44864/74412]\n",
      "loss: 1.033102  [51264/74412]\n",
      "loss: 0.752689  [57664/74412]\n",
      "loss: 1.171835  [64064/74412]\n",
      "loss: 0.801279  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 1.130199 \n",
      "\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "loss: 1.439392  [   64/74412]\n",
      "loss: 0.949446  [ 6464/74412]\n",
      "loss: 0.925184  [12864/74412]\n",
      "loss: 0.936404  [19264/74412]\n",
      "loss: 1.034118  [25664/74412]\n",
      "loss: 1.138023  [32064/74412]\n",
      "loss: 1.052711  [38464/74412]\n",
      "loss: 0.875999  [44864/74412]\n",
      "loss: 1.033092  [51264/74412]\n",
      "loss: 0.751753  [57664/74412]\n",
      "loss: 1.171159  [64064/74412]\n",
      "loss: 0.801124  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 1.129213 \n",
      "\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "loss: 1.438006  [   64/74412]\n",
      "loss: 0.949433  [ 6464/74412]\n",
      "loss: 0.924734  [12864/74412]\n",
      "loss: 0.935556  [19264/74412]\n",
      "loss: 1.033283  [25664/74412]\n",
      "loss: 1.136365  [32064/74412]\n",
      "loss: 1.052058  [38464/74412]\n",
      "loss: 0.875151  [44864/74412]\n",
      "loss: 1.031717  [51264/74412]\n",
      "loss: 0.750791  [57664/74412]\n",
      "loss: 1.170010  [64064/74412]\n",
      "loss: 0.800882  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 1.128948 \n",
      "\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "loss: 1.437697  [   64/74412]\n",
      "loss: 0.949309  [ 6464/74412]\n",
      "loss: 0.924372  [12864/74412]\n",
      "loss: 0.934628  [19264/74412]\n",
      "loss: 1.031869  [25664/74412]\n",
      "loss: 1.135814  [32064/74412]\n",
      "loss: 1.051553  [38464/74412]\n",
      "loss: 0.874989  [44864/74412]\n",
      "loss: 1.031540  [51264/74412]\n",
      "loss: 0.749891  [57664/74412]\n",
      "loss: 1.168893  [64064/74412]\n",
      "loss: 0.800123  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 1.128608 \n",
      "\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "loss: 1.436436  [   64/74412]\n",
      "loss: 0.948043  [ 6464/74412]\n",
      "loss: 0.923217  [12864/74412]\n",
      "loss: 0.934195  [19264/74412]\n",
      "loss: 1.031634  [25664/74412]\n",
      "loss: 1.135408  [32064/74412]\n",
      "loss: 1.050558  [38464/74412]\n",
      "loss: 0.874009  [44864/74412]\n",
      "loss: 1.031661  [51264/74412]\n",
      "loss: 0.749618  [57664/74412]\n",
      "loss: 1.167481  [64064/74412]\n",
      "loss: 0.799529  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 1.127994 \n",
      "\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "loss: 1.435814  [   64/74412]\n",
      "loss: 0.947656  [ 6464/74412]\n",
      "loss: 0.920903  [12864/74412]\n",
      "loss: 0.933990  [19264/74412]\n",
      "loss: 1.031649  [25664/74412]\n",
      "loss: 1.135016  [32064/74412]\n",
      "loss: 1.050927  [38464/74412]\n",
      "loss: 0.873501  [44864/74412]\n",
      "loss: 1.030902  [51264/74412]\n",
      "loss: 0.748915  [57664/74412]\n",
      "loss: 1.167518  [64064/74412]\n",
      "loss: 0.799854  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 1.126628 \n",
      "\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "loss: 1.433480  [   64/74412]\n",
      "loss: 0.947560  [ 6464/74412]\n",
      "loss: 0.920493  [12864/74412]\n",
      "loss: 0.933493  [19264/74412]\n",
      "loss: 1.030293  [25664/74412]\n",
      "loss: 1.135034  [32064/74412]\n",
      "loss: 1.049824  [38464/74412]\n",
      "loss: 0.871741  [44864/74412]\n",
      "loss: 1.030414  [51264/74412]\n",
      "loss: 0.748362  [57664/74412]\n",
      "loss: 1.167073  [64064/74412]\n",
      "loss: 0.799636  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 1.127101 \n",
      "\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "loss: 1.434435  [   64/74412]\n",
      "loss: 0.947609  [ 6464/74412]\n",
      "loss: 0.920927  [12864/74412]\n",
      "loss: 0.933431  [19264/74412]\n",
      "loss: 1.030397  [25664/74412]\n",
      "loss: 1.132836  [32064/74412]\n",
      "loss: 1.049166  [38464/74412]\n",
      "loss: 0.871973  [44864/74412]\n",
      "loss: 1.029221  [51264/74412]\n",
      "loss: 0.747216  [57664/74412]\n",
      "loss: 1.166460  [64064/74412]\n",
      "loss: 0.798950  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 1.126143 \n",
      "\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "loss: 1.433233  [   64/74412]\n",
      "loss: 0.945704  [ 6464/74412]\n",
      "loss: 0.920489  [12864/74412]\n",
      "loss: 0.932143  [19264/74412]\n",
      "loss: 1.029788  [25664/74412]\n",
      "loss: 1.131747  [32064/74412]\n",
      "loss: 1.048511  [38464/74412]\n",
      "loss: 0.871356  [44864/74412]\n",
      "loss: 1.027907  [51264/74412]\n",
      "loss: 0.746060  [57664/74412]\n",
      "loss: 1.166372  [64064/74412]\n",
      "loss: 0.798584  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 1.125835 \n",
      "\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "loss: 1.432121  [   64/74412]\n",
      "loss: 0.945887  [ 6464/74412]\n",
      "loss: 0.920930  [12864/74412]\n",
      "loss: 0.931235  [19264/74412]\n",
      "loss: 1.028750  [25664/74412]\n",
      "loss: 1.131665  [32064/74412]\n",
      "loss: 1.048207  [38464/74412]\n",
      "loss: 0.870378  [44864/74412]\n",
      "loss: 1.027587  [51264/74412]\n",
      "loss: 0.745444  [57664/74412]\n",
      "loss: 1.164558  [64064/74412]\n",
      "loss: 0.798566  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 1.125626 \n",
      "\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "loss: 1.431353  [   64/74412]\n",
      "loss: 0.945600  [ 6464/74412]\n",
      "loss: 0.920074  [12864/74412]\n",
      "loss: 0.930675  [19264/74412]\n",
      "loss: 1.028916  [25664/74412]\n",
      "loss: 1.129272  [32064/74412]\n",
      "loss: 1.047333  [38464/74412]\n",
      "loss: 0.869361  [44864/74412]\n",
      "loss: 1.027153  [51264/74412]\n",
      "loss: 0.744200  [57664/74412]\n",
      "loss: 1.163901  [64064/74412]\n",
      "loss: 0.797494  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 1.124214 \n",
      "\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "loss: 1.429831  [   64/74412]\n",
      "loss: 0.945335  [ 6464/74412]\n",
      "loss: 0.919960  [12864/74412]\n",
      "loss: 0.929718  [19264/74412]\n",
      "loss: 1.027550  [25664/74412]\n",
      "loss: 1.128550  [32064/74412]\n",
      "loss: 1.046862  [38464/74412]\n",
      "loss: 0.868606  [44864/74412]\n",
      "loss: 1.026374  [51264/74412]\n",
      "loss: 0.743654  [57664/74412]\n",
      "loss: 1.163534  [64064/74412]\n",
      "loss: 0.797458  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 1.123803 \n",
      "\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "loss: 1.429634  [   64/74412]\n",
      "loss: 0.945004  [ 6464/74412]\n",
      "loss: 0.919084  [12864/74412]\n",
      "loss: 0.929447  [19264/74412]\n",
      "loss: 1.027266  [25664/74412]\n",
      "loss: 1.127907  [32064/74412]\n",
      "loss: 1.046184  [38464/74412]\n",
      "loss: 0.868131  [44864/74412]\n",
      "loss: 1.025938  [51264/74412]\n",
      "loss: 0.742548  [57664/74412]\n",
      "loss: 1.162825  [64064/74412]\n",
      "loss: 0.797158  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 1.123401 \n",
      "\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "loss: 1.428236  [   64/74412]\n",
      "loss: 0.943419  [ 6464/74412]\n",
      "loss: 0.918347  [12864/74412]\n",
      "loss: 0.929051  [19264/74412]\n",
      "loss: 1.026700  [25664/74412]\n",
      "loss: 1.127646  [32064/74412]\n",
      "loss: 1.045439  [38464/74412]\n",
      "loss: 0.867228  [44864/74412]\n",
      "loss: 1.024885  [51264/74412]\n",
      "loss: 0.742201  [57664/74412]\n",
      "loss: 1.161555  [64064/74412]\n",
      "loss: 0.797341  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 1.123077 \n",
      "\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "loss: 1.428089  [   64/74412]\n",
      "loss: 0.944010  [ 6464/74412]\n",
      "loss: 0.918765  [12864/74412]\n",
      "loss: 0.928790  [19264/74412]\n",
      "loss: 1.026006  [25664/74412]\n",
      "loss: 1.126940  [32064/74412]\n",
      "loss: 1.043946  [38464/74412]\n",
      "loss: 0.867112  [44864/74412]\n",
      "loss: 1.024738  [51264/74412]\n",
      "loss: 0.741686  [57664/74412]\n",
      "loss: 1.162349  [64064/74412]\n",
      "loss: 0.797034  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 1.121953 \n",
      "\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "loss: 1.426049  [   64/74412]\n",
      "loss: 0.943513  [ 6464/74412]\n",
      "loss: 0.918873  [12864/74412]\n",
      "loss: 0.927743  [19264/74412]\n",
      "loss: 1.025468  [25664/74412]\n",
      "loss: 1.126110  [32064/74412]\n",
      "loss: 1.043591  [38464/74412]\n",
      "loss: 0.865642  [44864/74412]\n",
      "loss: 1.024583  [51264/74412]\n",
      "loss: 0.740504  [57664/74412]\n",
      "loss: 1.160855  [64064/74412]\n",
      "loss: 0.796564  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 1.121477 \n",
      "\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "loss: 1.425462  [   64/74412]\n",
      "loss: 0.942869  [ 6464/74412]\n",
      "loss: 0.918045  [12864/74412]\n",
      "loss: 0.928231  [19264/74412]\n",
      "loss: 1.024603  [25664/74412]\n",
      "loss: 1.125208  [32064/74412]\n",
      "loss: 1.043596  [38464/74412]\n",
      "loss: 0.865410  [44864/74412]\n",
      "loss: 1.024228  [51264/74412]\n",
      "loss: 0.739566  [57664/74412]\n",
      "loss: 1.159948  [64064/74412]\n",
      "loss: 0.796342  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 1.121283 \n",
      "\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "loss: 1.424841  [   64/74412]\n",
      "loss: 0.942128  [ 6464/74412]\n",
      "loss: 0.918231  [12864/74412]\n",
      "loss: 0.926282  [19264/74412]\n",
      "loss: 1.023808  [25664/74412]\n",
      "loss: 1.124390  [32064/74412]\n",
      "loss: 1.043189  [38464/74412]\n",
      "loss: 0.864623  [44864/74412]\n",
      "loss: 1.023417  [51264/74412]\n",
      "loss: 0.739412  [57664/74412]\n",
      "loss: 1.158605  [64064/74412]\n",
      "loss: 0.795304  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 1.121018 \n",
      "\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "loss: 1.424604  [   64/74412]\n",
      "loss: 0.941552  [ 6464/74412]\n",
      "loss: 0.917292  [12864/74412]\n",
      "loss: 0.926405  [19264/74412]\n",
      "loss: 1.022893  [25664/74412]\n",
      "loss: 1.123461  [32064/74412]\n",
      "loss: 1.041377  [38464/74412]\n",
      "loss: 0.863879  [44864/74412]\n",
      "loss: 1.023252  [51264/74412]\n",
      "loss: 0.738402  [57664/74412]\n",
      "loss: 1.158336  [64064/74412]\n",
      "loss: 0.794356  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 1.120774 \n",
      "\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "loss: 1.424157  [   64/74412]\n",
      "loss: 0.939968  [ 6464/74412]\n",
      "loss: 0.917300  [12864/74412]\n",
      "loss: 0.924999  [19264/74412]\n",
      "loss: 1.022766  [25664/74412]\n",
      "loss: 1.122957  [32064/74412]\n",
      "loss: 1.041268  [38464/74412]\n",
      "loss: 0.863089  [44864/74412]\n",
      "loss: 1.022686  [51264/74412]\n",
      "loss: 0.737654  [57664/74412]\n",
      "loss: 1.157661  [64064/74412]\n",
      "loss: 0.794247  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 1.119930 \n",
      "\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "loss: 1.423376  [   64/74412]\n",
      "loss: 0.939715  [ 6464/74412]\n",
      "loss: 0.917148  [12864/74412]\n",
      "loss: 0.924893  [19264/74412]\n",
      "loss: 1.021473  [25664/74412]\n",
      "loss: 1.122270  [32064/74412]\n",
      "loss: 1.041582  [38464/74412]\n",
      "loss: 0.862078  [44864/74412]\n",
      "loss: 1.022301  [51264/74412]\n",
      "loss: 0.736335  [57664/74412]\n",
      "loss: 1.157341  [64064/74412]\n",
      "loss: 0.794287  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 1.118885 \n",
      "\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "loss: 1.421790  [   64/74412]\n",
      "loss: 0.939174  [ 6464/74412]\n",
      "loss: 0.916010  [12864/74412]\n",
      "loss: 0.924779  [19264/74412]\n",
      "loss: 1.021560  [25664/74412]\n",
      "loss: 1.121377  [32064/74412]\n",
      "loss: 1.041116  [38464/74412]\n",
      "loss: 0.862436  [44864/74412]\n",
      "loss: 1.020534  [51264/74412]\n",
      "loss: 0.734886  [57664/74412]\n",
      "loss: 1.158293  [64064/74412]\n",
      "loss: 0.793955  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 1.118405 \n",
      "\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "loss: 1.421183  [   64/74412]\n",
      "loss: 0.938301  [ 6464/74412]\n",
      "loss: 0.916694  [12864/74412]\n",
      "loss: 0.924143  [19264/74412]\n",
      "loss: 1.021541  [25664/74412]\n",
      "loss: 1.120876  [32064/74412]\n",
      "loss: 1.041254  [38464/74412]\n",
      "loss: 0.860853  [44864/74412]\n",
      "loss: 1.020236  [51264/74412]\n",
      "loss: 0.733900  [57664/74412]\n",
      "loss: 1.157012  [64064/74412]\n",
      "loss: 0.792757  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 1.117930 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "loss: 1.420001  [   64/74412]\n",
      "loss: 0.938040  [ 6464/74412]\n",
      "loss: 0.915383  [12864/74412]\n",
      "loss: 0.922386  [19264/74412]\n",
      "loss: 1.020757  [25664/74412]\n",
      "loss: 1.121153  [32064/74412]\n",
      "loss: 1.040697  [38464/74412]\n",
      "loss: 0.860493  [44864/74412]\n",
      "loss: 1.019674  [51264/74412]\n",
      "loss: 0.733492  [57664/74412]\n",
      "loss: 1.156264  [64064/74412]\n",
      "loss: 0.792699  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 1.117026 \n",
      "\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "loss: 1.417253  [   64/74412]\n",
      "loss: 0.938237  [ 6464/74412]\n",
      "loss: 0.915470  [12864/74412]\n",
      "loss: 0.921982  [19264/74412]\n",
      "loss: 1.019984  [25664/74412]\n",
      "loss: 1.120090  [32064/74412]\n",
      "loss: 1.040921  [38464/74412]\n",
      "loss: 0.859319  [44864/74412]\n",
      "loss: 1.019061  [51264/74412]\n",
      "loss: 0.731951  [57664/74412]\n",
      "loss: 1.155543  [64064/74412]\n",
      "loss: 0.792564  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 1.116260 \n",
      "\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "loss: 1.416293  [   64/74412]\n",
      "loss: 0.937515  [ 6464/74412]\n",
      "loss: 0.915277  [12864/74412]\n",
      "loss: 0.921246  [19264/74412]\n",
      "loss: 1.018796  [25664/74412]\n",
      "loss: 1.118943  [32064/74412]\n",
      "loss: 1.040185  [38464/74412]\n",
      "loss: 0.858159  [44864/74412]\n",
      "loss: 1.017592  [51264/74412]\n",
      "loss: 0.731384  [57664/74412]\n",
      "loss: 1.154449  [64064/74412]\n",
      "loss: 0.792637  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 1.115251 \n",
      "\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "loss: 1.413762  [   64/74412]\n",
      "loss: 0.936831  [ 6464/74412]\n",
      "loss: 0.915054  [12864/74412]\n",
      "loss: 0.920690  [19264/74412]\n",
      "loss: 1.019033  [25664/74412]\n",
      "loss: 1.118487  [32064/74412]\n",
      "loss: 1.039852  [38464/74412]\n",
      "loss: 0.857898  [44864/74412]\n",
      "loss: 1.017784  [51264/74412]\n",
      "loss: 0.729921  [57664/74412]\n",
      "loss: 1.153836  [64064/74412]\n",
      "loss: 0.792387  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 1.114822 \n",
      "\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "loss: 1.412868  [   64/74412]\n",
      "loss: 0.937950  [ 6464/74412]\n",
      "loss: 0.914952  [12864/74412]\n",
      "loss: 0.920700  [19264/74412]\n",
      "loss: 1.018010  [25664/74412]\n",
      "loss: 1.117436  [32064/74412]\n",
      "loss: 1.038973  [38464/74412]\n",
      "loss: 0.857375  [44864/74412]\n",
      "loss: 1.016646  [51264/74412]\n",
      "loss: 0.729409  [57664/74412]\n",
      "loss: 1.153474  [64064/74412]\n",
      "loss: 0.792390  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 1.114353 \n",
      "\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "loss: 1.412044  [   64/74412]\n",
      "loss: 0.937355  [ 6464/74412]\n",
      "loss: 0.914818  [12864/74412]\n",
      "loss: 0.919946  [19264/74412]\n",
      "loss: 1.017710  [25664/74412]\n",
      "loss: 1.116515  [32064/74412]\n",
      "loss: 1.038229  [38464/74412]\n",
      "loss: 0.857520  [44864/74412]\n",
      "loss: 1.016211  [51264/74412]\n",
      "loss: 0.728421  [57664/74412]\n",
      "loss: 1.153021  [64064/74412]\n",
      "loss: 0.792308  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 1.113479 \n",
      "\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "loss: 1.410902  [   64/74412]\n",
      "loss: 0.936488  [ 6464/74412]\n",
      "loss: 0.913463  [12864/74412]\n",
      "loss: 0.918370  [19264/74412]\n",
      "loss: 1.016517  [25664/74412]\n",
      "loss: 1.115575  [32064/74412]\n",
      "loss: 1.037399  [38464/74412]\n",
      "loss: 0.856668  [44864/74412]\n",
      "loss: 1.015897  [51264/74412]\n",
      "loss: 0.728213  [57664/74412]\n",
      "loss: 1.152992  [64064/74412]\n",
      "loss: 0.791558  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 1.112396 \n",
      "\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "loss: 1.410067  [   64/74412]\n",
      "loss: 0.935403  [ 6464/74412]\n",
      "loss: 0.913175  [12864/74412]\n",
      "loss: 0.917850  [19264/74412]\n",
      "loss: 1.015901  [25664/74412]\n",
      "loss: 1.114078  [32064/74412]\n",
      "loss: 1.037199  [38464/74412]\n",
      "loss: 0.857124  [44864/74412]\n",
      "loss: 1.017216  [51264/74412]\n",
      "loss: 0.727416  [57664/74412]\n",
      "loss: 1.151964  [64064/74412]\n",
      "loss: 0.791194  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 1.111621 \n",
      "\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "loss: 1.409359  [   64/74412]\n",
      "loss: 0.935338  [ 6464/74412]\n",
      "loss: 0.911808  [12864/74412]\n",
      "loss: 0.917955  [19264/74412]\n",
      "loss: 1.015375  [25664/74412]\n",
      "loss: 1.113359  [32064/74412]\n",
      "loss: 1.036401  [38464/74412]\n",
      "loss: 0.855035  [44864/74412]\n",
      "loss: 1.016388  [51264/74412]\n",
      "loss: 0.726267  [57664/74412]\n",
      "loss: 1.151364  [64064/74412]\n",
      "loss: 0.790450  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 1.111106 \n",
      "\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "loss: 1.408302  [   64/74412]\n",
      "loss: 0.934448  [ 6464/74412]\n",
      "loss: 0.910996  [12864/74412]\n",
      "loss: 0.917742  [19264/74412]\n",
      "loss: 1.014782  [25664/74412]\n",
      "loss: 1.112663  [32064/74412]\n",
      "loss: 1.036600  [38464/74412]\n",
      "loss: 0.854308  [44864/74412]\n",
      "loss: 1.015330  [51264/74412]\n",
      "loss: 0.724807  [57664/74412]\n",
      "loss: 1.150391  [64064/74412]\n",
      "loss: 0.790243  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 1.111545 \n",
      "\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "loss: 1.408818  [   64/74412]\n",
      "loss: 0.933776  [ 6464/74412]\n",
      "loss: 0.911097  [12864/74412]\n",
      "loss: 0.916856  [19264/74412]\n",
      "loss: 1.014948  [25664/74412]\n",
      "loss: 1.111257  [32064/74412]\n",
      "loss: 1.035857  [38464/74412]\n",
      "loss: 0.855063  [44864/74412]\n",
      "loss: 1.014607  [51264/74412]\n",
      "loss: 0.724336  [57664/74412]\n",
      "loss: 1.150587  [64064/74412]\n",
      "loss: 0.790631  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 1.110137 \n",
      "\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "loss: 1.406540  [   64/74412]\n",
      "loss: 0.933066  [ 6464/74412]\n",
      "loss: 0.909826  [12864/74412]\n",
      "loss: 0.915854  [19264/74412]\n",
      "loss: 1.013636  [25664/74412]\n",
      "loss: 1.109784  [32064/74412]\n",
      "loss: 1.035793  [38464/74412]\n",
      "loss: 0.852805  [44864/74412]\n",
      "loss: 1.015189  [51264/74412]\n",
      "loss: 0.722833  [57664/74412]\n",
      "loss: 1.149821  [64064/74412]\n",
      "loss: 0.789851  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.109447 \n",
      "\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "loss: 1.405764  [   64/74412]\n",
      "loss: 0.933093  [ 6464/74412]\n",
      "loss: 0.909080  [12864/74412]\n",
      "loss: 0.914476  [19264/74412]\n",
      "loss: 1.013664  [25664/74412]\n",
      "loss: 1.109137  [32064/74412]\n",
      "loss: 1.035582  [38464/74412]\n",
      "loss: 0.851290  [44864/74412]\n",
      "loss: 1.014173  [51264/74412]\n",
      "loss: 0.722229  [57664/74412]\n",
      "loss: 1.149502  [64064/74412]\n",
      "loss: 0.789432  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.109334 \n",
      "\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "loss: 1.405370  [   64/74412]\n",
      "loss: 0.932599  [ 6464/74412]\n",
      "loss: 0.907717  [12864/74412]\n",
      "loss: 0.915328  [19264/74412]\n",
      "loss: 1.012270  [25664/74412]\n",
      "loss: 1.108197  [32064/74412]\n",
      "loss: 1.034674  [38464/74412]\n",
      "loss: 0.850823  [44864/74412]\n",
      "loss: 1.015348  [51264/74412]\n",
      "loss: 0.721798  [57664/74412]\n",
      "loss: 1.147752  [64064/74412]\n",
      "loss: 0.789258  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.108803 \n",
      "\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "loss: 1.405232  [   64/74412]\n",
      "loss: 0.931980  [ 6464/74412]\n",
      "loss: 0.907201  [12864/74412]\n",
      "loss: 0.914227  [19264/74412]\n",
      "loss: 1.012218  [25664/74412]\n",
      "loss: 1.107945  [32064/74412]\n",
      "loss: 1.034121  [38464/74412]\n",
      "loss: 0.850747  [44864/74412]\n",
      "loss: 1.014045  [51264/74412]\n",
      "loss: 0.720734  [57664/74412]\n",
      "loss: 1.146955  [64064/74412]\n",
      "loss: 0.789735  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.108516 \n",
      "\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "loss: 1.405705  [   64/74412]\n",
      "loss: 0.931665  [ 6464/74412]\n",
      "loss: 0.907342  [12864/74412]\n",
      "loss: 0.914439  [19264/74412]\n",
      "loss: 1.011178  [25664/74412]\n",
      "loss: 1.106384  [32064/74412]\n",
      "loss: 1.034506  [38464/74412]\n",
      "loss: 0.850219  [44864/74412]\n",
      "loss: 1.013389  [51264/74412]\n",
      "loss: 0.719648  [57664/74412]\n",
      "loss: 1.146071  [64064/74412]\n",
      "loss: 0.788962  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.108060 \n",
      "\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "loss: 1.404160  [   64/74412]\n",
      "loss: 0.931261  [ 6464/74412]\n",
      "loss: 0.907129  [12864/74412]\n",
      "loss: 0.912836  [19264/74412]\n",
      "loss: 1.012023  [25664/74412]\n",
      "loss: 1.106383  [32064/74412]\n",
      "loss: 1.033378  [38464/74412]\n",
      "loss: 0.848598  [44864/74412]\n",
      "loss: 1.012382  [51264/74412]\n",
      "loss: 0.718916  [57664/74412]\n",
      "loss: 1.146337  [64064/74412]\n",
      "loss: 0.788431  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.107540 \n",
      "\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "loss: 1.403309  [   64/74412]\n",
      "loss: 0.930249  [ 6464/74412]\n",
      "loss: 0.906115  [12864/74412]\n",
      "loss: 0.912422  [19264/74412]\n",
      "loss: 1.010922  [25664/74412]\n",
      "loss: 1.104888  [32064/74412]\n",
      "loss: 1.032665  [38464/74412]\n",
      "loss: 0.847617  [44864/74412]\n",
      "loss: 1.012096  [51264/74412]\n",
      "loss: 0.718300  [57664/74412]\n",
      "loss: 1.144878  [64064/74412]\n",
      "loss: 0.788189  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.107231 \n",
      "\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "loss: 1.403145  [   64/74412]\n",
      "loss: 0.929303  [ 6464/74412]\n",
      "loss: 0.905596  [12864/74412]\n",
      "loss: 0.912353  [19264/74412]\n",
      "loss: 1.010338  [25664/74412]\n",
      "loss: 1.103914  [32064/74412]\n",
      "loss: 1.032140  [38464/74412]\n",
      "loss: 0.846963  [44864/74412]\n",
      "loss: 1.010846  [51264/74412]\n",
      "loss: 0.717459  [57664/74412]\n",
      "loss: 1.144405  [64064/74412]\n",
      "loss: 0.788360  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.107351 \n",
      "\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "loss: 1.402823  [   64/74412]\n",
      "loss: 0.929039  [ 6464/74412]\n",
      "loss: 0.906249  [12864/74412]\n",
      "loss: 0.911102  [19264/74412]\n",
      "loss: 1.010652  [25664/74412]\n",
      "loss: 1.103580  [32064/74412]\n",
      "loss: 1.031954  [38464/74412]\n",
      "loss: 0.846989  [44864/74412]\n",
      "loss: 1.010086  [51264/74412]\n",
      "loss: 0.716095  [57664/74412]\n",
      "loss: 1.144361  [64064/74412]\n",
      "loss: 0.787762  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.106575 \n",
      "\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "loss: 1.401939  [   64/74412]\n",
      "loss: 0.928177  [ 6464/74412]\n",
      "loss: 0.905286  [12864/74412]\n",
      "loss: 0.910821  [19264/74412]\n",
      "loss: 1.010025  [25664/74412]\n",
      "loss: 1.102609  [32064/74412]\n",
      "loss: 1.030661  [38464/74412]\n",
      "loss: 0.846590  [44864/74412]\n",
      "loss: 1.009789  [51264/74412]\n",
      "loss: 0.715163  [57664/74412]\n",
      "loss: 1.143319  [64064/74412]\n",
      "loss: 0.787279  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.106353 \n",
      "\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "loss: 1.402154  [   64/74412]\n",
      "loss: 0.927771  [ 6464/74412]\n",
      "loss: 0.904330  [12864/74412]\n",
      "loss: 0.909135  [19264/74412]\n",
      "loss: 1.008242  [25664/74412]\n",
      "loss: 1.102261  [32064/74412]\n",
      "loss: 1.030308  [38464/74412]\n",
      "loss: 0.844906  [44864/74412]\n",
      "loss: 1.009182  [51264/74412]\n",
      "loss: 0.714110  [57664/74412]\n",
      "loss: 1.142613  [64064/74412]\n",
      "loss: 0.786713  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 1.105860 \n",
      "\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "loss: 1.400263  [   64/74412]\n",
      "loss: 0.927469  [ 6464/74412]\n",
      "loss: 0.904337  [12864/74412]\n",
      "loss: 0.908546  [19264/74412]\n",
      "loss: 1.008004  [25664/74412]\n",
      "loss: 1.101892  [32064/74412]\n",
      "loss: 1.029785  [38464/74412]\n",
      "loss: 0.844711  [44864/74412]\n",
      "loss: 1.008237  [51264/74412]\n",
      "loss: 0.713636  [57664/74412]\n",
      "loss: 1.142633  [64064/74412]\n",
      "loss: 0.786815  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.105783 \n",
      "\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "loss: 1.400733  [   64/74412]\n",
      "loss: 0.926478  [ 6464/74412]\n",
      "loss: 0.904164  [12864/74412]\n",
      "loss: 0.906953  [19264/74412]\n",
      "loss: 1.007498  [25664/74412]\n",
      "loss: 1.101363  [32064/74412]\n",
      "loss: 1.028473  [38464/74412]\n",
      "loss: 0.843693  [44864/74412]\n",
      "loss: 1.008643  [51264/74412]\n",
      "loss: 0.712799  [57664/74412]\n",
      "loss: 1.141189  [64064/74412]\n",
      "loss: 0.786522  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 1.105497 \n",
      "\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "loss: 1.400202  [   64/74412]\n",
      "loss: 0.926544  [ 6464/74412]\n",
      "loss: 0.903585  [12864/74412]\n",
      "loss: 0.906142  [19264/74412]\n",
      "loss: 1.007551  [25664/74412]\n",
      "loss: 1.100178  [32064/74412]\n",
      "loss: 1.028316  [38464/74412]\n",
      "loss: 0.842048  [44864/74412]\n",
      "loss: 1.007135  [51264/74412]\n",
      "loss: 0.712023  [57664/74412]\n",
      "loss: 1.140166  [64064/74412]\n",
      "loss: 0.786265  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 1.104690 \n",
      "\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "loss: 1.399381  [   64/74412]\n",
      "loss: 0.925678  [ 6464/74412]\n",
      "loss: 0.903466  [12864/74412]\n",
      "loss: 0.906114  [19264/74412]\n",
      "loss: 1.006792  [25664/74412]\n",
      "loss: 1.099523  [32064/74412]\n",
      "loss: 1.028262  [38464/74412]\n",
      "loss: 0.841351  [44864/74412]\n",
      "loss: 1.006294  [51264/74412]\n",
      "loss: 0.711115  [57664/74412]\n",
      "loss: 1.139337  [64064/74412]\n",
      "loss: 0.785972  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 1.104304 \n",
      "\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "loss: 1.398565  [   64/74412]\n",
      "loss: 0.925116  [ 6464/74412]\n",
      "loss: 0.902778  [12864/74412]\n",
      "loss: 0.905725  [19264/74412]\n",
      "loss: 1.006156  [25664/74412]\n",
      "loss: 1.099140  [32064/74412]\n",
      "loss: 1.027491  [38464/74412]\n",
      "loss: 0.841444  [44864/74412]\n",
      "loss: 1.005901  [51264/74412]\n",
      "loss: 0.710249  [57664/74412]\n",
      "loss: 1.138236  [64064/74412]\n",
      "loss: 0.785844  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 1.103800 \n",
      "\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "loss: 1.397339  [   64/74412]\n",
      "loss: 0.924690  [ 6464/74412]\n",
      "loss: 0.901755  [12864/74412]\n",
      "loss: 0.905930  [19264/74412]\n",
      "loss: 1.004641  [25664/74412]\n",
      "loss: 1.098429  [32064/74412]\n",
      "loss: 1.027198  [38464/74412]\n",
      "loss: 0.842040  [44864/74412]\n",
      "loss: 1.005373  [51264/74412]\n",
      "loss: 0.709786  [57664/74412]\n",
      "loss: 1.137951  [64064/74412]\n",
      "loss: 0.785715  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 1.103201 \n",
      "\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "loss: 1.397882  [   64/74412]\n",
      "loss: 0.924047  [ 6464/74412]\n",
      "loss: 0.901201  [12864/74412]\n",
      "loss: 0.904642  [19264/74412]\n",
      "loss: 1.005921  [25664/74412]\n",
      "loss: 1.097407  [32064/74412]\n",
      "loss: 1.026714  [38464/74412]\n",
      "loss: 0.838142  [44864/74412]\n",
      "loss: 1.005537  [51264/74412]\n",
      "loss: 0.708922  [57664/74412]\n",
      "loss: 1.136709  [64064/74412]\n",
      "loss: 0.784794  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 1.102795 \n",
      "\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "loss: 1.396837  [   64/74412]\n",
      "loss: 0.923503  [ 6464/74412]\n",
      "loss: 0.900956  [12864/74412]\n",
      "loss: 0.904106  [19264/74412]\n",
      "loss: 1.004204  [25664/74412]\n",
      "loss: 1.097001  [32064/74412]\n",
      "loss: 1.026166  [38464/74412]\n",
      "loss: 0.838657  [44864/74412]\n",
      "loss: 1.005085  [51264/74412]\n",
      "loss: 0.708356  [57664/74412]\n",
      "loss: 1.136644  [64064/74412]\n",
      "loss: 0.784935  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 1.102629 \n",
      "\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "loss: 1.396411  [   64/74412]\n",
      "loss: 0.923753  [ 6464/74412]\n",
      "loss: 0.901094  [12864/74412]\n",
      "loss: 0.903705  [19264/74412]\n",
      "loss: 1.004231  [25664/74412]\n",
      "loss: 1.095719  [32064/74412]\n",
      "loss: 1.025676  [38464/74412]\n",
      "loss: 0.837186  [44864/74412]\n",
      "loss: 1.004323  [51264/74412]\n",
      "loss: 0.707355  [57664/74412]\n",
      "loss: 1.134649  [64064/74412]\n",
      "loss: 0.784766  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 1.102698 \n",
      "\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "loss: 1.395356  [   64/74412]\n",
      "loss: 0.923172  [ 6464/74412]\n",
      "loss: 0.900525  [12864/74412]\n",
      "loss: 0.903093  [19264/74412]\n",
      "loss: 1.003813  [25664/74412]\n",
      "loss: 1.094591  [32064/74412]\n",
      "loss: 1.024613  [38464/74412]\n",
      "loss: 0.837045  [44864/74412]\n",
      "loss: 1.003854  [51264/74412]\n",
      "loss: 0.706153  [57664/74412]\n",
      "loss: 1.135329  [64064/74412]\n",
      "loss: 0.784325  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 1.101180 \n",
      "\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "loss: 1.394162  [   64/74412]\n",
      "loss: 0.923105  [ 6464/74412]\n",
      "loss: 0.899874  [12864/74412]\n",
      "loss: 0.902633  [19264/74412]\n",
      "loss: 1.003090  [25664/74412]\n",
      "loss: 1.094127  [32064/74412]\n",
      "loss: 1.024576  [38464/74412]\n",
      "loss: 0.835726  [44864/74412]\n",
      "loss: 1.003673  [51264/74412]\n",
      "loss: 0.705077  [57664/74412]\n",
      "loss: 1.133151  [64064/74412]\n",
      "loss: 0.784306  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 1.101609 \n",
      "\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "loss: 1.393519  [   64/74412]\n",
      "loss: 0.922690  [ 6464/74412]\n",
      "loss: 0.899389  [12864/74412]\n",
      "loss: 0.901572  [19264/74412]\n",
      "loss: 1.002518  [25664/74412]\n",
      "loss: 1.092610  [32064/74412]\n",
      "loss: 1.024802  [38464/74412]\n",
      "loss: 0.835714  [44864/74412]\n",
      "loss: 1.003071  [51264/74412]\n",
      "loss: 0.704591  [57664/74412]\n",
      "loss: 1.131565  [64064/74412]\n",
      "loss: 0.783538  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 1.100712 \n",
      "\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "loss: 1.392227  [   64/74412]\n",
      "loss: 0.922607  [ 6464/74412]\n",
      "loss: 0.898799  [12864/74412]\n",
      "loss: 0.901377  [19264/74412]\n",
      "loss: 1.002191  [25664/74412]\n",
      "loss: 1.091533  [32064/74412]\n",
      "loss: 1.023858  [38464/74412]\n",
      "loss: 0.834570  [44864/74412]\n",
      "loss: 1.002465  [51264/74412]\n",
      "loss: 0.704027  [57664/74412]\n",
      "loss: 1.131505  [64064/74412]\n",
      "loss: 0.783057  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 1.100403 \n",
      "\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "loss: 1.392322  [   64/74412]\n",
      "loss: 0.921150  [ 6464/74412]\n",
      "loss: 0.898257  [12864/74412]\n",
      "loss: 0.901223  [19264/74412]\n",
      "loss: 1.001627  [25664/74412]\n",
      "loss: 1.090705  [32064/74412]\n",
      "loss: 1.022973  [38464/74412]\n",
      "loss: 0.833572  [44864/74412]\n",
      "loss: 1.001951  [51264/74412]\n",
      "loss: 0.702676  [57664/74412]\n",
      "loss: 1.130524  [64064/74412]\n",
      "loss: 0.782966  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 1.099872 \n",
      "\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "loss: 1.391217  [   64/74412]\n",
      "loss: 0.921570  [ 6464/74412]\n",
      "loss: 0.897349  [12864/74412]\n",
      "loss: 0.900134  [19264/74412]\n",
      "loss: 1.000520  [25664/74412]\n",
      "loss: 1.089992  [32064/74412]\n",
      "loss: 1.022219  [38464/74412]\n",
      "loss: 0.832763  [44864/74412]\n",
      "loss: 1.000480  [51264/74412]\n",
      "loss: 0.702132  [57664/74412]\n",
      "loss: 1.130044  [64064/74412]\n",
      "loss: 0.783202  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 1.098936 \n",
      "\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "loss: 1.389521  [   64/74412]\n",
      "loss: 0.920352  [ 6464/74412]\n",
      "loss: 0.896503  [12864/74412]\n",
      "loss: 0.899925  [19264/74412]\n",
      "loss: 0.999517  [25664/74412]\n",
      "loss: 1.090210  [32064/74412]\n",
      "loss: 1.022380  [38464/74412]\n",
      "loss: 0.831613  [44864/74412]\n",
      "loss: 1.000716  [51264/74412]\n",
      "loss: 0.701780  [57664/74412]\n",
      "loss: 1.129742  [64064/74412]\n",
      "loss: 0.782033  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 1.098445 \n",
      "\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "loss: 1.388630  [   64/74412]\n",
      "loss: 0.919423  [ 6464/74412]\n",
      "loss: 0.897258  [12864/74412]\n",
      "loss: 0.897634  [19264/74412]\n",
      "loss: 0.998607  [25664/74412]\n",
      "loss: 1.088989  [32064/74412]\n",
      "loss: 1.021079  [38464/74412]\n",
      "loss: 0.831170  [44864/74412]\n",
      "loss: 0.999970  [51264/74412]\n",
      "loss: 0.700585  [57664/74412]\n",
      "loss: 1.129130  [64064/74412]\n",
      "loss: 0.782670  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 1.097348 \n",
      "\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "loss: 1.387525  [   64/74412]\n",
      "loss: 0.918371  [ 6464/74412]\n",
      "loss: 0.897184  [12864/74412]\n",
      "loss: 0.897575  [19264/74412]\n",
      "loss: 1.000457  [25664/74412]\n",
      "loss: 1.088369  [32064/74412]\n",
      "loss: 1.020436  [38464/74412]\n",
      "loss: 0.830027  [44864/74412]\n",
      "loss: 0.998813  [51264/74412]\n",
      "loss: 0.700265  [57664/74412]\n",
      "loss: 1.128498  [64064/74412]\n",
      "loss: 0.782423  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 1.097113 \n",
      "\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "loss: 1.387889  [   64/74412]\n",
      "loss: 0.919402  [ 6464/74412]\n",
      "loss: 0.897040  [12864/74412]\n",
      "loss: 0.897771  [19264/74412]\n",
      "loss: 0.999131  [25664/74412]\n",
      "loss: 1.088306  [32064/74412]\n",
      "loss: 1.019707  [38464/74412]\n",
      "loss: 0.830204  [44864/74412]\n",
      "loss: 0.998582  [51264/74412]\n",
      "loss: 0.699318  [57664/74412]\n",
      "loss: 1.128277  [64064/74412]\n",
      "loss: 0.781944  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 1.096228 \n",
      "\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "loss: 1.385819  [   64/74412]\n",
      "loss: 0.918350  [ 6464/74412]\n",
      "loss: 0.894744  [12864/74412]\n",
      "loss: 0.896608  [19264/74412]\n",
      "loss: 0.996459  [25664/74412]\n",
      "loss: 1.086872  [32064/74412]\n",
      "loss: 1.018369  [38464/74412]\n",
      "loss: 0.828619  [44864/74412]\n",
      "loss: 0.998169  [51264/74412]\n",
      "loss: 0.698597  [57664/74412]\n",
      "loss: 1.128030  [64064/74412]\n",
      "loss: 0.781506  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 1.095225 \n",
      "\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "loss: 1.385189  [   64/74412]\n",
      "loss: 0.917444  [ 6464/74412]\n",
      "loss: 0.895562  [12864/74412]\n",
      "loss: 0.896901  [19264/74412]\n",
      "loss: 0.999064  [25664/74412]\n",
      "loss: 1.086651  [32064/74412]\n",
      "loss: 1.018955  [38464/74412]\n",
      "loss: 0.828546  [44864/74412]\n",
      "loss: 0.996744  [51264/74412]\n",
      "loss: 0.697903  [57664/74412]\n",
      "loss: 1.127004  [64064/74412]\n",
      "loss: 0.781405  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 1.095010 \n",
      "\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "loss: 1.383936  [   64/74412]\n",
      "loss: 0.917481  [ 6464/74412]\n",
      "loss: 0.893276  [12864/74412]\n",
      "loss: 0.896683  [19264/74412]\n",
      "loss: 0.999548  [25664/74412]\n",
      "loss: 1.085101  [32064/74412]\n",
      "loss: 1.017793  [38464/74412]\n",
      "loss: 0.827219  [44864/74412]\n",
      "loss: 0.995960  [51264/74412]\n",
      "loss: 0.696252  [57664/74412]\n",
      "loss: 1.126118  [64064/74412]\n",
      "loss: 0.780677  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 1.094979 \n",
      "\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "loss: 1.384537  [   64/74412]\n",
      "loss: 0.917007  [ 6464/74412]\n",
      "loss: 0.893380  [12864/74412]\n",
      "loss: 0.895778  [19264/74412]\n",
      "loss: 0.998584  [25664/74412]\n",
      "loss: 1.085006  [32064/74412]\n",
      "loss: 1.017987  [38464/74412]\n",
      "loss: 0.826140  [44864/74412]\n",
      "loss: 0.994973  [51264/74412]\n",
      "loss: 0.696309  [57664/74412]\n",
      "loss: 1.125569  [64064/74412]\n",
      "loss: 0.780623  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 1.094454 \n",
      "\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "loss: 1.382928  [   64/74412]\n",
      "loss: 0.915589  [ 6464/74412]\n",
      "loss: 0.894463  [12864/74412]\n",
      "loss: 0.895159  [19264/74412]\n",
      "loss: 0.998059  [25664/74412]\n",
      "loss: 1.081170  [32064/74412]\n",
      "loss: 1.016651  [38464/74412]\n",
      "loss: 0.825535  [44864/74412]\n",
      "loss: 0.993903  [51264/74412]\n",
      "loss: 0.695029  [57664/74412]\n",
      "loss: 1.124943  [64064/74412]\n",
      "loss: 0.779618  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 1.093831 \n",
      "\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "loss: 1.381997  [   64/74412]\n",
      "loss: 0.914641  [ 6464/74412]\n",
      "loss: 0.893330  [12864/74412]\n",
      "loss: 0.895378  [19264/74412]\n",
      "loss: 0.997049  [25664/74412]\n",
      "loss: 1.080774  [32064/74412]\n",
      "loss: 1.016168  [38464/74412]\n",
      "loss: 0.825200  [44864/74412]\n",
      "loss: 0.994875  [51264/74412]\n",
      "loss: 0.694697  [57664/74412]\n",
      "loss: 1.124441  [64064/74412]\n",
      "loss: 0.779952  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 1.093408 \n",
      "\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "loss: 1.381202  [   64/74412]\n",
      "loss: 0.914200  [ 6464/74412]\n",
      "loss: 0.893152  [12864/74412]\n",
      "loss: 0.895508  [19264/74412]\n",
      "loss: 0.995548  [25664/74412]\n",
      "loss: 1.079344  [32064/74412]\n",
      "loss: 1.016367  [38464/74412]\n",
      "loss: 0.825735  [44864/74412]\n",
      "loss: 0.993565  [51264/74412]\n",
      "loss: 0.694102  [57664/74412]\n",
      "loss: 1.123387  [64064/74412]\n",
      "loss: 0.779255  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 1.092891 \n",
      "\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "loss: 1.380573  [   64/74412]\n",
      "loss: 0.913991  [ 6464/74412]\n",
      "loss: 0.893002  [12864/74412]\n",
      "loss: 0.895140  [19264/74412]\n",
      "loss: 0.995019  [25664/74412]\n",
      "loss: 1.078827  [32064/74412]\n",
      "loss: 1.015191  [38464/74412]\n",
      "loss: 0.824867  [44864/74412]\n",
      "loss: 0.993477  [51264/74412]\n",
      "loss: 0.692453  [57664/74412]\n",
      "loss: 1.122288  [64064/74412]\n",
      "loss: 0.778955  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 1.092573 \n",
      "\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "loss: 1.379764  [   64/74412]\n",
      "loss: 0.914071  [ 6464/74412]\n",
      "loss: 0.891436  [12864/74412]\n",
      "loss: 0.893582  [19264/74412]\n",
      "loss: 0.994908  [25664/74412]\n",
      "loss: 1.077990  [32064/74412]\n",
      "loss: 1.014944  [38464/74412]\n",
      "loss: 0.824395  [44864/74412]\n",
      "loss: 0.992633  [51264/74412]\n",
      "loss: 0.691950  [57664/74412]\n",
      "loss: 1.121438  [64064/74412]\n",
      "loss: 0.779154  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 1.092341 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "loss: 1.379718  [   64/74412]\n",
      "loss: 0.913443  [ 6464/74412]\n",
      "loss: 0.890988  [12864/74412]\n",
      "loss: 0.894422  [19264/74412]\n",
      "loss: 0.993868  [25664/74412]\n",
      "loss: 1.077514  [32064/74412]\n",
      "loss: 1.014296  [38464/74412]\n",
      "loss: 0.823373  [44864/74412]\n",
      "loss: 0.991817  [51264/74412]\n",
      "loss: 0.691209  [57664/74412]\n",
      "loss: 1.121241  [64064/74412]\n",
      "loss: 0.778286  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 1.091762 \n",
      "\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "loss: 1.379763  [   64/74412]\n",
      "loss: 0.913041  [ 6464/74412]\n",
      "loss: 0.891343  [12864/74412]\n",
      "loss: 0.893000  [19264/74412]\n",
      "loss: 0.993291  [25664/74412]\n",
      "loss: 1.077006  [32064/74412]\n",
      "loss: 1.014177  [38464/74412]\n",
      "loss: 0.822327  [44864/74412]\n",
      "loss: 0.992348  [51264/74412]\n",
      "loss: 0.690461  [57664/74412]\n",
      "loss: 1.120625  [64064/74412]\n",
      "loss: 0.778447  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 1.091260 \n",
      "\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "loss: 1.378641  [   64/74412]\n",
      "loss: 0.913171  [ 6464/74412]\n",
      "loss: 0.892153  [12864/74412]\n",
      "loss: 0.892395  [19264/74412]\n",
      "loss: 0.992776  [25664/74412]\n",
      "loss: 1.077086  [32064/74412]\n",
      "loss: 1.014216  [38464/74412]\n",
      "loss: 0.821948  [44864/74412]\n",
      "loss: 0.990332  [51264/74412]\n",
      "loss: 0.689970  [57664/74412]\n",
      "loss: 1.119260  [64064/74412]\n",
      "loss: 0.778407  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 1.090862 \n",
      "\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "loss: 1.378923  [   64/74412]\n",
      "loss: 0.911190  [ 6464/74412]\n",
      "loss: 0.889783  [12864/74412]\n",
      "loss: 0.891781  [19264/74412]\n",
      "loss: 0.992944  [25664/74412]\n",
      "loss: 1.075425  [32064/74412]\n",
      "loss: 1.013248  [38464/74412]\n",
      "loss: 0.820813  [44864/74412]\n",
      "loss: 0.989727  [51264/74412]\n",
      "loss: 0.688833  [57664/74412]\n",
      "loss: 1.118948  [64064/74412]\n",
      "loss: 0.777356  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 1.089765 \n",
      "\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "loss: 1.377418  [   64/74412]\n",
      "loss: 0.911637  [ 6464/74412]\n",
      "loss: 0.889128  [12864/74412]\n",
      "loss: 0.891695  [19264/74412]\n",
      "loss: 0.993185  [25664/74412]\n",
      "loss: 1.075033  [32064/74412]\n",
      "loss: 1.013099  [38464/74412]\n",
      "loss: 0.822523  [44864/74412]\n",
      "loss: 0.989848  [51264/74412]\n",
      "loss: 0.688439  [57664/74412]\n",
      "loss: 1.119262  [64064/74412]\n",
      "loss: 0.778153  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 1.089444 \n",
      "\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "loss: 1.376983  [   64/74412]\n",
      "loss: 0.909954  [ 6464/74412]\n",
      "loss: 0.889084  [12864/74412]\n",
      "loss: 0.890310  [19264/74412]\n",
      "loss: 0.992006  [25664/74412]\n",
      "loss: 1.073069  [32064/74412]\n",
      "loss: 1.012202  [38464/74412]\n",
      "loss: 0.819575  [44864/74412]\n",
      "loss: 0.989028  [51264/74412]\n",
      "loss: 0.687727  [57664/74412]\n",
      "loss: 1.118248  [64064/74412]\n",
      "loss: 0.777761  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 1.088688 \n",
      "\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "loss: 1.376266  [   64/74412]\n",
      "loss: 0.909760  [ 6464/74412]\n",
      "loss: 0.889513  [12864/74412]\n",
      "loss: 0.890690  [19264/74412]\n",
      "loss: 0.992334  [25664/74412]\n",
      "loss: 1.071727  [32064/74412]\n",
      "loss: 1.012248  [38464/74412]\n",
      "loss: 0.819383  [44864/74412]\n",
      "loss: 0.988580  [51264/74412]\n",
      "loss: 0.686432  [57664/74412]\n",
      "loss: 1.117618  [64064/74412]\n",
      "loss: 0.777131  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.088180 \n",
      "\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "loss: 1.375913  [   64/74412]\n",
      "loss: 0.909539  [ 6464/74412]\n",
      "loss: 0.887948  [12864/74412]\n",
      "loss: 0.888203  [19264/74412]\n",
      "loss: 0.991233  [25664/74412]\n",
      "loss: 1.070823  [32064/74412]\n",
      "loss: 1.011452  [38464/74412]\n",
      "loss: 0.818793  [44864/74412]\n",
      "loss: 0.987236  [51264/74412]\n",
      "loss: 0.684782  [57664/74412]\n",
      "loss: 1.116824  [64064/74412]\n",
      "loss: 0.777449  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.087694 \n",
      "\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "loss: 1.374914  [   64/74412]\n",
      "loss: 0.908111  [ 6464/74412]\n",
      "loss: 0.888600  [12864/74412]\n",
      "loss: 0.888120  [19264/74412]\n",
      "loss: 0.991496  [25664/74412]\n",
      "loss: 1.070650  [32064/74412]\n",
      "loss: 1.010900  [38464/74412]\n",
      "loss: 0.820688  [44864/74412]\n",
      "loss: 0.987284  [51264/74412]\n",
      "loss: 0.684847  [57664/74412]\n",
      "loss: 1.116240  [64064/74412]\n",
      "loss: 0.776790  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.087363 \n",
      "\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "loss: 1.373893  [   64/74412]\n",
      "loss: 0.908112  [ 6464/74412]\n",
      "loss: 0.888358  [12864/74412]\n",
      "loss: 0.887538  [19264/74412]\n",
      "loss: 0.990087  [25664/74412]\n",
      "loss: 1.069289  [32064/74412]\n",
      "loss: 1.010980  [38464/74412]\n",
      "loss: 0.819893  [44864/74412]\n",
      "loss: 0.986108  [51264/74412]\n",
      "loss: 0.683686  [57664/74412]\n",
      "loss: 1.115192  [64064/74412]\n",
      "loss: 0.775657  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.086956 \n",
      "\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "loss: 1.372674  [   64/74412]\n",
      "loss: 0.907369  [ 6464/74412]\n",
      "loss: 0.886483  [12864/74412]\n",
      "loss: 0.886895  [19264/74412]\n",
      "loss: 0.989169  [25664/74412]\n",
      "loss: 1.069868  [32064/74412]\n",
      "loss: 1.009969  [38464/74412]\n",
      "loss: 0.818895  [44864/74412]\n",
      "loss: 0.986347  [51264/74412]\n",
      "loss: 0.682609  [57664/74412]\n",
      "loss: 1.114537  [64064/74412]\n",
      "loss: 0.775878  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.086480 \n",
      "\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "loss: 1.372703  [   64/74412]\n",
      "loss: 0.907517  [ 6464/74412]\n",
      "loss: 0.886987  [12864/74412]\n",
      "loss: 0.885506  [19264/74412]\n",
      "loss: 0.989364  [25664/74412]\n",
      "loss: 1.069374  [32064/74412]\n",
      "loss: 1.010087  [38464/74412]\n",
      "loss: 0.815968  [44864/74412]\n",
      "loss: 0.985699  [51264/74412]\n",
      "loss: 0.682139  [57664/74412]\n",
      "loss: 1.115048  [64064/74412]\n",
      "loss: 0.776162  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.085970 \n",
      "\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "loss: 1.371484  [   64/74412]\n",
      "loss: 0.907139  [ 6464/74412]\n",
      "loss: 0.887310  [12864/74412]\n",
      "loss: 0.884593  [19264/74412]\n",
      "loss: 0.988845  [25664/74412]\n",
      "loss: 1.067310  [32064/74412]\n",
      "loss: 1.009467  [38464/74412]\n",
      "loss: 0.815464  [44864/74412]\n",
      "loss: 0.984902  [51264/74412]\n",
      "loss: 0.681680  [57664/74412]\n",
      "loss: 1.113774  [64064/74412]\n",
      "loss: 0.775803  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.085490 \n",
      "\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "loss: 1.370571  [   64/74412]\n",
      "loss: 0.906861  [ 6464/74412]\n",
      "loss: 0.885754  [12864/74412]\n",
      "loss: 0.884233  [19264/74412]\n",
      "loss: 0.987298  [25664/74412]\n",
      "loss: 1.066637  [32064/74412]\n",
      "loss: 1.009325  [38464/74412]\n",
      "loss: 0.817045  [44864/74412]\n",
      "loss: 0.984822  [51264/74412]\n",
      "loss: 0.679758  [57664/74412]\n",
      "loss: 1.113202  [64064/74412]\n",
      "loss: 0.774986  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.084887 \n",
      "\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "loss: 1.369676  [   64/74412]\n",
      "loss: 0.905868  [ 6464/74412]\n",
      "loss: 0.885369  [12864/74412]\n",
      "loss: 0.883423  [19264/74412]\n",
      "loss: 0.987396  [25664/74412]\n",
      "loss: 1.065374  [32064/74412]\n",
      "loss: 1.008636  [38464/74412]\n",
      "loss: 0.814387  [44864/74412]\n",
      "loss: 0.984404  [51264/74412]\n",
      "loss: 0.678455  [57664/74412]\n",
      "loss: 1.112665  [64064/74412]\n",
      "loss: 0.775154  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 1.084493 \n",
      "\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "loss: 1.369271  [   64/74412]\n",
      "loss: 0.904690  [ 6464/74412]\n",
      "loss: 0.885387  [12864/74412]\n",
      "loss: 0.882363  [19264/74412]\n",
      "loss: 0.987569  [25664/74412]\n",
      "loss: 1.064909  [32064/74412]\n",
      "loss: 1.007780  [38464/74412]\n",
      "loss: 0.814249  [44864/74412]\n",
      "loss: 0.984257  [51264/74412]\n",
      "loss: 0.678368  [57664/74412]\n",
      "loss: 1.111664  [64064/74412]\n",
      "loss: 0.774384  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 1.083286 \n",
      "\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "loss: 1.366776  [   64/74412]\n",
      "loss: 0.904055  [ 6464/74412]\n",
      "loss: 0.884778  [12864/74412]\n",
      "loss: 0.882065  [19264/74412]\n",
      "loss: 0.986377  [25664/74412]\n",
      "loss: 1.063998  [32064/74412]\n",
      "loss: 1.007483  [38464/74412]\n",
      "loss: 0.814969  [44864/74412]\n",
      "loss: 0.983693  [51264/74412]\n",
      "loss: 0.678131  [57664/74412]\n",
      "loss: 1.111062  [64064/74412]\n",
      "loss: 0.773754  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 1.082923 \n",
      "\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "loss: 1.366812  [   64/74412]\n",
      "loss: 0.903645  [ 6464/74412]\n",
      "loss: 0.884661  [12864/74412]\n",
      "loss: 0.881886  [19264/74412]\n",
      "loss: 0.986285  [25664/74412]\n",
      "loss: 1.063410  [32064/74412]\n",
      "loss: 1.007558  [38464/74412]\n",
      "loss: 0.812123  [44864/74412]\n",
      "loss: 0.983009  [51264/74412]\n",
      "loss: 0.676612  [57664/74412]\n",
      "loss: 1.110293  [64064/74412]\n",
      "loss: 0.773678  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 1.082451 \n",
      "\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "loss: 1.365674  [   64/74412]\n",
      "loss: 0.903457  [ 6464/74412]\n",
      "loss: 0.883738  [12864/74412]\n",
      "loss: 0.881099  [19264/74412]\n",
      "loss: 0.985415  [25664/74412]\n",
      "loss: 1.061928  [32064/74412]\n",
      "loss: 1.007657  [38464/74412]\n",
      "loss: 0.813502  [44864/74412]\n",
      "loss: 0.983735  [51264/74412]\n",
      "loss: 0.676437  [57664/74412]\n",
      "loss: 1.109730  [64064/74412]\n",
      "loss: 0.773233  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 1.082133 \n",
      "\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "loss: 1.366483  [   64/74412]\n",
      "loss: 0.902866  [ 6464/74412]\n",
      "loss: 0.886260  [12864/74412]\n",
      "loss: 0.880550  [19264/74412]\n",
      "loss: 0.985595  [25664/74412]\n",
      "loss: 1.060989  [32064/74412]\n",
      "loss: 1.007420  [38464/74412]\n",
      "loss: 0.810791  [44864/74412]\n",
      "loss: 0.982116  [51264/74412]\n",
      "loss: 0.676069  [57664/74412]\n",
      "loss: 1.108500  [64064/74412]\n",
      "loss: 0.772912  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 1.081702 \n",
      "\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "loss: 1.365892  [   64/74412]\n",
      "loss: 0.902060  [ 6464/74412]\n",
      "loss: 0.883989  [12864/74412]\n",
      "loss: 0.879902  [19264/74412]\n",
      "loss: 0.984905  [25664/74412]\n",
      "loss: 1.061338  [32064/74412]\n",
      "loss: 1.006136  [38464/74412]\n",
      "loss: 0.810223  [44864/74412]\n",
      "loss: 0.982704  [51264/74412]\n",
      "loss: 0.675029  [57664/74412]\n",
      "loss: 1.107576  [64064/74412]\n",
      "loss: 0.772806  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 1.080800 \n",
      "\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "loss: 1.365442  [   64/74412]\n",
      "loss: 0.901380  [ 6464/74412]\n",
      "loss: 0.885040  [12864/74412]\n",
      "loss: 0.879158  [19264/74412]\n",
      "loss: 0.985082  [25664/74412]\n",
      "loss: 1.061209  [32064/74412]\n",
      "loss: 1.005306  [38464/74412]\n",
      "loss: 0.809275  [44864/74412]\n",
      "loss: 0.982806  [51264/74412]\n",
      "loss: 0.674343  [57664/74412]\n",
      "loss: 1.106843  [64064/74412]\n",
      "loss: 0.772531  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 1.081070 \n",
      "\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "loss: 1.365552  [   64/74412]\n",
      "loss: 0.901728  [ 6464/74412]\n",
      "loss: 0.883249  [12864/74412]\n",
      "loss: 0.878802  [19264/74412]\n",
      "loss: 0.984667  [25664/74412]\n",
      "loss: 1.059896  [32064/74412]\n",
      "loss: 1.005329  [38464/74412]\n",
      "loss: 0.808866  [44864/74412]\n",
      "loss: 0.981639  [51264/74412]\n",
      "loss: 0.673952  [57664/74412]\n",
      "loss: 1.106335  [64064/74412]\n",
      "loss: 0.772356  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 1.080684 \n",
      "\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "loss: 1.364223  [   64/74412]\n",
      "loss: 0.901159  [ 6464/74412]\n",
      "loss: 0.882775  [12864/74412]\n",
      "loss: 0.877605  [19264/74412]\n",
      "loss: 0.983876  [25664/74412]\n",
      "loss: 1.060555  [32064/74412]\n",
      "loss: 1.005588  [38464/74412]\n",
      "loss: 0.807143  [44864/74412]\n",
      "loss: 0.981199  [51264/74412]\n",
      "loss: 0.673358  [57664/74412]\n",
      "loss: 1.106018  [64064/74412]\n",
      "loss: 0.771984  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 1.079272 \n",
      "\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "loss: 1.361281  [   64/74412]\n",
      "loss: 0.899557  [ 6464/74412]\n",
      "loss: 0.882241  [12864/74412]\n",
      "loss: 0.877116  [19264/74412]\n",
      "loss: 0.983224  [25664/74412]\n",
      "loss: 1.059088  [32064/74412]\n",
      "loss: 1.005201  [38464/74412]\n",
      "loss: 0.806142  [44864/74412]\n",
      "loss: 0.980366  [51264/74412]\n",
      "loss: 0.672615  [57664/74412]\n",
      "loss: 1.104522  [64064/74412]\n",
      "loss: 0.771902  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 1.078222 \n",
      "\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "loss: 1.360244  [   64/74412]\n",
      "loss: 0.900586  [ 6464/74412]\n",
      "loss: 0.879303  [12864/74412]\n",
      "loss: 0.876761  [19264/74412]\n",
      "loss: 0.983255  [25664/74412]\n",
      "loss: 1.057774  [32064/74412]\n",
      "loss: 1.005190  [38464/74412]\n",
      "loss: 0.807623  [44864/74412]\n",
      "loss: 0.979493  [51264/74412]\n",
      "loss: 0.671712  [57664/74412]\n",
      "loss: 1.104934  [64064/74412]\n",
      "loss: 0.770421  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 1.077888 \n",
      "\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "loss: 1.359145  [   64/74412]\n",
      "loss: 0.898979  [ 6464/74412]\n",
      "loss: 0.877542  [12864/74412]\n",
      "loss: 0.876010  [19264/74412]\n",
      "loss: 0.983782  [25664/74412]\n",
      "loss: 1.057266  [32064/74412]\n",
      "loss: 1.003901  [38464/74412]\n",
      "loss: 0.807481  [44864/74412]\n",
      "loss: 0.979075  [51264/74412]\n",
      "loss: 0.671547  [57664/74412]\n",
      "loss: 1.104068  [64064/74412]\n",
      "loss: 0.770948  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 1.077264 \n",
      "\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "loss: 1.358177  [   64/74412]\n",
      "loss: 0.898740  [ 6464/74412]\n",
      "loss: 0.879897  [12864/74412]\n",
      "loss: 0.874818  [19264/74412]\n",
      "loss: 0.982469  [25664/74412]\n",
      "loss: 1.055896  [32064/74412]\n",
      "loss: 1.003091  [38464/74412]\n",
      "loss: 0.803978  [44864/74412]\n",
      "loss: 0.979511  [51264/74412]\n",
      "loss: 0.670534  [57664/74412]\n",
      "loss: 1.103362  [64064/74412]\n",
      "loss: 0.770519  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 1.076816 \n",
      "\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "loss: 1.357237  [   64/74412]\n",
      "loss: 0.897418  [ 6464/74412]\n",
      "loss: 0.879460  [12864/74412]\n",
      "loss: 0.874337  [19264/74412]\n",
      "loss: 0.982915  [25664/74412]\n",
      "loss: 1.056729  [32064/74412]\n",
      "loss: 1.002767  [38464/74412]\n",
      "loss: 0.803564  [44864/74412]\n",
      "loss: 0.979104  [51264/74412]\n",
      "loss: 0.670009  [57664/74412]\n",
      "loss: 1.102665  [64064/74412]\n",
      "loss: 0.770462  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1.076162 \n",
      "\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "loss: 1.356180  [   64/74412]\n",
      "loss: 0.896449  [ 6464/74412]\n",
      "loss: 0.878501  [12864/74412]\n",
      "loss: 0.873858  [19264/74412]\n",
      "loss: 0.981316  [25664/74412]\n",
      "loss: 1.055761  [32064/74412]\n",
      "loss: 1.001171  [38464/74412]\n",
      "loss: 0.802950  [44864/74412]\n",
      "loss: 0.978577  [51264/74412]\n",
      "loss: 0.668792  [57664/74412]\n",
      "loss: 1.101763  [64064/74412]\n",
      "loss: 0.770528  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 1.075679 \n",
      "\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "loss: 1.355885  [   64/74412]\n",
      "loss: 0.896152  [ 6464/74412]\n",
      "loss: 0.877369  [12864/74412]\n",
      "loss: 0.873333  [19264/74412]\n",
      "loss: 0.981626  [25664/74412]\n",
      "loss: 1.055042  [32064/74412]\n",
      "loss: 1.001189  [38464/74412]\n",
      "loss: 0.802548  [44864/74412]\n",
      "loss: 0.977893  [51264/74412]\n",
      "loss: 0.668330  [57664/74412]\n",
      "loss: 1.101239  [64064/74412]\n",
      "loss: 0.770343  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1.075214 \n",
      "\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "loss: 1.357039  [   64/74412]\n",
      "loss: 0.895446  [ 6464/74412]\n",
      "loss: 0.876913  [12864/74412]\n",
      "loss: 0.872482  [19264/74412]\n",
      "loss: 0.981083  [25664/74412]\n",
      "loss: 1.054196  [32064/74412]\n",
      "loss: 1.001197  [38464/74412]\n",
      "loss: 0.801727  [44864/74412]\n",
      "loss: 0.977217  [51264/74412]\n",
      "loss: 0.667887  [57664/74412]\n",
      "loss: 1.099090  [64064/74412]\n",
      "loss: 0.770205  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1.074984 \n",
      "\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "loss: 1.357286  [   64/74412]\n",
      "loss: 0.895242  [ 6464/74412]\n",
      "loss: 0.874765  [12864/74412]\n",
      "loss: 0.872153  [19264/74412]\n",
      "loss: 0.980897  [25664/74412]\n",
      "loss: 1.052744  [32064/74412]\n",
      "loss: 1.000612  [38464/74412]\n",
      "loss: 0.800883  [44864/74412]\n",
      "loss: 0.976921  [51264/74412]\n",
      "loss: 0.667415  [57664/74412]\n",
      "loss: 1.098896  [64064/74412]\n",
      "loss: 0.769844  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1.074267 \n",
      "\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "loss: 1.355399  [   64/74412]\n",
      "loss: 0.893563  [ 6464/74412]\n",
      "loss: 0.876871  [12864/74412]\n",
      "loss: 0.871534  [19264/74412]\n",
      "loss: 0.979772  [25664/74412]\n",
      "loss: 1.052994  [32064/74412]\n",
      "loss: 1.001466  [38464/74412]\n",
      "loss: 0.799785  [44864/74412]\n",
      "loss: 0.976185  [51264/74412]\n",
      "loss: 0.666435  [57664/74412]\n",
      "loss: 1.097919  [64064/74412]\n",
      "loss: 0.769392  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1.073649 \n",
      "\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "loss: 1.354733  [   64/74412]\n",
      "loss: 0.894865  [ 6464/74412]\n",
      "loss: 0.876275  [12864/74412]\n",
      "loss: 0.870576  [19264/74412]\n",
      "loss: 0.979671  [25664/74412]\n",
      "loss: 1.051651  [32064/74412]\n",
      "loss: 1.000117  [38464/74412]\n",
      "loss: 0.799072  [44864/74412]\n",
      "loss: 0.976289  [51264/74412]\n",
      "loss: 0.666380  [57664/74412]\n",
      "loss: 1.096997  [64064/74412]\n",
      "loss: 0.769724  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1.073201 \n",
      "\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "loss: 1.354082  [   64/74412]\n",
      "loss: 0.893807  [ 6464/74412]\n",
      "loss: 0.874663  [12864/74412]\n",
      "loss: 0.870267  [19264/74412]\n",
      "loss: 0.977646  [25664/74412]\n",
      "loss: 1.050376  [32064/74412]\n",
      "loss: 0.999952  [38464/74412]\n",
      "loss: 0.799321  [44864/74412]\n",
      "loss: 0.975100  [51264/74412]\n",
      "loss: 0.664875  [57664/74412]\n",
      "loss: 1.096215  [64064/74412]\n",
      "loss: 0.769911  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1.072635 \n",
      "\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "loss: 1.353311  [   64/74412]\n",
      "loss: 0.892681  [ 6464/74412]\n",
      "loss: 0.874426  [12864/74412]\n",
      "loss: 0.869224  [19264/74412]\n",
      "loss: 0.977226  [25664/74412]\n",
      "loss: 1.049372  [32064/74412]\n",
      "loss: 0.999095  [38464/74412]\n",
      "loss: 0.797887  [44864/74412]\n",
      "loss: 0.974605  [51264/74412]\n",
      "loss: 0.664689  [57664/74412]\n",
      "loss: 1.095837  [64064/74412]\n",
      "loss: 0.768391  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 1.072201 \n",
      "\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "loss: 1.352401  [   64/74412]\n",
      "loss: 0.893034  [ 6464/74412]\n",
      "loss: 0.873778  [12864/74412]\n",
      "loss: 0.868739  [19264/74412]\n",
      "loss: 0.976988  [25664/74412]\n",
      "loss: 1.048765  [32064/74412]\n",
      "loss: 0.999250  [38464/74412]\n",
      "loss: 0.796782  [44864/74412]\n",
      "loss: 0.973670  [51264/74412]\n",
      "loss: 0.663956  [57664/74412]\n",
      "loss: 1.095077  [64064/74412]\n",
      "loss: 0.769256  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 1.071678 \n",
      "\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "loss: 1.352355  [   64/74412]\n",
      "loss: 0.892120  [ 6464/74412]\n",
      "loss: 0.874516  [12864/74412]\n",
      "loss: 0.868715  [19264/74412]\n",
      "loss: 0.976133  [25664/74412]\n",
      "loss: 1.049106  [32064/74412]\n",
      "loss: 0.998360  [38464/74412]\n",
      "loss: 0.796911  [44864/74412]\n",
      "loss: 0.973311  [51264/74412]\n",
      "loss: 0.664038  [57664/74412]\n",
      "loss: 1.094487  [64064/74412]\n",
      "loss: 0.768713  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 1.071245 \n",
      "\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "loss: 1.350995  [   64/74412]\n",
      "loss: 0.892036  [ 6464/74412]\n",
      "loss: 0.873174  [12864/74412]\n",
      "loss: 0.867277  [19264/74412]\n",
      "loss: 0.976535  [25664/74412]\n",
      "loss: 1.047420  [32064/74412]\n",
      "loss: 0.998146  [38464/74412]\n",
      "loss: 0.795646  [44864/74412]\n",
      "loss: 0.972672  [51264/74412]\n",
      "loss: 0.662855  [57664/74412]\n",
      "loss: 1.093564  [64064/74412]\n",
      "loss: 0.769210  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 1.070779 \n",
      "\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "loss: 1.349927  [   64/74412]\n",
      "loss: 0.890933  [ 6464/74412]\n",
      "loss: 0.872769  [12864/74412]\n",
      "loss: 0.866849  [19264/74412]\n",
      "loss: 0.975633  [25664/74412]\n",
      "loss: 1.046568  [32064/74412]\n",
      "loss: 0.997409  [38464/74412]\n",
      "loss: 0.794643  [44864/74412]\n",
      "loss: 0.972062  [51264/74412]\n",
      "loss: 0.662369  [57664/74412]\n",
      "loss: 1.092613  [64064/74412]\n",
      "loss: 0.769040  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 1.070289 \n",
      "\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "loss: 1.348333  [   64/74412]\n",
      "loss: 0.890645  [ 6464/74412]\n",
      "loss: 0.871565  [12864/74412]\n",
      "loss: 0.865929  [19264/74412]\n",
      "loss: 0.975073  [25664/74412]\n",
      "loss: 1.045656  [32064/74412]\n",
      "loss: 0.996559  [38464/74412]\n",
      "loss: 0.795996  [44864/74412]\n",
      "loss: 0.972155  [51264/74412]\n",
      "loss: 0.661938  [57664/74412]\n",
      "loss: 1.091185  [64064/74412]\n",
      "loss: 0.769554  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 1.069368 \n",
      "\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "loss: 1.347449  [   64/74412]\n",
      "loss: 0.890246  [ 6464/74412]\n",
      "loss: 0.872054  [12864/74412]\n",
      "loss: 0.866591  [19264/74412]\n",
      "loss: 0.974284  [25664/74412]\n",
      "loss: 1.044950  [32064/74412]\n",
      "loss: 0.996841  [38464/74412]\n",
      "loss: 0.795623  [44864/74412]\n",
      "loss: 0.971201  [51264/74412]\n",
      "loss: 0.661241  [57664/74412]\n",
      "loss: 1.090267  [64064/74412]\n",
      "loss: 0.768355  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 1.068981 \n",
      "\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "loss: 1.346487  [   64/74412]\n",
      "loss: 0.889318  [ 6464/74412]\n",
      "loss: 0.870430  [12864/74412]\n",
      "loss: 0.864569  [19264/74412]\n",
      "loss: 0.973963  [25664/74412]\n",
      "loss: 1.043673  [32064/74412]\n",
      "loss: 0.996289  [38464/74412]\n",
      "loss: 0.795100  [44864/74412]\n",
      "loss: 0.971078  [51264/74412]\n",
      "loss: 0.661153  [57664/74412]\n",
      "loss: 1.089552  [64064/74412]\n",
      "loss: 0.767644  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1.072762 \n",
      "\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "loss: 1.347388  [   64/74412]\n",
      "loss: 0.889396  [ 6464/74412]\n",
      "loss: 0.870621  [12864/74412]\n",
      "loss: 0.863996  [19264/74412]\n",
      "loss: 0.973594  [25664/74412]\n",
      "loss: 1.043959  [32064/74412]\n",
      "loss: 0.996057  [38464/74412]\n",
      "loss: 0.793394  [44864/74412]\n",
      "loss: 0.970987  [51264/74412]\n",
      "loss: 0.660597  [57664/74412]\n",
      "loss: 1.088958  [64064/74412]\n",
      "loss: 0.767766  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1.072260 \n",
      "\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "loss: 1.346063  [   64/74412]\n",
      "loss: 0.888459  [ 6464/74412]\n",
      "loss: 0.870891  [12864/74412]\n",
      "loss: 0.863707  [19264/74412]\n",
      "loss: 0.972840  [25664/74412]\n",
      "loss: 1.043449  [32064/74412]\n",
      "loss: 0.994935  [38464/74412]\n",
      "loss: 0.794041  [44864/74412]\n",
      "loss: 0.970839  [51264/74412]\n",
      "loss: 0.660412  [57664/74412]\n",
      "loss: 1.087677  [64064/74412]\n",
      "loss: 0.767754  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 1.071579 \n",
      "\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "loss: 1.344830  [   64/74412]\n",
      "loss: 0.888125  [ 6464/74412]\n",
      "loss: 0.871145  [12864/74412]\n",
      "loss: 0.863251  [19264/74412]\n",
      "loss: 0.972409  [25664/74412]\n",
      "loss: 1.042902  [32064/74412]\n",
      "loss: 0.994541  [38464/74412]\n",
      "loss: 0.794852  [44864/74412]\n",
      "loss: 0.970256  [51264/74412]\n",
      "loss: 0.659429  [57664/74412]\n",
      "loss: 1.087948  [64064/74412]\n",
      "loss: 0.766757  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 1.071188 \n",
      "\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "loss: 1.344620  [   64/74412]\n",
      "loss: 0.887725  [ 6464/74412]\n",
      "loss: 0.869954  [12864/74412]\n",
      "loss: 0.862725  [19264/74412]\n",
      "loss: 0.972230  [25664/74412]\n",
      "loss: 1.042320  [32064/74412]\n",
      "loss: 0.994548  [38464/74412]\n",
      "loss: 0.792163  [44864/74412]\n",
      "loss: 0.969906  [51264/74412]\n",
      "loss: 0.658827  [57664/74412]\n",
      "loss: 1.086804  [64064/74412]\n",
      "loss: 0.767298  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 1.070779 \n",
      "\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "loss: 1.342873  [   64/74412]\n",
      "loss: 0.886659  [ 6464/74412]\n",
      "loss: 0.869506  [12864/74412]\n",
      "loss: 0.862247  [19264/74412]\n",
      "loss: 0.972023  [25664/74412]\n",
      "loss: 1.041798  [32064/74412]\n",
      "loss: 0.993342  [38464/74412]\n",
      "loss: 0.791796  [44864/74412]\n",
      "loss: 0.969919  [51264/74412]\n",
      "loss: 0.658041  [57664/74412]\n",
      "loss: 1.086122  [64064/74412]\n",
      "loss: 0.768116  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 1.069944 \n",
      "\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "loss: 1.342599  [   64/74412]\n",
      "loss: 0.885579  [ 6464/74412]\n",
      "loss: 0.870498  [12864/74412]\n",
      "loss: 0.860907  [19264/74412]\n",
      "loss: 0.970708  [25664/74412]\n",
      "loss: 1.041700  [32064/74412]\n",
      "loss: 0.993121  [38464/74412]\n",
      "loss: 0.791643  [44864/74412]\n",
      "loss: 0.969695  [51264/74412]\n",
      "loss: 0.657567  [57664/74412]\n",
      "loss: 1.084864  [64064/74412]\n",
      "loss: 0.768150  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 1.069696 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "loss: 1.341678  [   64/74412]\n",
      "loss: 0.884933  [ 6464/74412]\n",
      "loss: 0.870594  [12864/74412]\n",
      "loss: 0.860929  [19264/74412]\n",
      "loss: 0.970376  [25664/74412]\n",
      "loss: 1.040141  [32064/74412]\n",
      "loss: 0.993307  [38464/74412]\n",
      "loss: 0.791015  [44864/74412]\n",
      "loss: 0.969355  [51264/74412]\n",
      "loss: 0.657269  [57664/74412]\n",
      "loss: 1.084662  [64064/74412]\n",
      "loss: 0.767472  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 1.069026 \n",
      "\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "loss: 1.340551  [   64/74412]\n",
      "loss: 0.884048  [ 6464/74412]\n",
      "loss: 0.870363  [12864/74412]\n",
      "loss: 0.859578  [19264/74412]\n",
      "loss: 0.970690  [25664/74412]\n",
      "loss: 1.039683  [32064/74412]\n",
      "loss: 0.992139  [38464/74412]\n",
      "loss: 0.789646  [44864/74412]\n",
      "loss: 0.968983  [51264/74412]\n",
      "loss: 0.656454  [57664/74412]\n",
      "loss: 1.083667  [64064/74412]\n",
      "loss: 0.765241  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 1.065706 \n",
      "\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "loss: 1.338237  [   64/74412]\n",
      "loss: 0.883883  [ 6464/74412]\n",
      "loss: 0.868899  [12864/74412]\n",
      "loss: 0.859374  [19264/74412]\n",
      "loss: 0.969632  [25664/74412]\n",
      "loss: 1.038671  [32064/74412]\n",
      "loss: 0.990461  [38464/74412]\n",
      "loss: 0.788500  [44864/74412]\n",
      "loss: 0.968114  [51264/74412]\n",
      "loss: 0.655616  [57664/74412]\n",
      "loss: 1.083158  [64064/74412]\n",
      "loss: 0.765252  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 1.068272 \n",
      "\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "loss: 1.338791  [   64/74412]\n",
      "loss: 0.882531  [ 6464/74412]\n",
      "loss: 0.868912  [12864/74412]\n",
      "loss: 0.857998  [19264/74412]\n",
      "loss: 0.969372  [25664/74412]\n",
      "loss: 1.037801  [32064/74412]\n",
      "loss: 0.989764  [38464/74412]\n",
      "loss: 0.788051  [44864/74412]\n",
      "loss: 0.968025  [51264/74412]\n",
      "loss: 0.655195  [57664/74412]\n",
      "loss: 1.082326  [64064/74412]\n",
      "loss: 0.765774  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 1.067659 \n",
      "\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "loss: 1.338037  [   64/74412]\n",
      "loss: 0.881908  [ 6464/74412]\n",
      "loss: 0.868411  [12864/74412]\n",
      "loss: 0.858706  [19264/74412]\n",
      "loss: 0.968378  [25664/74412]\n",
      "loss: 1.037915  [32064/74412]\n",
      "loss: 0.988733  [38464/74412]\n",
      "loss: 0.788518  [44864/74412]\n",
      "loss: 0.966312  [51264/74412]\n",
      "loss: 0.654011  [57664/74412]\n",
      "loss: 1.081730  [64064/74412]\n",
      "loss: 0.764197  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 1.067300 \n",
      "\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "loss: 1.337747  [   64/74412]\n",
      "loss: 0.881099  [ 6464/74412]\n",
      "loss: 0.867058  [12864/74412]\n",
      "loss: 0.857396  [19264/74412]\n",
      "loss: 0.969475  [25664/74412]\n",
      "loss: 1.036233  [32064/74412]\n",
      "loss: 0.988730  [38464/74412]\n",
      "loss: 0.787120  [44864/74412]\n",
      "loss: 0.966041  [51264/74412]\n",
      "loss: 0.653801  [57664/74412]\n",
      "loss: 1.081458  [64064/74412]\n",
      "loss: 0.765124  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 1.066968 \n",
      "\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "loss: 1.335961  [   64/74412]\n",
      "loss: 0.880595  [ 6464/74412]\n",
      "loss: 0.867389  [12864/74412]\n",
      "loss: 0.856582  [19264/74412]\n",
      "loss: 0.969067  [25664/74412]\n",
      "loss: 1.035685  [32064/74412]\n",
      "loss: 0.988266  [38464/74412]\n",
      "loss: 0.786175  [44864/74412]\n",
      "loss: 0.964883  [51264/74412]\n",
      "loss: 0.653093  [57664/74412]\n",
      "loss: 1.080106  [64064/74412]\n",
      "loss: 0.764575  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 1.066174 \n",
      "\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "loss: 1.335940  [   64/74412]\n",
      "loss: 0.880079  [ 6464/74412]\n",
      "loss: 0.866910  [12864/74412]\n",
      "loss: 0.856197  [19264/74412]\n",
      "loss: 0.968113  [25664/74412]\n",
      "loss: 1.034998  [32064/74412]\n",
      "loss: 0.987966  [38464/74412]\n",
      "loss: 0.786401  [44864/74412]\n",
      "loss: 0.964857  [51264/74412]\n",
      "loss: 0.653036  [57664/74412]\n",
      "loss: 1.079449  [64064/74412]\n",
      "loss: 0.764003  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 1.065507 \n",
      "\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "loss: 1.334869  [   64/74412]\n",
      "loss: 0.880441  [ 6464/74412]\n",
      "loss: 0.866922  [12864/74412]\n",
      "loss: 0.855359  [19264/74412]\n",
      "loss: 0.967458  [25664/74412]\n",
      "loss: 1.034382  [32064/74412]\n",
      "loss: 0.987264  [38464/74412]\n",
      "loss: 0.786029  [44864/74412]\n",
      "loss: 0.963993  [51264/74412]\n",
      "loss: 0.652598  [57664/74412]\n",
      "loss: 1.078818  [64064/74412]\n",
      "loss: 0.764516  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 1.065257 \n",
      "\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "loss: 1.333285  [   64/74412]\n",
      "loss: 0.880141  [ 6464/74412]\n",
      "loss: 0.866622  [12864/74412]\n",
      "loss: 0.855738  [19264/74412]\n",
      "loss: 0.967229  [25664/74412]\n",
      "loss: 1.034329  [32064/74412]\n",
      "loss: 0.986780  [38464/74412]\n",
      "loss: 0.785237  [44864/74412]\n",
      "loss: 0.964864  [51264/74412]\n",
      "loss: 0.652191  [57664/74412]\n",
      "loss: 1.077751  [64064/74412]\n",
      "loss: 0.764090  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 1.064884 \n",
      "\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "loss: 1.333722  [   64/74412]\n",
      "loss: 0.879243  [ 6464/74412]\n",
      "loss: 0.865882  [12864/74412]\n",
      "loss: 0.854783  [19264/74412]\n",
      "loss: 0.966341  [25664/74412]\n",
      "loss: 1.033545  [32064/74412]\n",
      "loss: 0.986253  [38464/74412]\n",
      "loss: 0.784574  [44864/74412]\n",
      "loss: 0.964496  [51264/74412]\n",
      "loss: 0.651684  [57664/74412]\n",
      "loss: 1.076121  [64064/74412]\n",
      "loss: 0.764024  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 1.063595 \n",
      "\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "loss: 1.331794  [   64/74412]\n",
      "loss: 0.879047  [ 6464/74412]\n",
      "loss: 0.865839  [12864/74412]\n",
      "loss: 0.854133  [19264/74412]\n",
      "loss: 0.966091  [25664/74412]\n",
      "loss: 1.033054  [32064/74412]\n",
      "loss: 0.985155  [38464/74412]\n",
      "loss: 0.783652  [44864/74412]\n",
      "loss: 0.964242  [51264/74412]\n",
      "loss: 0.651286  [57664/74412]\n",
      "loss: 1.075981  [64064/74412]\n",
      "loss: 0.763999  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 1.063347 \n",
      "\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "loss: 1.330137  [   64/74412]\n",
      "loss: 0.878673  [ 6464/74412]\n",
      "loss: 0.865658  [12864/74412]\n",
      "loss: 0.853800  [19264/74412]\n",
      "loss: 0.965396  [25664/74412]\n",
      "loss: 1.032452  [32064/74412]\n",
      "loss: 0.984094  [38464/74412]\n",
      "loss: 0.783199  [44864/74412]\n",
      "loss: 0.963594  [51264/74412]\n",
      "loss: 0.651002  [57664/74412]\n",
      "loss: 1.074613  [64064/74412]\n",
      "loss: 0.764447  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 1.062654 \n",
      "\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "loss: 1.329332  [   64/74412]\n",
      "loss: 0.878273  [ 6464/74412]\n",
      "loss: 0.865385  [12864/74412]\n",
      "loss: 0.852335  [19264/74412]\n",
      "loss: 0.964766  [25664/74412]\n",
      "loss: 1.032188  [32064/74412]\n",
      "loss: 0.984013  [38464/74412]\n",
      "loss: 0.782784  [44864/74412]\n",
      "loss: 0.963374  [51264/74412]\n",
      "loss: 0.650471  [57664/74412]\n",
      "loss: 1.073920  [64064/74412]\n",
      "loss: 0.764073  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 1.062157 \n",
      "\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "loss: 1.328703  [   64/74412]\n",
      "loss: 0.878960  [ 6464/74412]\n",
      "loss: 0.864578  [12864/74412]\n",
      "loss: 0.851801  [19264/74412]\n",
      "loss: 0.963674  [25664/74412]\n",
      "loss: 1.031305  [32064/74412]\n",
      "loss: 0.984124  [38464/74412]\n",
      "loss: 0.781732  [44864/74412]\n",
      "loss: 0.962247  [51264/74412]\n",
      "loss: 0.649798  [57664/74412]\n",
      "loss: 1.072918  [64064/74412]\n",
      "loss: 0.762900  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 1.061506 \n",
      "\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "loss: 1.327920  [   64/74412]\n",
      "loss: 0.877533  [ 6464/74412]\n",
      "loss: 0.865046  [12864/74412]\n",
      "loss: 0.850529  [19264/74412]\n",
      "loss: 0.963656  [25664/74412]\n",
      "loss: 1.030666  [32064/74412]\n",
      "loss: 0.984114  [38464/74412]\n",
      "loss: 0.780754  [44864/74412]\n",
      "loss: 0.962038  [51264/74412]\n",
      "loss: 0.649483  [57664/74412]\n",
      "loss: 1.072922  [64064/74412]\n",
      "loss: 0.763829  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 1.061367 \n",
      "\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "loss: 1.327385  [   64/74412]\n",
      "loss: 0.877443  [ 6464/74412]\n",
      "loss: 0.864203  [12864/74412]\n",
      "loss: 0.851876  [19264/74412]\n",
      "loss: 0.963140  [25664/74412]\n",
      "loss: 1.029717  [32064/74412]\n",
      "loss: 0.982414  [38464/74412]\n",
      "loss: 0.780059  [44864/74412]\n",
      "loss: 0.961559  [51264/74412]\n",
      "loss: 0.648231  [57664/74412]\n",
      "loss: 1.072139  [64064/74412]\n",
      "loss: 0.763662  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 1.060947 \n",
      "\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "loss: 1.326062  [   64/74412]\n",
      "loss: 0.876487  [ 6464/74412]\n",
      "loss: 0.863935  [12864/74412]\n",
      "loss: 0.850777  [19264/74412]\n",
      "loss: 0.962445  [25664/74412]\n",
      "loss: 1.028944  [32064/74412]\n",
      "loss: 0.981348  [38464/74412]\n",
      "loss: 0.781653  [44864/74412]\n",
      "loss: 0.961368  [51264/74412]\n",
      "loss: 0.647085  [57664/74412]\n",
      "loss: 1.071366  [64064/74412]\n",
      "loss: 0.763246  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 1.060146 \n",
      "\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "loss: 1.325289  [   64/74412]\n",
      "loss: 0.876943  [ 6464/74412]\n",
      "loss: 0.865207  [12864/74412]\n",
      "loss: 0.849976  [19264/74412]\n",
      "loss: 0.962055  [25664/74412]\n",
      "loss: 1.028314  [32064/74412]\n",
      "loss: 0.981455  [38464/74412]\n",
      "loss: 0.779664  [44864/74412]\n",
      "loss: 0.960749  [51264/74412]\n",
      "loss: 0.647303  [57664/74412]\n",
      "loss: 1.070919  [64064/74412]\n",
      "loss: 0.763155  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 1.059334 \n",
      "\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "loss: 1.323792  [   64/74412]\n",
      "loss: 0.875501  [ 6464/74412]\n",
      "loss: 0.864701  [12864/74412]\n",
      "loss: 0.850191  [19264/74412]\n",
      "loss: 0.961548  [25664/74412]\n",
      "loss: 1.027783  [32064/74412]\n",
      "loss: 0.981216  [38464/74412]\n",
      "loss: 0.779769  [44864/74412]\n",
      "loss: 0.958750  [51264/74412]\n",
      "loss: 0.646066  [57664/74412]\n",
      "loss: 1.070774  [64064/74412]\n",
      "loss: 0.762973  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 1.059194 \n",
      "\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "loss: 1.324119  [   64/74412]\n",
      "loss: 0.876950  [ 6464/74412]\n",
      "loss: 0.862710  [12864/74412]\n",
      "loss: 0.848731  [19264/74412]\n",
      "loss: 0.964562  [25664/74412]\n",
      "loss: 1.027369  [32064/74412]\n",
      "loss: 0.980465  [38464/74412]\n",
      "loss: 0.777848  [44864/74412]\n",
      "loss: 0.959710  [51264/74412]\n",
      "loss: 0.646544  [57664/74412]\n",
      "loss: 1.069800  [64064/74412]\n",
      "loss: 0.762585  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 1.058222 \n",
      "\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "loss: 1.322198  [   64/74412]\n",
      "loss: 0.875551  [ 6464/74412]\n",
      "loss: 0.862633  [12864/74412]\n",
      "loss: 0.848104  [19264/74412]\n",
      "loss: 0.961844  [25664/74412]\n",
      "loss: 1.026646  [32064/74412]\n",
      "loss: 0.980154  [38464/74412]\n",
      "loss: 0.778562  [44864/74412]\n",
      "loss: 0.959298  [51264/74412]\n",
      "loss: 0.646721  [57664/74412]\n",
      "loss: 1.068909  [64064/74412]\n",
      "loss: 0.762352  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 1.057757 \n",
      "\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "loss: 1.321462  [   64/74412]\n",
      "loss: 0.873592  [ 6464/74412]\n",
      "loss: 0.862000  [12864/74412]\n",
      "loss: 0.847769  [19264/74412]\n",
      "loss: 0.960797  [25664/74412]\n",
      "loss: 1.026617  [32064/74412]\n",
      "loss: 0.979584  [38464/74412]\n",
      "loss: 0.777398  [44864/74412]\n",
      "loss: 0.958908  [51264/74412]\n",
      "loss: 0.646487  [57664/74412]\n",
      "loss: 1.068923  [64064/74412]\n",
      "loss: 0.761939  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 1.057488 \n",
      "\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "loss: 1.319874  [   64/74412]\n",
      "loss: 0.873309  [ 6464/74412]\n",
      "loss: 0.861026  [12864/74412]\n",
      "loss: 0.846512  [19264/74412]\n",
      "loss: 0.960590  [25664/74412]\n",
      "loss: 1.025570  [32064/74412]\n",
      "loss: 0.979653  [38464/74412]\n",
      "loss: 0.776249  [44864/74412]\n",
      "loss: 0.958064  [51264/74412]\n",
      "loss: 0.646380  [57664/74412]\n",
      "loss: 1.067549  [64064/74412]\n",
      "loss: 0.761888  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 1.056765 \n",
      "\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "loss: 1.319038  [   64/74412]\n",
      "loss: 0.873881  [ 6464/74412]\n",
      "loss: 0.862203  [12864/74412]\n",
      "loss: 0.846310  [19264/74412]\n",
      "loss: 0.960318  [25664/74412]\n",
      "loss: 1.024539  [32064/74412]\n",
      "loss: 0.979238  [38464/74412]\n",
      "loss: 0.776611  [44864/74412]\n",
      "loss: 0.957397  [51264/74412]\n",
      "loss: 0.644869  [57664/74412]\n",
      "loss: 1.067294  [64064/74412]\n",
      "loss: 0.761442  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 1.056301 \n",
      "\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "loss: 1.319688  [   64/74412]\n",
      "loss: 0.872670  [ 6464/74412]\n",
      "loss: 0.859655  [12864/74412]\n",
      "loss: 0.844531  [19264/74412]\n",
      "loss: 0.960253  [25664/74412]\n",
      "loss: 1.023331  [32064/74412]\n",
      "loss: 0.978609  [38464/74412]\n",
      "loss: 0.775971  [44864/74412]\n",
      "loss: 0.957485  [51264/74412]\n",
      "loss: 0.644778  [57664/74412]\n",
      "loss: 1.066863  [64064/74412]\n",
      "loss: 0.760573  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 1.055911 \n",
      "\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "loss: 1.317985  [   64/74412]\n",
      "loss: 0.871647  [ 6464/74412]\n",
      "loss: 0.861405  [12864/74412]\n",
      "loss: 0.843752  [19264/74412]\n",
      "loss: 0.959134  [25664/74412]\n",
      "loss: 1.022459  [32064/74412]\n",
      "loss: 0.977826  [38464/74412]\n",
      "loss: 0.773167  [44864/74412]\n",
      "loss: 0.956504  [51264/74412]\n",
      "loss: 0.644187  [57664/74412]\n",
      "loss: 1.065830  [64064/74412]\n",
      "loss: 0.760494  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 1.055352 \n",
      "\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "loss: 1.317718  [   64/74412]\n",
      "loss: 0.871137  [ 6464/74412]\n",
      "loss: 0.860524  [12864/74412]\n",
      "loss: 0.843962  [19264/74412]\n",
      "loss: 0.959165  [25664/74412]\n",
      "loss: 1.021929  [32064/74412]\n",
      "loss: 0.977712  [38464/74412]\n",
      "loss: 0.771047  [44864/74412]\n",
      "loss: 0.956157  [51264/74412]\n",
      "loss: 0.643396  [57664/74412]\n",
      "loss: 1.065889  [64064/74412]\n",
      "loss: 0.761438  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 1.055004 \n",
      "\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "loss: 1.316516  [   64/74412]\n",
      "loss: 0.870748  [ 6464/74412]\n",
      "loss: 0.860522  [12864/74412]\n",
      "loss: 0.843161  [19264/74412]\n",
      "loss: 0.958181  [25664/74412]\n",
      "loss: 1.020698  [32064/74412]\n",
      "loss: 0.977180  [38464/74412]\n",
      "loss: 0.770548  [44864/74412]\n",
      "loss: 0.956458  [51264/74412]\n",
      "loss: 0.642458  [57664/74412]\n",
      "loss: 1.065729  [64064/74412]\n",
      "loss: 0.759477  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 1.054347 \n",
      "\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "loss: 1.315747  [   64/74412]\n",
      "loss: 0.870584  [ 6464/74412]\n",
      "loss: 0.860543  [12864/74412]\n",
      "loss: 0.841815  [19264/74412]\n",
      "loss: 0.957901  [25664/74412]\n",
      "loss: 1.020800  [32064/74412]\n",
      "loss: 0.976298  [38464/74412]\n",
      "loss: 0.770051  [44864/74412]\n",
      "loss: 0.955712  [51264/74412]\n",
      "loss: 0.642902  [57664/74412]\n",
      "loss: 1.064611  [64064/74412]\n",
      "loss: 0.760103  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 1.053815 \n",
      "\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "loss: 1.314157  [   64/74412]\n",
      "loss: 0.870109  [ 6464/74412]\n",
      "loss: 0.859585  [12864/74412]\n",
      "loss: 0.841568  [19264/74412]\n",
      "loss: 0.958174  [25664/74412]\n",
      "loss: 1.019859  [32064/74412]\n",
      "loss: 0.975464  [38464/74412]\n",
      "loss: 0.771498  [44864/74412]\n",
      "loss: 0.955111  [51264/74412]\n",
      "loss: 0.642020  [57664/74412]\n",
      "loss: 1.065340  [64064/74412]\n",
      "loss: 0.760593  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 1.053362 \n",
      "\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "loss: 1.314208  [   64/74412]\n",
      "loss: 0.869871  [ 6464/74412]\n",
      "loss: 0.858959  [12864/74412]\n",
      "loss: 0.841356  [19264/74412]\n",
      "loss: 0.956932  [25664/74412]\n",
      "loss: 1.019956  [32064/74412]\n",
      "loss: 0.975276  [38464/74412]\n",
      "loss: 0.771058  [44864/74412]\n",
      "loss: 0.954402  [51264/74412]\n",
      "loss: 0.641721  [57664/74412]\n",
      "loss: 1.064526  [64064/74412]\n",
      "loss: 0.760694  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 1.053174 \n",
      "\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "loss: 1.313448  [   64/74412]\n",
      "loss: 0.868564  [ 6464/74412]\n",
      "loss: 0.858037  [12864/74412]\n",
      "loss: 0.840988  [19264/74412]\n",
      "loss: 0.956740  [25664/74412]\n",
      "loss: 1.018865  [32064/74412]\n",
      "loss: 0.974159  [38464/74412]\n",
      "loss: 0.769607  [44864/74412]\n",
      "loss: 0.954488  [51264/74412]\n",
      "loss: 0.641310  [57664/74412]\n",
      "loss: 1.063785  [64064/74412]\n",
      "loss: 0.759935  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 1.052699 \n",
      "\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "loss: 1.313297  [   64/74412]\n",
      "loss: 0.868464  [ 6464/74412]\n",
      "loss: 0.858259  [12864/74412]\n",
      "loss: 0.840517  [19264/74412]\n",
      "loss: 0.957651  [25664/74412]\n",
      "loss: 1.018216  [32064/74412]\n",
      "loss: 0.975183  [38464/74412]\n",
      "loss: 0.768471  [44864/74412]\n",
      "loss: 0.953550  [51264/74412]\n",
      "loss: 0.640410  [57664/74412]\n",
      "loss: 1.062839  [64064/74412]\n",
      "loss: 0.759702  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 1.052186 \n",
      "\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "loss: 1.311906  [   64/74412]\n",
      "loss: 0.867737  [ 6464/74412]\n",
      "loss: 0.857994  [12864/74412]\n",
      "loss: 0.840438  [19264/74412]\n",
      "loss: 0.957833  [25664/74412]\n",
      "loss: 1.017532  [32064/74412]\n",
      "loss: 0.974182  [38464/74412]\n",
      "loss: 0.768178  [44864/74412]\n",
      "loss: 0.953059  [51264/74412]\n",
      "loss: 0.640149  [57664/74412]\n",
      "loss: 1.061576  [64064/74412]\n",
      "loss: 0.759419  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 1.051759 \n",
      "\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "loss: 1.311476  [   64/74412]\n",
      "loss: 0.867621  [ 6464/74412]\n",
      "loss: 0.857442  [12864/74412]\n",
      "loss: 0.839463  [19264/74412]\n",
      "loss: 0.957717  [25664/74412]\n",
      "loss: 1.015715  [32064/74412]\n",
      "loss: 0.973225  [38464/74412]\n",
      "loss: 0.767357  [44864/74412]\n",
      "loss: 0.952642  [51264/74412]\n",
      "loss: 0.639211  [57664/74412]\n",
      "loss: 1.063421  [64064/74412]\n",
      "loss: 0.759413  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 1.051236 \n",
      "\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "loss: 1.311406  [   64/74412]\n",
      "loss: 0.867096  [ 6464/74412]\n",
      "loss: 0.857386  [12864/74412]\n",
      "loss: 0.838604  [19264/74412]\n",
      "loss: 0.954796  [25664/74412]\n",
      "loss: 1.015823  [32064/74412]\n",
      "loss: 0.974178  [38464/74412]\n",
      "loss: 0.764076  [44864/74412]\n",
      "loss: 0.951850  [51264/74412]\n",
      "loss: 0.638226  [57664/74412]\n",
      "loss: 1.061804  [64064/74412]\n",
      "loss: 0.759079  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 1.050513 \n",
      "\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "loss: 1.309217  [   64/74412]\n",
      "loss: 0.865866  [ 6464/74412]\n",
      "loss: 0.855542  [12864/74412]\n",
      "loss: 0.836292  [19264/74412]\n",
      "loss: 0.956526  [25664/74412]\n",
      "loss: 1.015509  [32064/74412]\n",
      "loss: 0.973456  [38464/74412]\n",
      "loss: 0.764011  [44864/74412]\n",
      "loss: 0.951658  [51264/74412]\n",
      "loss: 0.638256  [57664/74412]\n",
      "loss: 1.059326  [64064/74412]\n",
      "loss: 0.758266  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 1.050311 \n",
      "\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "loss: 1.308443  [   64/74412]\n",
      "loss: 0.866469  [ 6464/74412]\n",
      "loss: 0.855773  [12864/74412]\n",
      "loss: 0.837328  [19264/74412]\n",
      "loss: 0.954801  [25664/74412]\n",
      "loss: 1.014626  [32064/74412]\n",
      "loss: 0.972543  [38464/74412]\n",
      "loss: 0.762881  [44864/74412]\n",
      "loss: 0.951001  [51264/74412]\n",
      "loss: 0.636882  [57664/74412]\n",
      "loss: 1.059480  [64064/74412]\n",
      "loss: 0.758626  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 1.050538 \n",
      "\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "loss: 1.309872  [   64/74412]\n",
      "loss: 0.866377  [ 6464/74412]\n",
      "loss: 0.855595  [12864/74412]\n",
      "loss: 0.835447  [19264/74412]\n",
      "loss: 0.954495  [25664/74412]\n",
      "loss: 1.014586  [32064/74412]\n",
      "loss: 0.972410  [38464/74412]\n",
      "loss: 0.762074  [44864/74412]\n",
      "loss: 0.950715  [51264/74412]\n",
      "loss: 0.637260  [57664/74412]\n",
      "loss: 1.058584  [64064/74412]\n",
      "loss: 0.759736  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 1.049803 \n",
      "\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "loss: 1.309984  [   64/74412]\n",
      "loss: 0.865892  [ 6464/74412]\n",
      "loss: 0.858435  [12864/74412]\n",
      "loss: 0.835031  [19264/74412]\n",
      "loss: 0.954374  [25664/74412]\n",
      "loss: 1.013012  [32064/74412]\n",
      "loss: 0.971962  [38464/74412]\n",
      "loss: 0.764802  [44864/74412]\n",
      "loss: 0.948869  [51264/74412]\n",
      "loss: 0.636377  [57664/74412]\n",
      "loss: 1.058230  [64064/74412]\n",
      "loss: 0.760000  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 1.049252 \n",
      "\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "loss: 1.308460  [   64/74412]\n",
      "loss: 0.865785  [ 6464/74412]\n",
      "loss: 0.856059  [12864/74412]\n",
      "loss: 0.834295  [19264/74412]\n",
      "loss: 0.954277  [25664/74412]\n",
      "loss: 1.013085  [32064/74412]\n",
      "loss: 0.971554  [38464/74412]\n",
      "loss: 0.762427  [44864/74412]\n",
      "loss: 0.948734  [51264/74412]\n",
      "loss: 0.636219  [57664/74412]\n",
      "loss: 1.058103  [64064/74412]\n",
      "loss: 0.759994  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 1.048781 \n",
      "\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "loss: 1.307824  [   64/74412]\n",
      "loss: 0.865298  [ 6464/74412]\n",
      "loss: 0.855890  [12864/74412]\n",
      "loss: 0.834700  [19264/74412]\n",
      "loss: 0.953890  [25664/74412]\n",
      "loss: 1.012451  [32064/74412]\n",
      "loss: 0.971221  [38464/74412]\n",
      "loss: 0.759749  [44864/74412]\n",
      "loss: 0.947906  [51264/74412]\n",
      "loss: 0.635587  [57664/74412]\n",
      "loss: 1.056717  [64064/74412]\n",
      "loss: 0.759446  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 1.048306 \n",
      "\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "loss: 1.306599  [   64/74412]\n",
      "loss: 0.864792  [ 6464/74412]\n",
      "loss: 0.857449  [12864/74412]\n",
      "loss: 0.832950  [19264/74412]\n",
      "loss: 0.953017  [25664/74412]\n",
      "loss: 1.011999  [32064/74412]\n",
      "loss: 0.971070  [38464/74412]\n",
      "loss: 0.759731  [44864/74412]\n",
      "loss: 0.946964  [51264/74412]\n",
      "loss: 0.635187  [57664/74412]\n",
      "loss: 1.055753  [64064/74412]\n",
      "loss: 0.759957  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 1.047491 \n",
      "\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "loss: 1.304974  [   64/74412]\n",
      "loss: 0.864584  [ 6464/74412]\n",
      "loss: 0.857381  [12864/74412]\n",
      "loss: 0.832216  [19264/74412]\n",
      "loss: 0.952195  [25664/74412]\n",
      "loss: 1.010576  [32064/74412]\n",
      "loss: 0.970672  [38464/74412]\n",
      "loss: 0.758620  [44864/74412]\n",
      "loss: 0.947152  [51264/74412]\n",
      "loss: 0.635218  [57664/74412]\n",
      "loss: 1.054894  [64064/74412]\n",
      "loss: 0.759370  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 1.047148 \n",
      "\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "loss: 1.304546  [   64/74412]\n",
      "loss: 0.865001  [ 6464/74412]\n",
      "loss: 0.854867  [12864/74412]\n",
      "loss: 0.830809  [19264/74412]\n",
      "loss: 0.952170  [25664/74412]\n",
      "loss: 1.010366  [32064/74412]\n",
      "loss: 0.969555  [38464/74412]\n",
      "loss: 0.757907  [44864/74412]\n",
      "loss: 0.947168  [51264/74412]\n",
      "loss: 0.633858  [57664/74412]\n",
      "loss: 1.055176  [64064/74412]\n",
      "loss: 0.759833  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 1.046733 \n",
      "\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "loss: 1.303384  [   64/74412]\n",
      "loss: 0.864807  [ 6464/74412]\n",
      "loss: 0.854980  [12864/74412]\n",
      "loss: 0.830511  [19264/74412]\n",
      "loss: 0.950522  [25664/74412]\n",
      "loss: 1.009070  [32064/74412]\n",
      "loss: 0.969040  [38464/74412]\n",
      "loss: 0.757094  [44864/74412]\n",
      "loss: 0.947087  [51264/74412]\n",
      "loss: 0.634152  [57664/74412]\n",
      "loss: 1.053167  [64064/74412]\n",
      "loss: 0.759068  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 1.046278 \n",
      "\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "loss: 1.302513  [   64/74412]\n",
      "loss: 0.864035  [ 6464/74412]\n",
      "loss: 0.855924  [12864/74412]\n",
      "loss: 0.831100  [19264/74412]\n",
      "loss: 0.950153  [25664/74412]\n",
      "loss: 1.008452  [32064/74412]\n",
      "loss: 0.966962  [38464/74412]\n",
      "loss: 0.756696  [44864/74412]\n",
      "loss: 0.946270  [51264/74412]\n",
      "loss: 0.634151  [57664/74412]\n",
      "loss: 1.052467  [64064/74412]\n",
      "loss: 0.758800  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 1.045723 \n",
      "\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "loss: 1.301852  [   64/74412]\n",
      "loss: 0.863228  [ 6464/74412]\n",
      "loss: 0.855784  [12864/74412]\n",
      "loss: 0.830112  [19264/74412]\n",
      "loss: 0.947799  [25664/74412]\n",
      "loss: 1.008049  [32064/74412]\n",
      "loss: 0.966805  [38464/74412]\n",
      "loss: 0.756210  [44864/74412]\n",
      "loss: 0.945125  [51264/74412]\n",
      "loss: 0.633142  [57664/74412]\n",
      "loss: 1.052051  [64064/74412]\n",
      "loss: 0.758689  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 1.045022 \n",
      "\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "loss: 1.299066  [   64/74412]\n",
      "loss: 0.863694  [ 6464/74412]\n",
      "loss: 0.856369  [12864/74412]\n",
      "loss: 0.828656  [19264/74412]\n",
      "loss: 0.949684  [25664/74412]\n",
      "loss: 1.006991  [32064/74412]\n",
      "loss: 0.966081  [38464/74412]\n",
      "loss: 0.755101  [44864/74412]\n",
      "loss: 0.945660  [51264/74412]\n",
      "loss: 0.633196  [57664/74412]\n",
      "loss: 1.050989  [64064/74412]\n",
      "loss: 0.758514  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 1.044803 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "loss: 1.299946  [   64/74412]\n",
      "loss: 0.862891  [ 6464/74412]\n",
      "loss: 0.852260  [12864/74412]\n",
      "loss: 0.828786  [19264/74412]\n",
      "loss: 0.949862  [25664/74412]\n",
      "loss: 1.006624  [32064/74412]\n",
      "loss: 0.964926  [38464/74412]\n",
      "loss: 0.753462  [44864/74412]\n",
      "loss: 0.944591  [51264/74412]\n",
      "loss: 0.632972  [57664/74412]\n",
      "loss: 1.050330  [64064/74412]\n",
      "loss: 0.758434  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 1.044406 \n",
      "\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "loss: 1.299446  [   64/74412]\n",
      "loss: 0.862774  [ 6464/74412]\n",
      "loss: 0.853968  [12864/74412]\n",
      "loss: 0.827049  [19264/74412]\n",
      "loss: 0.949359  [25664/74412]\n",
      "loss: 1.005058  [32064/74412]\n",
      "loss: 0.965377  [38464/74412]\n",
      "loss: 0.753935  [44864/74412]\n",
      "loss: 0.945246  [51264/74412]\n",
      "loss: 0.632296  [57664/74412]\n",
      "loss: 1.050693  [64064/74412]\n",
      "loss: 0.758606  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 1.043965 \n",
      "\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "loss: 1.298774  [   64/74412]\n",
      "loss: 0.862852  [ 6464/74412]\n",
      "loss: 0.853648  [12864/74412]\n",
      "loss: 0.827096  [19264/74412]\n",
      "loss: 0.949081  [25664/74412]\n",
      "loss: 1.004950  [32064/74412]\n",
      "loss: 0.965796  [38464/74412]\n",
      "loss: 0.753048  [44864/74412]\n",
      "loss: 0.944262  [51264/74412]\n",
      "loss: 0.631305  [57664/74412]\n",
      "loss: 1.049127  [64064/74412]\n",
      "loss: 0.757664  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 1.043446 \n",
      "\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "loss: 1.297381  [   64/74412]\n",
      "loss: 0.862182  [ 6464/74412]\n",
      "loss: 0.853589  [12864/74412]\n",
      "loss: 0.826188  [19264/74412]\n",
      "loss: 0.949319  [25664/74412]\n",
      "loss: 1.003968  [32064/74412]\n",
      "loss: 0.963939  [38464/74412]\n",
      "loss: 0.751002  [44864/74412]\n",
      "loss: 0.944181  [51264/74412]\n",
      "loss: 0.630933  [57664/74412]\n",
      "loss: 1.049845  [64064/74412]\n",
      "loss: 0.757191  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.041479 \n",
      "\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "loss: 1.295188  [   64/74412]\n",
      "loss: 0.862621  [ 6464/74412]\n",
      "loss: 0.853069  [12864/74412]\n",
      "loss: 0.825901  [19264/74412]\n",
      "loss: 0.949056  [25664/74412]\n",
      "loss: 1.003540  [32064/74412]\n",
      "loss: 0.964182  [38464/74412]\n",
      "loss: 0.751238  [44864/74412]\n",
      "loss: 0.944026  [51264/74412]\n",
      "loss: 0.630102  [57664/74412]\n",
      "loss: 1.048676  [64064/74412]\n",
      "loss: 0.757350  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 1.042050 \n",
      "\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "loss: 1.294627  [   64/74412]\n",
      "loss: 0.861973  [ 6464/74412]\n",
      "loss: 0.852503  [12864/74412]\n",
      "loss: 0.824642  [19264/74412]\n",
      "loss: 0.948373  [25664/74412]\n",
      "loss: 1.001939  [32064/74412]\n",
      "loss: 0.964259  [38464/74412]\n",
      "loss: 0.750135  [44864/74412]\n",
      "loss: 0.943538  [51264/74412]\n",
      "loss: 0.629397  [57664/74412]\n",
      "loss: 1.046774  [64064/74412]\n",
      "loss: 0.756707  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.040597 \n",
      "\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "loss: 1.293558  [   64/74412]\n",
      "loss: 0.861356  [ 6464/74412]\n",
      "loss: 0.852140  [12864/74412]\n",
      "loss: 0.824313  [19264/74412]\n",
      "loss: 0.947135  [25664/74412]\n",
      "loss: 1.001975  [32064/74412]\n",
      "loss: 0.963408  [38464/74412]\n",
      "loss: 0.749878  [44864/74412]\n",
      "loss: 0.941925  [51264/74412]\n",
      "loss: 0.629881  [57664/74412]\n",
      "loss: 1.047403  [64064/74412]\n",
      "loss: 0.757153  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.041234 \n",
      "\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "loss: 1.294085  [   64/74412]\n",
      "loss: 0.860256  [ 6464/74412]\n",
      "loss: 0.851641  [12864/74412]\n",
      "loss: 0.823931  [19264/74412]\n",
      "loss: 0.947797  [25664/74412]\n",
      "loss: 1.001636  [32064/74412]\n",
      "loss: 0.962900  [38464/74412]\n",
      "loss: 0.748847  [44864/74412]\n",
      "loss: 0.942793  [51264/74412]\n",
      "loss: 0.629669  [57664/74412]\n",
      "loss: 1.046797  [64064/74412]\n",
      "loss: 0.757249  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.040759 \n",
      "\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "loss: 1.292096  [   64/74412]\n",
      "loss: 0.860537  [ 6464/74412]\n",
      "loss: 0.852231  [12864/74412]\n",
      "loss: 0.823359  [19264/74412]\n",
      "loss: 0.946296  [25664/74412]\n",
      "loss: 1.000668  [32064/74412]\n",
      "loss: 0.963424  [38464/74412]\n",
      "loss: 0.748489  [44864/74412]\n",
      "loss: 0.942329  [51264/74412]\n",
      "loss: 0.628407  [57664/74412]\n",
      "loss: 1.047485  [64064/74412]\n",
      "loss: 0.755732  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.039620 \n",
      "\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "loss: 1.292290  [   64/74412]\n",
      "loss: 0.860382  [ 6464/74412]\n",
      "loss: 0.850959  [12864/74412]\n",
      "loss: 0.823228  [19264/74412]\n",
      "loss: 0.946742  [25664/74412]\n",
      "loss: 0.999807  [32064/74412]\n",
      "loss: 0.962047  [38464/74412]\n",
      "loss: 0.748123  [44864/74412]\n",
      "loss: 0.941146  [51264/74412]\n",
      "loss: 0.628877  [57664/74412]\n",
      "loss: 1.047440  [64064/74412]\n",
      "loss: 0.755092  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.038978 \n",
      "\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "loss: 1.290367  [   64/74412]\n",
      "loss: 0.858940  [ 6464/74412]\n",
      "loss: 0.851432  [12864/74412]\n",
      "loss: 0.822792  [19264/74412]\n",
      "loss: 0.946958  [25664/74412]\n",
      "loss: 0.998734  [32064/74412]\n",
      "loss: 0.961951  [38464/74412]\n",
      "loss: 0.747627  [44864/74412]\n",
      "loss: 0.941065  [51264/74412]\n",
      "loss: 0.627836  [57664/74412]\n",
      "loss: 1.045601  [64064/74412]\n",
      "loss: 0.753622  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.038516 \n",
      "\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "loss: 1.289770  [   64/74412]\n",
      "loss: 0.858910  [ 6464/74412]\n",
      "loss: 0.850758  [12864/74412]\n",
      "loss: 0.822062  [19264/74412]\n",
      "loss: 0.947115  [25664/74412]\n",
      "loss: 0.998335  [32064/74412]\n",
      "loss: 0.961098  [38464/74412]\n",
      "loss: 0.748341  [44864/74412]\n",
      "loss: 0.939764  [51264/74412]\n",
      "loss: 0.627381  [57664/74412]\n",
      "loss: 1.045922  [64064/74412]\n",
      "loss: 0.754402  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.037629 \n",
      "\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "loss: 1.287635  [   64/74412]\n",
      "loss: 0.859601  [ 6464/74412]\n",
      "loss: 0.850915  [12864/74412]\n",
      "loss: 0.821517  [19264/74412]\n",
      "loss: 0.945805  [25664/74412]\n",
      "loss: 0.997492  [32064/74412]\n",
      "loss: 0.960963  [38464/74412]\n",
      "loss: 0.746479  [44864/74412]\n",
      "loss: 0.939677  [51264/74412]\n",
      "loss: 0.626199  [57664/74412]\n",
      "loss: 1.045721  [64064/74412]\n",
      "loss: 0.753869  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 1.036995 \n",
      "\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "loss: 1.287369  [   64/74412]\n",
      "loss: 0.859026  [ 6464/74412]\n",
      "loss: 0.851031  [12864/74412]\n",
      "loss: 0.820902  [19264/74412]\n",
      "loss: 0.945762  [25664/74412]\n",
      "loss: 0.997264  [32064/74412]\n",
      "loss: 0.961812  [38464/74412]\n",
      "loss: 0.747360  [44864/74412]\n",
      "loss: 0.938271  [51264/74412]\n",
      "loss: 0.626384  [57664/74412]\n",
      "loss: 1.045034  [64064/74412]\n",
      "loss: 0.753293  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 1.036754 \n",
      "\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "loss: 1.287217  [   64/74412]\n",
      "loss: 0.858928  [ 6464/74412]\n",
      "loss: 0.849961  [12864/74412]\n",
      "loss: 0.819904  [19264/74412]\n",
      "loss: 0.944836  [25664/74412]\n",
      "loss: 0.996736  [32064/74412]\n",
      "loss: 0.960306  [38464/74412]\n",
      "loss: 0.746548  [44864/74412]\n",
      "loss: 0.938331  [51264/74412]\n",
      "loss: 0.625743  [57664/74412]\n",
      "loss: 1.044626  [64064/74412]\n",
      "loss: 0.753193  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 1.036344 \n",
      "\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "loss: 1.286047  [   64/74412]\n",
      "loss: 0.857540  [ 6464/74412]\n",
      "loss: 0.850239  [12864/74412]\n",
      "loss: 0.819607  [19264/74412]\n",
      "loss: 0.944591  [25664/74412]\n",
      "loss: 0.996564  [32064/74412]\n",
      "loss: 0.959672  [38464/74412]\n",
      "loss: 0.746089  [44864/74412]\n",
      "loss: 0.937627  [51264/74412]\n",
      "loss: 0.624941  [57664/74412]\n",
      "loss: 1.043983  [64064/74412]\n",
      "loss: 0.753428  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 1.036327 \n",
      "\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "loss: 1.286230  [   64/74412]\n",
      "loss: 0.857420  [ 6464/74412]\n",
      "loss: 0.850580  [12864/74412]\n",
      "loss: 0.818594  [19264/74412]\n",
      "loss: 0.944437  [25664/74412]\n",
      "loss: 0.995544  [32064/74412]\n",
      "loss: 0.958927  [38464/74412]\n",
      "loss: 0.745370  [44864/74412]\n",
      "loss: 0.935912  [51264/74412]\n",
      "loss: 0.624369  [57664/74412]\n",
      "loss: 1.043854  [64064/74412]\n",
      "loss: 0.753231  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 1.035835 \n",
      "\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "loss: 1.284005  [   64/74412]\n",
      "loss: 0.858378  [ 6464/74412]\n",
      "loss: 0.850072  [12864/74412]\n",
      "loss: 0.818276  [19264/74412]\n",
      "loss: 0.943970  [25664/74412]\n",
      "loss: 0.995378  [32064/74412]\n",
      "loss: 0.958737  [38464/74412]\n",
      "loss: 0.744714  [44864/74412]\n",
      "loss: 0.936886  [51264/74412]\n",
      "loss: 0.623616  [57664/74412]\n",
      "loss: 1.042531  [64064/74412]\n",
      "loss: 0.753236  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 1.035893 \n",
      "\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "loss: 1.284768  [   64/74412]\n",
      "loss: 0.856542  [ 6464/74412]\n",
      "loss: 0.849612  [12864/74412]\n",
      "loss: 0.818614  [19264/74412]\n",
      "loss: 0.943360  [25664/74412]\n",
      "loss: 0.995070  [32064/74412]\n",
      "loss: 0.958254  [38464/74412]\n",
      "loss: 0.743139  [44864/74412]\n",
      "loss: 0.935139  [51264/74412]\n",
      "loss: 0.623863  [57664/74412]\n",
      "loss: 1.040704  [64064/74412]\n",
      "loss: 0.752064  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 1.034968 \n",
      "\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "loss: 1.282008  [   64/74412]\n",
      "loss: 0.857912  [ 6464/74412]\n",
      "loss: 0.848959  [12864/74412]\n",
      "loss: 0.817985  [19264/74412]\n",
      "loss: 0.941832  [25664/74412]\n",
      "loss: 0.995127  [32064/74412]\n",
      "loss: 0.957768  [38464/74412]\n",
      "loss: 0.743046  [44864/74412]\n",
      "loss: 0.934970  [51264/74412]\n",
      "loss: 0.623279  [57664/74412]\n",
      "loss: 1.040616  [64064/74412]\n",
      "loss: 0.752425  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 1.034898 \n",
      "\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "loss: 1.281749  [   64/74412]\n",
      "loss: 0.856983  [ 6464/74412]\n",
      "loss: 0.848368  [12864/74412]\n",
      "loss: 0.816408  [19264/74412]\n",
      "loss: 0.942660  [25664/74412]\n",
      "loss: 0.994032  [32064/74412]\n",
      "loss: 0.957492  [38464/74412]\n",
      "loss: 0.742387  [44864/74412]\n",
      "loss: 0.934467  [51264/74412]\n",
      "loss: 0.622664  [57664/74412]\n",
      "loss: 1.040203  [64064/74412]\n",
      "loss: 0.751964  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 1.034408 \n",
      "\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "loss: 1.280654  [   64/74412]\n",
      "loss: 0.857715  [ 6464/74412]\n",
      "loss: 0.847657  [12864/74412]\n",
      "loss: 0.816507  [19264/74412]\n",
      "loss: 0.942772  [25664/74412]\n",
      "loss: 0.992924  [32064/74412]\n",
      "loss: 0.958049  [38464/74412]\n",
      "loss: 0.742510  [44864/74412]\n",
      "loss: 0.933551  [51264/74412]\n",
      "loss: 0.621842  [57664/74412]\n",
      "loss: 1.040087  [64064/74412]\n",
      "loss: 0.751061  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 1.033753 \n",
      "\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "loss: 1.280157  [   64/74412]\n",
      "loss: 0.854685  [ 6464/74412]\n",
      "loss: 0.848001  [12864/74412]\n",
      "loss: 0.816239  [19264/74412]\n",
      "loss: 0.941511  [25664/74412]\n",
      "loss: 0.992428  [32064/74412]\n",
      "loss: 0.957551  [38464/74412]\n",
      "loss: 0.741417  [44864/74412]\n",
      "loss: 0.933046  [51264/74412]\n",
      "loss: 0.619288  [57664/74412]\n",
      "loss: 1.039076  [64064/74412]\n",
      "loss: 0.751466  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 1.033749 \n",
      "\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "loss: 1.280232  [   64/74412]\n",
      "loss: 0.856793  [ 6464/74412]\n",
      "loss: 0.848107  [12864/74412]\n",
      "loss: 0.816182  [19264/74412]\n",
      "loss: 0.942571  [25664/74412]\n",
      "loss: 0.992301  [32064/74412]\n",
      "loss: 0.957497  [38464/74412]\n",
      "loss: 0.739760  [44864/74412]\n",
      "loss: 0.932324  [51264/74412]\n",
      "loss: 0.619773  [57664/74412]\n",
      "loss: 1.037630  [64064/74412]\n",
      "loss: 0.750993  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 1.033186 \n",
      "\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "loss: 1.278811  [   64/74412]\n",
      "loss: 0.853986  [ 6464/74412]\n",
      "loss: 0.847498  [12864/74412]\n",
      "loss: 0.814299  [19264/74412]\n",
      "loss: 0.940550  [25664/74412]\n",
      "loss: 0.991520  [32064/74412]\n",
      "loss: 0.956964  [38464/74412]\n",
      "loss: 0.738594  [44864/74412]\n",
      "loss: 0.932429  [51264/74412]\n",
      "loss: 0.619082  [57664/74412]\n",
      "loss: 1.038844  [64064/74412]\n",
      "loss: 0.751306  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 1.032770 \n",
      "\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "loss: 1.277806  [   64/74412]\n",
      "loss: 0.853542  [ 6464/74412]\n",
      "loss: 0.847580  [12864/74412]\n",
      "loss: 0.815545  [19264/74412]\n",
      "loss: 0.941999  [25664/74412]\n",
      "loss: 0.990983  [32064/74412]\n",
      "loss: 0.956899  [38464/74412]\n",
      "loss: 0.738682  [44864/74412]\n",
      "loss: 0.931746  [51264/74412]\n",
      "loss: 0.619279  [57664/74412]\n",
      "loss: 1.037793  [64064/74412]\n",
      "loss: 0.750383  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 1.032193 \n",
      "\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "loss: 1.276917  [   64/74412]\n",
      "loss: 0.852639  [ 6464/74412]\n",
      "loss: 0.847172  [12864/74412]\n",
      "loss: 0.814598  [19264/74412]\n",
      "loss: 0.940284  [25664/74412]\n",
      "loss: 0.990747  [32064/74412]\n",
      "loss: 0.956403  [38464/74412]\n",
      "loss: 0.736217  [44864/74412]\n",
      "loss: 0.931960  [51264/74412]\n",
      "loss: 0.618743  [57664/74412]\n",
      "loss: 1.036911  [64064/74412]\n",
      "loss: 0.750321  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 1.031718 \n",
      "\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "loss: 1.276687  [   64/74412]\n",
      "loss: 0.852072  [ 6464/74412]\n",
      "loss: 0.846700  [12864/74412]\n",
      "loss: 0.814683  [19264/74412]\n",
      "loss: 0.939970  [25664/74412]\n",
      "loss: 0.990003  [32064/74412]\n",
      "loss: 0.957740  [38464/74412]\n",
      "loss: 0.736154  [44864/74412]\n",
      "loss: 0.930326  [51264/74412]\n",
      "loss: 0.618507  [57664/74412]\n",
      "loss: 1.037395  [64064/74412]\n",
      "loss: 0.750175  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 1.031386 \n",
      "\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "loss: 1.275586  [   64/74412]\n",
      "loss: 0.851265  [ 6464/74412]\n",
      "loss: 0.847166  [12864/74412]\n",
      "loss: 0.813040  [19264/74412]\n",
      "loss: 0.941262  [25664/74412]\n",
      "loss: 0.989063  [32064/74412]\n",
      "loss: 0.957246  [38464/74412]\n",
      "loss: 0.736377  [44864/74412]\n",
      "loss: 0.930201  [51264/74412]\n",
      "loss: 0.617804  [57664/74412]\n",
      "loss: 1.035692  [64064/74412]\n",
      "loss: 0.750166  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 1.031155 \n",
      "\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "loss: 1.275250  [   64/74412]\n",
      "loss: 0.853048  [ 6464/74412]\n",
      "loss: 0.847206  [12864/74412]\n",
      "loss: 0.813021  [19264/74412]\n",
      "loss: 0.940572  [25664/74412]\n",
      "loss: 0.988728  [32064/74412]\n",
      "loss: 0.956691  [38464/74412]\n",
      "loss: 0.734559  [44864/74412]\n",
      "loss: 0.931018  [51264/74412]\n",
      "loss: 0.618045  [57664/74412]\n",
      "loss: 1.035442  [64064/74412]\n",
      "loss: 0.749169  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 1.030731 \n",
      "\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "loss: 1.274300  [   64/74412]\n",
      "loss: 0.850676  [ 6464/74412]\n",
      "loss: 0.846636  [12864/74412]\n",
      "loss: 0.812504  [19264/74412]\n",
      "loss: 0.938449  [25664/74412]\n",
      "loss: 0.988051  [32064/74412]\n",
      "loss: 0.957251  [38464/74412]\n",
      "loss: 0.734846  [44864/74412]\n",
      "loss: 0.930736  [51264/74412]\n",
      "loss: 0.616332  [57664/74412]\n",
      "loss: 1.035115  [64064/74412]\n",
      "loss: 0.748805  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 1.030031 \n",
      "\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "loss: 1.271825  [   64/74412]\n",
      "loss: 0.849847  [ 6464/74412]\n",
      "loss: 0.846420  [12864/74412]\n",
      "loss: 0.812655  [19264/74412]\n",
      "loss: 0.938737  [25664/74412]\n",
      "loss: 0.987669  [32064/74412]\n",
      "loss: 0.955529  [38464/74412]\n",
      "loss: 0.734742  [44864/74412]\n",
      "loss: 0.929765  [51264/74412]\n",
      "loss: 0.616254  [57664/74412]\n",
      "loss: 1.034229  [64064/74412]\n",
      "loss: 0.748579  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 1.029590 \n",
      "\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "loss: 1.273911  [   64/74412]\n",
      "loss: 0.850410  [ 6464/74412]\n",
      "loss: 0.846248  [12864/74412]\n",
      "loss: 0.811166  [19264/74412]\n",
      "loss: 0.937286  [25664/74412]\n",
      "loss: 0.986650  [32064/74412]\n",
      "loss: 0.955953  [38464/74412]\n",
      "loss: 0.733283  [44864/74412]\n",
      "loss: 0.929634  [51264/74412]\n",
      "loss: 0.616018  [57664/74412]\n",
      "loss: 1.033692  [64064/74412]\n",
      "loss: 0.748635  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 1.028879 \n",
      "\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "loss: 1.271649  [   64/74412]\n",
      "loss: 0.849750  [ 6464/74412]\n",
      "loss: 0.844689  [12864/74412]\n",
      "loss: 0.810421  [19264/74412]\n",
      "loss: 0.936955  [25664/74412]\n",
      "loss: 0.986004  [32064/74412]\n",
      "loss: 0.954996  [38464/74412]\n",
      "loss: 0.732183  [44864/74412]\n",
      "loss: 0.928156  [51264/74412]\n",
      "loss: 0.614456  [57664/74412]\n",
      "loss: 1.033238  [64064/74412]\n",
      "loss: 0.748164  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 1.028654 \n",
      "\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "loss: 1.271034  [   64/74412]\n",
      "loss: 0.849705  [ 6464/74412]\n",
      "loss: 0.844826  [12864/74412]\n",
      "loss: 0.811014  [19264/74412]\n",
      "loss: 0.938304  [25664/74412]\n",
      "loss: 0.985520  [32064/74412]\n",
      "loss: 0.955080  [38464/74412]\n",
      "loss: 0.733054  [44864/74412]\n",
      "loss: 0.928780  [51264/74412]\n",
      "loss: 0.614727  [57664/74412]\n",
      "loss: 1.032206  [64064/74412]\n",
      "loss: 0.749424  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 1.027509 \n",
      "\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "loss: 1.268132  [   64/74412]\n",
      "loss: 0.848778  [ 6464/74412]\n",
      "loss: 0.845249  [12864/74412]\n",
      "loss: 0.810577  [19264/74412]\n",
      "loss: 0.935873  [25664/74412]\n",
      "loss: 0.984591  [32064/74412]\n",
      "loss: 0.954558  [38464/74412]\n",
      "loss: 0.732167  [44864/74412]\n",
      "loss: 0.929452  [51264/74412]\n",
      "loss: 0.613911  [57664/74412]\n",
      "loss: 1.030708  [64064/74412]\n",
      "loss: 0.748649  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 1.027710 \n",
      "\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "loss: 1.270319  [   64/74412]\n",
      "loss: 0.849156  [ 6464/74412]\n",
      "loss: 0.843620  [12864/74412]\n",
      "loss: 0.809109  [19264/74412]\n",
      "loss: 0.936573  [25664/74412]\n",
      "loss: 0.984145  [32064/74412]\n",
      "loss: 0.953850  [38464/74412]\n",
      "loss: 0.731863  [44864/74412]\n",
      "loss: 0.928087  [51264/74412]\n",
      "loss: 0.612798  [57664/74412]\n",
      "loss: 1.029812  [64064/74412]\n",
      "loss: 0.748552  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 1.026728 \n",
      "\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "loss: 1.268291  [   64/74412]\n",
      "loss: 0.849423  [ 6464/74412]\n",
      "loss: 0.844425  [12864/74412]\n",
      "loss: 0.808768  [19264/74412]\n",
      "loss: 0.934666  [25664/74412]\n",
      "loss: 0.983291  [32064/74412]\n",
      "loss: 0.952974  [38464/74412]\n",
      "loss: 0.731039  [44864/74412]\n",
      "loss: 0.927152  [51264/74412]\n",
      "loss: 0.613064  [57664/74412]\n",
      "loss: 1.029406  [64064/74412]\n",
      "loss: 0.748427  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 1.026864 \n",
      "\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "loss: 1.268721  [   64/74412]\n",
      "loss: 0.849213  [ 6464/74412]\n",
      "loss: 0.844388  [12864/74412]\n",
      "loss: 0.808875  [19264/74412]\n",
      "loss: 0.934296  [25664/74412]\n",
      "loss: 0.982021  [32064/74412]\n",
      "loss: 0.951990  [38464/74412]\n",
      "loss: 0.731868  [44864/74412]\n",
      "loss: 0.925990  [51264/74412]\n",
      "loss: 0.612197  [57664/74412]\n",
      "loss: 1.028222  [64064/74412]\n",
      "loss: 0.747994  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 1.026866 \n",
      "\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "loss: 1.268273  [   64/74412]\n",
      "loss: 0.848821  [ 6464/74412]\n",
      "loss: 0.843573  [12864/74412]\n",
      "loss: 0.806923  [19264/74412]\n",
      "loss: 0.934269  [25664/74412]\n",
      "loss: 0.981759  [32064/74412]\n",
      "loss: 0.952825  [38464/74412]\n",
      "loss: 0.731200  [44864/74412]\n",
      "loss: 0.925424  [51264/74412]\n",
      "loss: 0.611928  [57664/74412]\n",
      "loss: 1.027117  [64064/74412]\n",
      "loss: 0.748357  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 1.025226 \n",
      "\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "loss: 1.265749  [   64/74412]\n",
      "loss: 0.848242  [ 6464/74412]\n",
      "loss: 0.843386  [12864/74412]\n",
      "loss: 0.807165  [19264/74412]\n",
      "loss: 0.933990  [25664/74412]\n",
      "loss: 0.981800  [32064/74412]\n",
      "loss: 0.952531  [38464/74412]\n",
      "loss: 0.729271  [44864/74412]\n",
      "loss: 0.924601  [51264/74412]\n",
      "loss: 0.610985  [57664/74412]\n",
      "loss: 1.027112  [64064/74412]\n",
      "loss: 0.748098  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 1.025558 \n",
      "\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "loss: 1.265910  [   64/74412]\n",
      "loss: 0.847915  [ 6464/74412]\n",
      "loss: 0.843423  [12864/74412]\n",
      "loss: 0.807087  [19264/74412]\n",
      "loss: 0.933591  [25664/74412]\n",
      "loss: 0.981017  [32064/74412]\n",
      "loss: 0.952060  [38464/74412]\n",
      "loss: 0.728924  [44864/74412]\n",
      "loss: 0.924834  [51264/74412]\n",
      "loss: 0.610237  [57664/74412]\n",
      "loss: 1.026776  [64064/74412]\n",
      "loss: 0.747660  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 1.025021 \n",
      "\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "loss: 1.263662  [   64/74412]\n",
      "loss: 0.848147  [ 6464/74412]\n",
      "loss: 0.842542  [12864/74412]\n",
      "loss: 0.806033  [19264/74412]\n",
      "loss: 0.933217  [25664/74412]\n",
      "loss: 0.980487  [32064/74412]\n",
      "loss: 0.951754  [38464/74412]\n",
      "loss: 0.727974  [44864/74412]\n",
      "loss: 0.923901  [51264/74412]\n",
      "loss: 0.609840  [57664/74412]\n",
      "loss: 1.026145  [64064/74412]\n",
      "loss: 0.748334  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 1.025249 \n",
      "\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "loss: 1.264441  [   64/74412]\n",
      "loss: 0.847017  [ 6464/74412]\n",
      "loss: 0.842634  [12864/74412]\n",
      "loss: 0.805968  [19264/74412]\n",
      "loss: 0.933136  [25664/74412]\n",
      "loss: 0.980024  [32064/74412]\n",
      "loss: 0.951097  [38464/74412]\n",
      "loss: 0.727724  [44864/74412]\n",
      "loss: 0.925197  [51264/74412]\n",
      "loss: 0.609734  [57664/74412]\n",
      "loss: 1.024630  [64064/74412]\n",
      "loss: 0.747778  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 1.024235 \n",
      "\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "loss: 1.263300  [   64/74412]\n",
      "loss: 0.846431  [ 6464/74412]\n",
      "loss: 0.843823  [12864/74412]\n",
      "loss: 0.805975  [19264/74412]\n",
      "loss: 0.932207  [25664/74412]\n",
      "loss: 0.979367  [32064/74412]\n",
      "loss: 0.951094  [38464/74412]\n",
      "loss: 0.728052  [44864/74412]\n",
      "loss: 0.923825  [51264/74412]\n",
      "loss: 0.608949  [57664/74412]\n",
      "loss: 1.023726  [64064/74412]\n",
      "loss: 0.747378  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 1.023864 \n",
      "\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "loss: 1.262629  [   64/74412]\n",
      "loss: 0.848152  [ 6464/74412]\n",
      "loss: 0.842183  [12864/74412]\n",
      "loss: 0.805200  [19264/74412]\n",
      "loss: 0.931324  [25664/74412]\n",
      "loss: 0.979218  [32064/74412]\n",
      "loss: 0.948124  [38464/74412]\n",
      "loss: 0.727047  [44864/74412]\n",
      "loss: 0.922036  [51264/74412]\n",
      "loss: 0.608647  [57664/74412]\n",
      "loss: 1.023185  [64064/74412]\n",
      "loss: 0.747572  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 1.023561 \n",
      "\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "loss: 1.261550  [   64/74412]\n",
      "loss: 0.848616  [ 6464/74412]\n",
      "loss: 0.841967  [12864/74412]\n",
      "loss: 0.804649  [19264/74412]\n",
      "loss: 0.930901  [25664/74412]\n",
      "loss: 0.978632  [32064/74412]\n",
      "loss: 0.947738  [38464/74412]\n",
      "loss: 0.726258  [44864/74412]\n",
      "loss: 0.922258  [51264/74412]\n",
      "loss: 0.608006  [57664/74412]\n",
      "loss: 1.022459  [64064/74412]\n",
      "loss: 0.747465  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 1.023439 \n",
      "\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "loss: 1.261352  [   64/74412]\n",
      "loss: 0.847730  [ 6464/74412]\n",
      "loss: 0.843217  [12864/74412]\n",
      "loss: 0.803870  [19264/74412]\n",
      "loss: 0.931283  [25664/74412]\n",
      "loss: 0.977798  [32064/74412]\n",
      "loss: 0.948707  [38464/74412]\n",
      "loss: 0.726730  [44864/74412]\n",
      "loss: 0.921744  [51264/74412]\n",
      "loss: 0.607627  [57664/74412]\n",
      "loss: 1.022450  [64064/74412]\n",
      "loss: 0.747303  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 1.023553 \n",
      "\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "loss: 1.259810  [   64/74412]\n",
      "loss: 0.847523  [ 6464/74412]\n",
      "loss: 0.841501  [12864/74412]\n",
      "loss: 0.803921  [19264/74412]\n",
      "loss: 0.931032  [25664/74412]\n",
      "loss: 0.976237  [32064/74412]\n",
      "loss: 0.945810  [38464/74412]\n",
      "loss: 0.725045  [44864/74412]\n",
      "loss: 0.921723  [51264/74412]\n",
      "loss: 0.606534  [57664/74412]\n",
      "loss: 1.022012  [64064/74412]\n",
      "loss: 0.746908  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 1.022422 \n",
      "\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "loss: 1.259320  [   64/74412]\n",
      "loss: 0.847386  [ 6464/74412]\n",
      "loss: 0.842394  [12864/74412]\n",
      "loss: 0.802928  [19264/74412]\n",
      "loss: 0.929445  [25664/74412]\n",
      "loss: 0.976517  [32064/74412]\n",
      "loss: 0.948696  [38464/74412]\n",
      "loss: 0.724068  [44864/74412]\n",
      "loss: 0.922736  [51264/74412]\n",
      "loss: 0.607041  [57664/74412]\n",
      "loss: 1.020143  [64064/74412]\n",
      "loss: 0.746108  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 1.021859 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "loss: 1.257819  [   64/74412]\n",
      "loss: 0.848318  [ 6464/74412]\n",
      "loss: 0.842645  [12864/74412]\n",
      "loss: 0.802915  [19264/74412]\n",
      "loss: 0.929271  [25664/74412]\n",
      "loss: 0.975752  [32064/74412]\n",
      "loss: 0.947579  [38464/74412]\n",
      "loss: 0.723820  [44864/74412]\n",
      "loss: 0.921313  [51264/74412]\n",
      "loss: 0.605659  [57664/74412]\n",
      "loss: 1.019268  [64064/74412]\n",
      "loss: 0.746835  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 1.021704 \n",
      "\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "loss: 1.257128  [   64/74412]\n",
      "loss: 0.848428  [ 6464/74412]\n",
      "loss: 0.842151  [12864/74412]\n",
      "loss: 0.803364  [19264/74412]\n",
      "loss: 0.928198  [25664/74412]\n",
      "loss: 0.975104  [32064/74412]\n",
      "loss: 0.947253  [38464/74412]\n",
      "loss: 0.724259  [44864/74412]\n",
      "loss: 0.921901  [51264/74412]\n",
      "loss: 0.605664  [57664/74412]\n",
      "loss: 1.018817  [64064/74412]\n",
      "loss: 0.746714  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 1.021259 \n",
      "\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "loss: 1.256129  [   64/74412]\n",
      "loss: 0.846224  [ 6464/74412]\n",
      "loss: 0.842409  [12864/74412]\n",
      "loss: 0.802623  [19264/74412]\n",
      "loss: 0.927036  [25664/74412]\n",
      "loss: 0.973533  [32064/74412]\n",
      "loss: 0.946815  [38464/74412]\n",
      "loss: 0.723657  [44864/74412]\n",
      "loss: 0.920541  [51264/74412]\n",
      "loss: 0.605857  [57664/74412]\n",
      "loss: 1.018755  [64064/74412]\n",
      "loss: 0.745434  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 1.020704 \n",
      "\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "loss: 1.255507  [   64/74412]\n",
      "loss: 0.847096  [ 6464/74412]\n",
      "loss: 0.842465  [12864/74412]\n",
      "loss: 0.801231  [19264/74412]\n",
      "loss: 0.925830  [25664/74412]\n",
      "loss: 0.973482  [32064/74412]\n",
      "loss: 0.947672  [38464/74412]\n",
      "loss: 0.723857  [44864/74412]\n",
      "loss: 0.920576  [51264/74412]\n",
      "loss: 0.605152  [57664/74412]\n",
      "loss: 1.018055  [64064/74412]\n",
      "loss: 0.746684  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 1.020189 \n",
      "\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "loss: 1.254300  [   64/74412]\n",
      "loss: 0.847157  [ 6464/74412]\n",
      "loss: 0.841146  [12864/74412]\n",
      "loss: 0.801076  [19264/74412]\n",
      "loss: 0.925841  [25664/74412]\n",
      "loss: 0.972589  [32064/74412]\n",
      "loss: 0.946734  [38464/74412]\n",
      "loss: 0.723363  [44864/74412]\n",
      "loss: 0.919624  [51264/74412]\n",
      "loss: 0.603621  [57664/74412]\n",
      "loss: 1.017312  [64064/74412]\n",
      "loss: 0.746412  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 1.019694 \n",
      "\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "loss: 1.253895  [   64/74412]\n",
      "loss: 0.846457  [ 6464/74412]\n",
      "loss: 0.841157  [12864/74412]\n",
      "loss: 0.799935  [19264/74412]\n",
      "loss: 0.925663  [25664/74412]\n",
      "loss: 0.972414  [32064/74412]\n",
      "loss: 0.946451  [38464/74412]\n",
      "loss: 0.722024  [44864/74412]\n",
      "loss: 0.918960  [51264/74412]\n",
      "loss: 0.604122  [57664/74412]\n",
      "loss: 1.016183  [64064/74412]\n",
      "loss: 0.746506  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 1.019446 \n",
      "\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "loss: 1.253408  [   64/74412]\n",
      "loss: 0.845995  [ 6464/74412]\n",
      "loss: 0.839997  [12864/74412]\n",
      "loss: 0.800313  [19264/74412]\n",
      "loss: 0.926292  [25664/74412]\n",
      "loss: 0.972074  [32064/74412]\n",
      "loss: 0.946539  [38464/74412]\n",
      "loss: 0.720906  [44864/74412]\n",
      "loss: 0.918208  [51264/74412]\n",
      "loss: 0.603104  [57664/74412]\n",
      "loss: 1.016247  [64064/74412]\n",
      "loss: 0.746726  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 1.018814 \n",
      "\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "loss: 1.252107  [   64/74412]\n",
      "loss: 0.846500  [ 6464/74412]\n",
      "loss: 0.839979  [12864/74412]\n",
      "loss: 0.800111  [19264/74412]\n",
      "loss: 0.925217  [25664/74412]\n",
      "loss: 0.971095  [32064/74412]\n",
      "loss: 0.946117  [38464/74412]\n",
      "loss: 0.720031  [44864/74412]\n",
      "loss: 0.917671  [51264/74412]\n",
      "loss: 0.603215  [57664/74412]\n",
      "loss: 1.015395  [64064/74412]\n",
      "loss: 0.746243  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 1.018328 \n",
      "\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "loss: 1.250510  [   64/74412]\n",
      "loss: 0.845801  [ 6464/74412]\n",
      "loss: 0.840032  [12864/74412]\n",
      "loss: 0.798485  [19264/74412]\n",
      "loss: 0.926031  [25664/74412]\n",
      "loss: 0.970517  [32064/74412]\n",
      "loss: 0.947006  [38464/74412]\n",
      "loss: 0.720515  [44864/74412]\n",
      "loss: 0.917721  [51264/74412]\n",
      "loss: 0.601772  [57664/74412]\n",
      "loss: 1.015130  [64064/74412]\n",
      "loss: 0.745562  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 1.018069 \n",
      "\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "loss: 1.249140  [   64/74412]\n",
      "loss: 0.846376  [ 6464/74412]\n",
      "loss: 0.839160  [12864/74412]\n",
      "loss: 0.798935  [19264/74412]\n",
      "loss: 0.925616  [25664/74412]\n",
      "loss: 0.970316  [32064/74412]\n",
      "loss: 0.947008  [38464/74412]\n",
      "loss: 0.718425  [44864/74412]\n",
      "loss: 0.916949  [51264/74412]\n",
      "loss: 0.601100  [57664/74412]\n",
      "loss: 1.014402  [64064/74412]\n",
      "loss: 0.745932  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 1.017919 \n",
      "\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "loss: 1.248497  [   64/74412]\n",
      "loss: 0.845841  [ 6464/74412]\n",
      "loss: 0.838027  [12864/74412]\n",
      "loss: 0.798915  [19264/74412]\n",
      "loss: 0.924969  [25664/74412]\n",
      "loss: 0.970046  [32064/74412]\n",
      "loss: 0.946511  [38464/74412]\n",
      "loss: 0.717863  [44864/74412]\n",
      "loss: 0.917165  [51264/74412]\n",
      "loss: 0.600583  [57664/74412]\n",
      "loss: 1.014346  [64064/74412]\n",
      "loss: 0.745093  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 1.017201 \n",
      "\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "loss: 1.246558  [   64/74412]\n",
      "loss: 0.844958  [ 6464/74412]\n",
      "loss: 0.839155  [12864/74412]\n",
      "loss: 0.797879  [19264/74412]\n",
      "loss: 0.924041  [25664/74412]\n",
      "loss: 0.968929  [32064/74412]\n",
      "loss: 0.944636  [38464/74412]\n",
      "loss: 0.719997  [44864/74412]\n",
      "loss: 0.916814  [51264/74412]\n",
      "loss: 0.600169  [57664/74412]\n",
      "loss: 1.013694  [64064/74412]\n",
      "loss: 0.743306  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 1.016954 \n",
      "\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "loss: 1.246306  [   64/74412]\n",
      "loss: 0.845807  [ 6464/74412]\n",
      "loss: 0.837835  [12864/74412]\n",
      "loss: 0.797932  [19264/74412]\n",
      "loss: 0.923746  [25664/74412]\n",
      "loss: 0.968122  [32064/74412]\n",
      "loss: 0.944339  [38464/74412]\n",
      "loss: 0.719265  [44864/74412]\n",
      "loss: 0.916580  [51264/74412]\n",
      "loss: 0.599462  [57664/74412]\n",
      "loss: 1.013334  [64064/74412]\n",
      "loss: 0.744292  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 1.016177 \n",
      "\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "loss: 1.245328  [   64/74412]\n",
      "loss: 0.844663  [ 6464/74412]\n",
      "loss: 0.838201  [12864/74412]\n",
      "loss: 0.797277  [19264/74412]\n",
      "loss: 0.924061  [25664/74412]\n",
      "loss: 0.967317  [32064/74412]\n",
      "loss: 0.945207  [38464/74412]\n",
      "loss: 0.719448  [44864/74412]\n",
      "loss: 0.915978  [51264/74412]\n",
      "loss: 0.599063  [57664/74412]\n",
      "loss: 1.012369  [64064/74412]\n",
      "loss: 0.743368  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 1.016340 \n",
      "\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "loss: 1.245314  [   64/74412]\n",
      "loss: 0.844281  [ 6464/74412]\n",
      "loss: 0.837540  [12864/74412]\n",
      "loss: 0.797007  [19264/74412]\n",
      "loss: 0.923279  [25664/74412]\n",
      "loss: 0.967119  [32064/74412]\n",
      "loss: 0.943474  [38464/74412]\n",
      "loss: 0.717350  [44864/74412]\n",
      "loss: 0.915492  [51264/74412]\n",
      "loss: 0.599342  [57664/74412]\n",
      "loss: 1.012118  [64064/74412]\n",
      "loss: 0.743819  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 1.016127 \n",
      "\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "loss: 1.245063  [   64/74412]\n",
      "loss: 0.842830  [ 6464/74412]\n",
      "loss: 0.837552  [12864/74412]\n",
      "loss: 0.796735  [19264/74412]\n",
      "loss: 0.922547  [25664/74412]\n",
      "loss: 0.966134  [32064/74412]\n",
      "loss: 0.944819  [38464/74412]\n",
      "loss: 0.716606  [44864/74412]\n",
      "loss: 0.914407  [51264/74412]\n",
      "loss: 0.597937  [57664/74412]\n",
      "loss: 1.012033  [64064/74412]\n",
      "loss: 0.743875  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 1.015780 \n",
      "\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "loss: 1.244146  [   64/74412]\n",
      "loss: 0.842597  [ 6464/74412]\n",
      "loss: 0.838120  [12864/74412]\n",
      "loss: 0.796016  [19264/74412]\n",
      "loss: 0.922975  [25664/74412]\n",
      "loss: 0.966298  [32064/74412]\n",
      "loss: 0.944441  [38464/74412]\n",
      "loss: 0.715609  [44864/74412]\n",
      "loss: 0.914399  [51264/74412]\n",
      "loss: 0.598012  [57664/74412]\n",
      "loss: 1.013466  [64064/74412]\n",
      "loss: 0.743493  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 1.014839 \n",
      "\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "loss: 1.242087  [   64/74412]\n",
      "loss: 0.842825  [ 6464/74412]\n",
      "loss: 0.837464  [12864/74412]\n",
      "loss: 0.795847  [19264/74412]\n",
      "loss: 0.921780  [25664/74412]\n",
      "loss: 0.965144  [32064/74412]\n",
      "loss: 0.943802  [38464/74412]\n",
      "loss: 0.716929  [44864/74412]\n",
      "loss: 0.913305  [51264/74412]\n",
      "loss: 0.598663  [57664/74412]\n",
      "loss: 1.011460  [64064/74412]\n",
      "loss: 0.743158  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.014137 \n",
      "\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "loss: 1.242053  [   64/74412]\n",
      "loss: 0.842437  [ 6464/74412]\n",
      "loss: 0.836226  [12864/74412]\n",
      "loss: 0.794771  [19264/74412]\n",
      "loss: 0.921629  [25664/74412]\n",
      "loss: 0.964305  [32064/74412]\n",
      "loss: 0.943402  [38464/74412]\n",
      "loss: 0.716255  [44864/74412]\n",
      "loss: 0.912347  [51264/74412]\n",
      "loss: 0.598493  [57664/74412]\n",
      "loss: 1.011002  [64064/74412]\n",
      "loss: 0.743099  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.013702 \n",
      "\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "loss: 1.240113  [   64/74412]\n",
      "loss: 0.842146  [ 6464/74412]\n",
      "loss: 0.837732  [12864/74412]\n",
      "loss: 0.794793  [19264/74412]\n",
      "loss: 0.921762  [25664/74412]\n",
      "loss: 0.964331  [32064/74412]\n",
      "loss: 0.943642  [38464/74412]\n",
      "loss: 0.714830  [44864/74412]\n",
      "loss: 0.909135  [51264/74412]\n",
      "loss: 0.598637  [57664/74412]\n",
      "loss: 1.010155  [64064/74412]\n",
      "loss: 0.743294  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.013985 \n",
      "\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "loss: 1.240317  [   64/74412]\n",
      "loss: 0.842102  [ 6464/74412]\n",
      "loss: 0.836896  [12864/74412]\n",
      "loss: 0.794095  [19264/74412]\n",
      "loss: 0.920813  [25664/74412]\n",
      "loss: 0.963405  [32064/74412]\n",
      "loss: 0.943421  [38464/74412]\n",
      "loss: 0.713875  [44864/74412]\n",
      "loss: 0.910159  [51264/74412]\n",
      "loss: 0.597398  [57664/74412]\n",
      "loss: 1.010839  [64064/74412]\n",
      "loss: 0.742558  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.013043 \n",
      "\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "loss: 1.237617  [   64/74412]\n",
      "loss: 0.841052  [ 6464/74412]\n",
      "loss: 0.836474  [12864/74412]\n",
      "loss: 0.794243  [19264/74412]\n",
      "loss: 0.920341  [25664/74412]\n",
      "loss: 0.962889  [32064/74412]\n",
      "loss: 0.942907  [38464/74412]\n",
      "loss: 0.713544  [44864/74412]\n",
      "loss: 0.912254  [51264/74412]\n",
      "loss: 0.597563  [57664/74412]\n",
      "loss: 1.007328  [64064/74412]\n",
      "loss: 0.742716  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.012504 \n",
      "\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "loss: 1.237676  [   64/74412]\n",
      "loss: 0.839745  [ 6464/74412]\n",
      "loss: 0.836592  [12864/74412]\n",
      "loss: 0.793116  [19264/74412]\n",
      "loss: 0.920698  [25664/74412]\n",
      "loss: 0.962549  [32064/74412]\n",
      "loss: 0.943204  [38464/74412]\n",
      "loss: 0.713632  [44864/74412]\n",
      "loss: 0.912026  [51264/74412]\n",
      "loss: 0.597117  [57664/74412]\n",
      "loss: 1.008033  [64064/74412]\n",
      "loss: 0.742128  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.012493 \n",
      "\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "loss: 1.238435  [   64/74412]\n",
      "loss: 0.840111  [ 6464/74412]\n",
      "loss: 0.835701  [12864/74412]\n",
      "loss: 0.792738  [19264/74412]\n",
      "loss: 0.919747  [25664/74412]\n",
      "loss: 0.961855  [32064/74412]\n",
      "loss: 0.941507  [38464/74412]\n",
      "loss: 0.712831  [44864/74412]\n",
      "loss: 0.907459  [51264/74412]\n",
      "loss: 0.596926  [57664/74412]\n",
      "loss: 1.008504  [64064/74412]\n",
      "loss: 0.742430  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.012078 \n",
      "\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "loss: 1.235202  [   64/74412]\n",
      "loss: 0.840028  [ 6464/74412]\n",
      "loss: 0.835429  [12864/74412]\n",
      "loss: 0.792757  [19264/74412]\n",
      "loss: 0.919261  [25664/74412]\n",
      "loss: 0.961329  [32064/74412]\n",
      "loss: 0.942537  [38464/74412]\n",
      "loss: 0.711514  [44864/74412]\n",
      "loss: 0.907763  [51264/74412]\n",
      "loss: 0.596074  [57664/74412]\n",
      "loss: 1.007479  [64064/74412]\n",
      "loss: 0.742202  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.011696 \n",
      "\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "loss: 1.236680  [   64/74412]\n",
      "loss: 0.838828  [ 6464/74412]\n",
      "loss: 0.835343  [12864/74412]\n",
      "loss: 0.792504  [19264/74412]\n",
      "loss: 0.919600  [25664/74412]\n",
      "loss: 0.960540  [32064/74412]\n",
      "loss: 0.941393  [38464/74412]\n",
      "loss: 0.711985  [44864/74412]\n",
      "loss: 0.909505  [51264/74412]\n",
      "loss: 0.595111  [57664/74412]\n",
      "loss: 1.007111  [64064/74412]\n",
      "loss: 0.741217  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.010623 \n",
      "\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "loss: 1.233892  [   64/74412]\n",
      "loss: 0.839016  [ 6464/74412]\n",
      "loss: 0.834884  [12864/74412]\n",
      "loss: 0.791923  [19264/74412]\n",
      "loss: 0.918796  [25664/74412]\n",
      "loss: 0.960000  [32064/74412]\n",
      "loss: 0.940961  [38464/74412]\n",
      "loss: 0.710115  [44864/74412]\n",
      "loss: 0.906040  [51264/74412]\n",
      "loss: 0.594748  [57664/74412]\n",
      "loss: 1.006265  [64064/74412]\n",
      "loss: 0.740629  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1.010429 \n",
      "\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "loss: 1.233709  [   64/74412]\n",
      "loss: 0.837990  [ 6464/74412]\n",
      "loss: 0.833684  [12864/74412]\n",
      "loss: 0.791431  [19264/74412]\n",
      "loss: 0.918664  [25664/74412]\n",
      "loss: 0.959649  [32064/74412]\n",
      "loss: 0.941293  [38464/74412]\n",
      "loss: 0.709513  [44864/74412]\n",
      "loss: 0.906002  [51264/74412]\n",
      "loss: 0.594427  [57664/74412]\n",
      "loss: 1.007142  [64064/74412]\n",
      "loss: 0.740365  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1.010619 \n",
      "\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "loss: 1.233558  [   64/74412]\n",
      "loss: 0.837192  [ 6464/74412]\n",
      "loss: 0.832846  [12864/74412]\n",
      "loss: 0.791053  [19264/74412]\n",
      "loss: 0.918156  [25664/74412]\n",
      "loss: 0.959062  [32064/74412]\n",
      "loss: 0.940464  [38464/74412]\n",
      "loss: 0.708927  [44864/74412]\n",
      "loss: 0.905470  [51264/74412]\n",
      "loss: 0.594510  [57664/74412]\n",
      "loss: 1.006412  [64064/74412]\n",
      "loss: 0.740533  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1.009558 \n",
      "\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "loss: 1.233471  [   64/74412]\n",
      "loss: 0.837143  [ 6464/74412]\n",
      "loss: 0.834876  [12864/74412]\n",
      "loss: 0.789596  [19264/74412]\n",
      "loss: 0.917221  [25664/74412]\n",
      "loss: 0.959229  [32064/74412]\n",
      "loss: 0.938941  [38464/74412]\n",
      "loss: 0.708313  [44864/74412]\n",
      "loss: 0.904917  [51264/74412]\n",
      "loss: 0.594067  [57664/74412]\n",
      "loss: 1.005281  [64064/74412]\n",
      "loss: 0.739559  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1.009696 \n",
      "\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "loss: 1.232137  [   64/74412]\n",
      "loss: 0.837699  [ 6464/74412]\n",
      "loss: 0.833518  [12864/74412]\n",
      "loss: 0.789983  [19264/74412]\n",
      "loss: 0.917451  [25664/74412]\n",
      "loss: 0.958265  [32064/74412]\n",
      "loss: 0.938414  [38464/74412]\n",
      "loss: 0.706926  [44864/74412]\n",
      "loss: 0.903937  [51264/74412]\n",
      "loss: 0.593283  [57664/74412]\n",
      "loss: 1.005483  [64064/74412]\n",
      "loss: 0.739188  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1.008607 \n",
      "\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "loss: 1.230447  [   64/74412]\n",
      "loss: 0.838734  [ 6464/74412]\n",
      "loss: 0.833542  [12864/74412]\n",
      "loss: 0.789283  [19264/74412]\n",
      "loss: 0.917227  [25664/74412]\n",
      "loss: 0.957912  [32064/74412]\n",
      "loss: 0.938354  [38464/74412]\n",
      "loss: 0.708119  [44864/74412]\n",
      "loss: 0.907229  [51264/74412]\n",
      "loss: 0.592866  [57664/74412]\n",
      "loss: 1.005104  [64064/74412]\n",
      "loss: 0.739043  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1.008328 \n",
      "\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "loss: 1.231829  [   64/74412]\n",
      "loss: 0.837966  [ 6464/74412]\n",
      "loss: 0.833449  [12864/74412]\n",
      "loss: 0.788764  [19264/74412]\n",
      "loss: 0.916691  [25664/74412]\n",
      "loss: 0.957873  [32064/74412]\n",
      "loss: 0.938783  [38464/74412]\n",
      "loss: 0.708041  [44864/74412]\n",
      "loss: 0.903365  [51264/74412]\n",
      "loss: 0.592035  [57664/74412]\n",
      "loss: 1.004961  [64064/74412]\n",
      "loss: 0.738916  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1.008261 \n",
      "\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "loss: 1.229910  [   64/74412]\n",
      "loss: 0.838003  [ 6464/74412]\n",
      "loss: 0.832296  [12864/74412]\n",
      "loss: 0.789164  [19264/74412]\n",
      "loss: 0.916344  [25664/74412]\n",
      "loss: 0.956997  [32064/74412]\n",
      "loss: 0.937601  [38464/74412]\n",
      "loss: 0.709060  [44864/74412]\n",
      "loss: 0.905985  [51264/74412]\n",
      "loss: 0.591947  [57664/74412]\n",
      "loss: 1.003218  [64064/74412]\n",
      "loss: 0.738888  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1.007421 \n",
      "\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "loss: 1.229405  [   64/74412]\n",
      "loss: 0.838028  [ 6464/74412]\n",
      "loss: 0.832972  [12864/74412]\n",
      "loss: 0.787914  [19264/74412]\n",
      "loss: 0.915869  [25664/74412]\n",
      "loss: 0.957156  [32064/74412]\n",
      "loss: 0.937548  [38464/74412]\n",
      "loss: 0.708129  [44864/74412]\n",
      "loss: 0.902230  [51264/74412]\n",
      "loss: 0.591196  [57664/74412]\n",
      "loss: 1.002820  [64064/74412]\n",
      "loss: 0.737747  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1.007352 \n",
      "\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "loss: 1.228293  [   64/74412]\n",
      "loss: 0.837002  [ 6464/74412]\n",
      "loss: 0.834350  [12864/74412]\n",
      "loss: 0.787592  [19264/74412]\n",
      "loss: 0.915705  [25664/74412]\n",
      "loss: 0.955323  [32064/74412]\n",
      "loss: 0.936566  [38464/74412]\n",
      "loss: 0.707318  [44864/74412]\n",
      "loss: 0.904938  [51264/74412]\n",
      "loss: 0.591092  [57664/74412]\n",
      "loss: 1.002518  [64064/74412]\n",
      "loss: 0.737429  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1.007021 \n",
      "\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "loss: 1.227694  [   64/74412]\n",
      "loss: 0.836694  [ 6464/74412]\n",
      "loss: 0.834133  [12864/74412]\n",
      "loss: 0.787314  [19264/74412]\n",
      "loss: 0.914766  [25664/74412]\n",
      "loss: 0.956057  [32064/74412]\n",
      "loss: 0.936852  [38464/74412]\n",
      "loss: 0.706713  [44864/74412]\n",
      "loss: 0.905207  [51264/74412]\n",
      "loss: 0.590639  [57664/74412]\n",
      "loss: 1.002171  [64064/74412]\n",
      "loss: 0.737176  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 1.006667 \n",
      "\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "loss: 1.228268  [   64/74412]\n",
      "loss: 0.837123  [ 6464/74412]\n",
      "loss: 0.833911  [12864/74412]\n",
      "loss: 0.785833  [19264/74412]\n",
      "loss: 0.915259  [25664/74412]\n",
      "loss: 0.954353  [32064/74412]\n",
      "loss: 0.936439  [38464/74412]\n",
      "loss: 0.706317  [44864/74412]\n",
      "loss: 0.901388  [51264/74412]\n",
      "loss: 0.590302  [57664/74412]\n",
      "loss: 1.000736  [64064/74412]\n",
      "loss: 0.736511  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 1.006070 \n",
      "\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "loss: 1.226321  [   64/74412]\n",
      "loss: 0.837168  [ 6464/74412]\n",
      "loss: 0.831953  [12864/74412]\n",
      "loss: 0.785560  [19264/74412]\n",
      "loss: 0.915787  [25664/74412]\n",
      "loss: 0.954289  [32064/74412]\n",
      "loss: 0.935668  [38464/74412]\n",
      "loss: 0.706499  [44864/74412]\n",
      "loss: 0.903840  [51264/74412]\n",
      "loss: 0.589847  [57664/74412]\n",
      "loss: 1.001212  [64064/74412]\n",
      "loss: 0.736093  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 1.005654 \n",
      "\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "loss: 1.226350  [   64/74412]\n",
      "loss: 0.835487  [ 6464/74412]\n",
      "loss: 0.831811  [12864/74412]\n",
      "loss: 0.785721  [19264/74412]\n",
      "loss: 0.913676  [25664/74412]\n",
      "loss: 0.953888  [32064/74412]\n",
      "loss: 0.935181  [38464/74412]\n",
      "loss: 0.704937  [44864/74412]\n",
      "loss: 0.898985  [51264/74412]\n",
      "loss: 0.588891  [57664/74412]\n",
      "loss: 1.000772  [64064/74412]\n",
      "loss: 0.736338  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 1.006290 \n",
      "\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "loss: 1.227201  [   64/74412]\n",
      "loss: 0.835478  [ 6464/74412]\n",
      "loss: 0.831028  [12864/74412]\n",
      "loss: 0.786157  [19264/74412]\n",
      "loss: 0.913608  [25664/74412]\n",
      "loss: 0.952791  [32064/74412]\n",
      "loss: 0.933715  [38464/74412]\n",
      "loss: 0.705505  [44864/74412]\n",
      "loss: 0.898939  [51264/74412]\n",
      "loss: 0.588379  [57664/74412]\n",
      "loss: 0.999836  [64064/74412]\n",
      "loss: 0.735850  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 1.005424 \n",
      "\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "loss: 1.225493  [   64/74412]\n",
      "loss: 0.835308  [ 6464/74412]\n",
      "loss: 0.832654  [12864/74412]\n",
      "loss: 0.785423  [19264/74412]\n",
      "loss: 0.912759  [25664/74412]\n",
      "loss: 0.952066  [32064/74412]\n",
      "loss: 0.933712  [38464/74412]\n",
      "loss: 0.705464  [44864/74412]\n",
      "loss: 0.899331  [51264/74412]\n",
      "loss: 0.587966  [57664/74412]\n",
      "loss: 0.999252  [64064/74412]\n",
      "loss: 0.735815  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 1.004734 \n",
      "\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "loss: 1.224697  [   64/74412]\n",
      "loss: 0.835699  [ 6464/74412]\n",
      "loss: 0.832326  [12864/74412]\n",
      "loss: 0.785295  [19264/74412]\n",
      "loss: 0.912580  [25664/74412]\n",
      "loss: 0.951851  [32064/74412]\n",
      "loss: 0.933958  [38464/74412]\n",
      "loss: 0.703963  [44864/74412]\n",
      "loss: 0.901050  [51264/74412]\n",
      "loss: 0.587695  [57664/74412]\n",
      "loss: 0.996955  [64064/74412]\n",
      "loss: 0.735293  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 1.004072 \n",
      "\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "loss: 1.224204  [   64/74412]\n",
      "loss: 0.835081  [ 6464/74412]\n",
      "loss: 0.831881  [12864/74412]\n",
      "loss: 0.784479  [19264/74412]\n",
      "loss: 0.912894  [25664/74412]\n",
      "loss: 0.950973  [32064/74412]\n",
      "loss: 0.932514  [38464/74412]\n",
      "loss: 0.703294  [44864/74412]\n",
      "loss: 0.896894  [51264/74412]\n",
      "loss: 0.586982  [57664/74412]\n",
      "loss: 0.997045  [64064/74412]\n",
      "loss: 0.734927  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 1.004922 \n",
      "\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "loss: 1.224059  [   64/74412]\n",
      "loss: 0.834446  [ 6464/74412]\n",
      "loss: 0.830932  [12864/74412]\n",
      "loss: 0.784758  [19264/74412]\n",
      "loss: 0.912122  [25664/74412]\n",
      "loss: 0.950160  [32064/74412]\n",
      "loss: 0.932207  [38464/74412]\n",
      "loss: 0.704020  [44864/74412]\n",
      "loss: 0.896893  [51264/74412]\n",
      "loss: 0.586730  [57664/74412]\n",
      "loss: 0.997222  [64064/74412]\n",
      "loss: 0.734560  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 1.003415 \n",
      "\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "loss: 1.223102  [   64/74412]\n",
      "loss: 0.834691  [ 6464/74412]\n",
      "loss: 0.830123  [12864/74412]\n",
      "loss: 0.784175  [19264/74412]\n",
      "loss: 0.911902  [25664/74412]\n",
      "loss: 0.949374  [32064/74412]\n",
      "loss: 0.932004  [38464/74412]\n",
      "loss: 0.701933  [44864/74412]\n",
      "loss: 0.896672  [51264/74412]\n",
      "loss: 0.585854  [57664/74412]\n",
      "loss: 0.996795  [64064/74412]\n",
      "loss: 0.734250  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 1.003520 \n",
      "\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "loss: 1.223339  [   64/74412]\n",
      "loss: 0.833830  [ 6464/74412]\n",
      "loss: 0.829079  [12864/74412]\n",
      "loss: 0.783596  [19264/74412]\n",
      "loss: 0.911760  [25664/74412]\n",
      "loss: 0.948585  [32064/74412]\n",
      "loss: 0.931771  [38464/74412]\n",
      "loss: 0.700966  [44864/74412]\n",
      "loss: 0.895737  [51264/74412]\n",
      "loss: 0.585633  [57664/74412]\n",
      "loss: 0.996755  [64064/74412]\n",
      "loss: 0.734587  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 1.001728 \n",
      "\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "loss: 1.220749  [   64/74412]\n",
      "loss: 0.833414  [ 6464/74412]\n",
      "loss: 0.828632  [12864/74412]\n",
      "loss: 0.783417  [19264/74412]\n",
      "loss: 0.911096  [25664/74412]\n",
      "loss: 0.948115  [32064/74412]\n",
      "loss: 0.931103  [38464/74412]\n",
      "loss: 0.700883  [44864/74412]\n",
      "loss: 0.898698  [51264/74412]\n",
      "loss: 0.584705  [57664/74412]\n",
      "loss: 0.994794  [64064/74412]\n",
      "loss: 0.733754  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 1.002046 \n",
      "\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "loss: 1.220900  [   64/74412]\n",
      "loss: 0.834259  [ 6464/74412]\n",
      "loss: 0.829090  [12864/74412]\n",
      "loss: 0.783313  [19264/74412]\n",
      "loss: 0.911801  [25664/74412]\n",
      "loss: 0.947996  [32064/74412]\n",
      "loss: 0.929926  [38464/74412]\n",
      "loss: 0.701359  [44864/74412]\n",
      "loss: 0.898639  [51264/74412]\n",
      "loss: 0.584210  [57664/74412]\n",
      "loss: 0.994639  [64064/74412]\n",
      "loss: 0.733614  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 1.003557 \n",
      "\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "loss: 1.221516  [   64/74412]\n",
      "loss: 0.832751  [ 6464/74412]\n",
      "loss: 0.828552  [12864/74412]\n",
      "loss: 0.783256  [19264/74412]\n",
      "loss: 0.910749  [25664/74412]\n",
      "loss: 0.948042  [32064/74412]\n",
      "loss: 0.930175  [38464/74412]\n",
      "loss: 0.699579  [44864/74412]\n",
      "loss: 0.898173  [51264/74412]\n",
      "loss: 0.584255  [57664/74412]\n",
      "loss: 0.995494  [64064/74412]\n",
      "loss: 0.733912  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 1.001049 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "loss: 1.219282  [   64/74412]\n",
      "loss: 0.833156  [ 6464/74412]\n",
      "loss: 0.828597  [12864/74412]\n",
      "loss: 0.782547  [19264/74412]\n",
      "loss: 0.910308  [25664/74412]\n",
      "loss: 0.946994  [32064/74412]\n",
      "loss: 0.930165  [38464/74412]\n",
      "loss: 0.700582  [44864/74412]\n",
      "loss: 0.898492  [51264/74412]\n",
      "loss: 0.583462  [57664/74412]\n",
      "loss: 0.994474  [64064/74412]\n",
      "loss: 0.732712  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 1.000739 \n",
      "\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "loss: 1.218604  [   64/74412]\n",
      "loss: 0.830833  [ 6464/74412]\n",
      "loss: 0.827535  [12864/74412]\n",
      "loss: 0.781662  [19264/74412]\n",
      "loss: 0.908524  [25664/74412]\n",
      "loss: 0.946663  [32064/74412]\n",
      "loss: 0.930063  [38464/74412]\n",
      "loss: 0.700270  [44864/74412]\n",
      "loss: 0.897631  [51264/74412]\n",
      "loss: 0.583059  [57664/74412]\n",
      "loss: 0.993373  [64064/74412]\n",
      "loss: 0.732773  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 1.000390 \n",
      "\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "loss: 1.217577  [   64/74412]\n",
      "loss: 0.832123  [ 6464/74412]\n",
      "loss: 0.827554  [12864/74412]\n",
      "loss: 0.781606  [19264/74412]\n",
      "loss: 0.909282  [25664/74412]\n",
      "loss: 0.946436  [32064/74412]\n",
      "loss: 0.930325  [38464/74412]\n",
      "loss: 0.700471  [44864/74412]\n",
      "loss: 0.895748  [51264/74412]\n",
      "loss: 0.582931  [57664/74412]\n",
      "loss: 0.992893  [64064/74412]\n",
      "loss: 0.732614  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 1.001736 \n",
      "\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "loss: 1.217846  [   64/74412]\n",
      "loss: 0.832117  [ 6464/74412]\n",
      "loss: 0.827219  [12864/74412]\n",
      "loss: 0.780855  [19264/74412]\n",
      "loss: 0.908194  [25664/74412]\n",
      "loss: 0.944995  [32064/74412]\n",
      "loss: 0.929211  [38464/74412]\n",
      "loss: 0.699350  [44864/74412]\n",
      "loss: 0.896166  [51264/74412]\n",
      "loss: 0.582542  [57664/74412]\n",
      "loss: 0.993478  [64064/74412]\n",
      "loss: 0.733429  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 1.000659 \n",
      "\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "loss: 1.217096  [   64/74412]\n",
      "loss: 0.830096  [ 6464/74412]\n",
      "loss: 0.826991  [12864/74412]\n",
      "loss: 0.779866  [19264/74412]\n",
      "loss: 0.908219  [25664/74412]\n",
      "loss: 0.944774  [32064/74412]\n",
      "loss: 0.928447  [38464/74412]\n",
      "loss: 0.698393  [44864/74412]\n",
      "loss: 0.895383  [51264/74412]\n",
      "loss: 0.581465  [57664/74412]\n",
      "loss: 0.991956  [64064/74412]\n",
      "loss: 0.733116  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 1.000765 \n",
      "\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "loss: 1.217180  [   64/74412]\n",
      "loss: 0.830950  [ 6464/74412]\n",
      "loss: 0.827021  [12864/74412]\n",
      "loss: 0.779855  [19264/74412]\n",
      "loss: 0.907479  [25664/74412]\n",
      "loss: 0.945220  [32064/74412]\n",
      "loss: 0.929021  [38464/74412]\n",
      "loss: 0.698679  [44864/74412]\n",
      "loss: 0.895181  [51264/74412]\n",
      "loss: 0.581529  [57664/74412]\n",
      "loss: 0.992395  [64064/74412]\n",
      "loss: 0.733208  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.999702 \n",
      "\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "loss: 1.214338  [   64/74412]\n",
      "loss: 0.830436  [ 6464/74412]\n",
      "loss: 0.827363  [12864/74412]\n",
      "loss: 0.779575  [19264/74412]\n",
      "loss: 0.905973  [25664/74412]\n",
      "loss: 0.944088  [32064/74412]\n",
      "loss: 0.928020  [38464/74412]\n",
      "loss: 0.698365  [44864/74412]\n",
      "loss: 0.894626  [51264/74412]\n",
      "loss: 0.582116  [57664/74412]\n",
      "loss: 0.991955  [64064/74412]\n",
      "loss: 0.731700  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.999780 \n",
      "\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "loss: 1.214714  [   64/74412]\n",
      "loss: 0.829000  [ 6464/74412]\n",
      "loss: 0.826914  [12864/74412]\n",
      "loss: 0.778836  [19264/74412]\n",
      "loss: 0.904655  [25664/74412]\n",
      "loss: 0.944060  [32064/74412]\n",
      "loss: 0.927577  [38464/74412]\n",
      "loss: 0.697598  [44864/74412]\n",
      "loss: 0.893814  [51264/74412]\n",
      "loss: 0.580940  [57664/74412]\n",
      "loss: 0.990860  [64064/74412]\n",
      "loss: 0.731807  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 1.000054 \n",
      "\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "loss: 1.215862  [   64/74412]\n",
      "loss: 0.829412  [ 6464/74412]\n",
      "loss: 0.823893  [12864/74412]\n",
      "loss: 0.779745  [19264/74412]\n",
      "loss: 0.905168  [25664/74412]\n",
      "loss: 0.943000  [32064/74412]\n",
      "loss: 0.927355  [38464/74412]\n",
      "loss: 0.697369  [44864/74412]\n",
      "loss: 0.893386  [51264/74412]\n",
      "loss: 0.580298  [57664/74412]\n",
      "loss: 0.989670  [64064/74412]\n",
      "loss: 0.732801  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.998944 \n",
      "\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "loss: 1.215460  [   64/74412]\n",
      "loss: 0.829794  [ 6464/74412]\n",
      "loss: 0.824335  [12864/74412]\n",
      "loss: 0.778481  [19264/74412]\n",
      "loss: 0.903553  [25664/74412]\n",
      "loss: 0.941988  [32064/74412]\n",
      "loss: 0.927312  [38464/74412]\n",
      "loss: 0.695966  [44864/74412]\n",
      "loss: 0.891966  [51264/74412]\n",
      "loss: 0.579733  [57664/74412]\n",
      "loss: 0.989714  [64064/74412]\n",
      "loss: 0.732377  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.999440 \n",
      "\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "loss: 1.215191  [   64/74412]\n",
      "loss: 0.829501  [ 6464/74412]\n",
      "loss: 0.825158  [12864/74412]\n",
      "loss: 0.777669  [19264/74412]\n",
      "loss: 0.903550  [25664/74412]\n",
      "loss: 0.942159  [32064/74412]\n",
      "loss: 0.928584  [38464/74412]\n",
      "loss: 0.695939  [44864/74412]\n",
      "loss: 0.888817  [51264/74412]\n",
      "loss: 0.578843  [57664/74412]\n",
      "loss: 0.989251  [64064/74412]\n",
      "loss: 0.732727  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.998572 \n",
      "\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "loss: 1.214045  [   64/74412]\n",
      "loss: 0.829372  [ 6464/74412]\n",
      "loss: 0.822997  [12864/74412]\n",
      "loss: 0.778300  [19264/74412]\n",
      "loss: 0.903357  [25664/74412]\n",
      "loss: 0.941996  [32064/74412]\n",
      "loss: 0.928643  [38464/74412]\n",
      "loss: 0.696128  [44864/74412]\n",
      "loss: 0.887640  [51264/74412]\n",
      "loss: 0.578673  [57664/74412]\n",
      "loss: 0.990212  [64064/74412]\n",
      "loss: 0.732449  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.997512 \n",
      "\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "loss: 1.212772  [   64/74412]\n",
      "loss: 0.828928  [ 6464/74412]\n",
      "loss: 0.824808  [12864/74412]\n",
      "loss: 0.777614  [19264/74412]\n",
      "loss: 0.902999  [25664/74412]\n",
      "loss: 0.941489  [32064/74412]\n",
      "loss: 0.927520  [38464/74412]\n",
      "loss: 0.693939  [44864/74412]\n",
      "loss: 0.887565  [51264/74412]\n",
      "loss: 0.577955  [57664/74412]\n",
      "loss: 0.986919  [64064/74412]\n",
      "loss: 0.732386  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.997158 \n",
      "\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "loss: 1.213414  [   64/74412]\n",
      "loss: 0.829090  [ 6464/74412]\n",
      "loss: 0.822508  [12864/74412]\n",
      "loss: 0.778489  [19264/74412]\n",
      "loss: 0.902270  [25664/74412]\n",
      "loss: 0.940790  [32064/74412]\n",
      "loss: 0.926578  [38464/74412]\n",
      "loss: 0.694866  [44864/74412]\n",
      "loss: 0.890494  [51264/74412]\n",
      "loss: 0.577507  [57664/74412]\n",
      "loss: 0.986764  [64064/74412]\n",
      "loss: 0.732158  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.996073 \n",
      "\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "loss: 1.211733  [   64/74412]\n",
      "loss: 0.828809  [ 6464/74412]\n",
      "loss: 0.822017  [12864/74412]\n",
      "loss: 0.777998  [19264/74412]\n",
      "loss: 0.902154  [25664/74412]\n",
      "loss: 0.940765  [32064/74412]\n",
      "loss: 0.927497  [38464/74412]\n",
      "loss: 0.693853  [44864/74412]\n",
      "loss: 0.885467  [51264/74412]\n",
      "loss: 0.577626  [57664/74412]\n",
      "loss: 0.986076  [64064/74412]\n",
      "loss: 0.731933  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.996350 \n",
      "\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "loss: 1.211784  [   64/74412]\n",
      "loss: 0.828398  [ 6464/74412]\n",
      "loss: 0.823752  [12864/74412]\n",
      "loss: 0.777503  [19264/74412]\n",
      "loss: 0.902112  [25664/74412]\n",
      "loss: 0.940920  [32064/74412]\n",
      "loss: 0.926469  [38464/74412]\n",
      "loss: 0.694253  [44864/74412]\n",
      "loss: 0.885802  [51264/74412]\n",
      "loss: 0.576309  [57664/74412]\n",
      "loss: 0.987225  [64064/74412]\n",
      "loss: 0.730617  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.996065 \n",
      "\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "loss: 1.212001  [   64/74412]\n",
      "loss: 0.827505  [ 6464/74412]\n",
      "loss: 0.821952  [12864/74412]\n",
      "loss: 0.776916  [19264/74412]\n",
      "loss: 0.901047  [25664/74412]\n",
      "loss: 0.940872  [32064/74412]\n",
      "loss: 0.927611  [38464/74412]\n",
      "loss: 0.694038  [44864/74412]\n",
      "loss: 0.885148  [51264/74412]\n",
      "loss: 0.575567  [57664/74412]\n",
      "loss: 0.987591  [64064/74412]\n",
      "loss: 0.731148  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.996464 \n",
      "\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "loss: 1.212274  [   64/74412]\n",
      "loss: 0.827603  [ 6464/74412]\n",
      "loss: 0.823119  [12864/74412]\n",
      "loss: 0.778340  [19264/74412]\n",
      "loss: 0.900831  [25664/74412]\n",
      "loss: 0.940527  [32064/74412]\n",
      "loss: 0.926432  [38464/74412]\n",
      "loss: 0.692739  [44864/74412]\n",
      "loss: 0.885245  [51264/74412]\n",
      "loss: 0.575861  [57664/74412]\n",
      "loss: 0.986593  [64064/74412]\n",
      "loss: 0.730956  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.995993 \n",
      "\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "loss: 1.211366  [   64/74412]\n",
      "loss: 0.827738  [ 6464/74412]\n",
      "loss: 0.823207  [12864/74412]\n",
      "loss: 0.777896  [19264/74412]\n",
      "loss: 0.899396  [25664/74412]\n",
      "loss: 0.939585  [32064/74412]\n",
      "loss: 0.926062  [38464/74412]\n",
      "loss: 0.692927  [44864/74412]\n",
      "loss: 0.884411  [51264/74412]\n",
      "loss: 0.575318  [57664/74412]\n",
      "loss: 0.985372  [64064/74412]\n",
      "loss: 0.731051  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.995015 \n",
      "\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "loss: 1.210868  [   64/74412]\n",
      "loss: 0.827264  [ 6464/74412]\n",
      "loss: 0.822831  [12864/74412]\n",
      "loss: 0.776474  [19264/74412]\n",
      "loss: 0.900416  [25664/74412]\n",
      "loss: 0.938772  [32064/74412]\n",
      "loss: 0.925761  [38464/74412]\n",
      "loss: 0.691347  [44864/74412]\n",
      "loss: 0.884070  [51264/74412]\n",
      "loss: 0.575632  [57664/74412]\n",
      "loss: 0.985344  [64064/74412]\n",
      "loss: 0.729189  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.995284 \n",
      "\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "loss: 1.209928  [   64/74412]\n",
      "loss: 0.826716  [ 6464/74412]\n",
      "loss: 0.822241  [12864/74412]\n",
      "loss: 0.776923  [19264/74412]\n",
      "loss: 0.900177  [25664/74412]\n",
      "loss: 0.938814  [32064/74412]\n",
      "loss: 0.925158  [38464/74412]\n",
      "loss: 0.691028  [44864/74412]\n",
      "loss: 0.883876  [51264/74412]\n",
      "loss: 0.575895  [57664/74412]\n",
      "loss: 0.986314  [64064/74412]\n",
      "loss: 0.730558  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.994378 \n",
      "\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "loss: 1.208215  [   64/74412]\n",
      "loss: 0.826519  [ 6464/74412]\n",
      "loss: 0.822093  [12864/74412]\n",
      "loss: 0.775843  [19264/74412]\n",
      "loss: 0.899451  [25664/74412]\n",
      "loss: 0.937961  [32064/74412]\n",
      "loss: 0.924803  [38464/74412]\n",
      "loss: 0.691140  [44864/74412]\n",
      "loss: 0.883209  [51264/74412]\n",
      "loss: 0.575717  [57664/74412]\n",
      "loss: 0.984615  [64064/74412]\n",
      "loss: 0.729604  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.994416 \n",
      "\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "loss: 1.207754  [   64/74412]\n",
      "loss: 0.823981  [ 6464/74412]\n",
      "loss: 0.821126  [12864/74412]\n",
      "loss: 0.775388  [19264/74412]\n",
      "loss: 0.899402  [25664/74412]\n",
      "loss: 0.937882  [32064/74412]\n",
      "loss: 0.922361  [38464/74412]\n",
      "loss: 0.690063  [44864/74412]\n",
      "loss: 0.882106  [51264/74412]\n",
      "loss: 0.574165  [57664/74412]\n",
      "loss: 0.983928  [64064/74412]\n",
      "loss: 0.729073  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.994144 \n",
      "\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "loss: 1.207768  [   64/74412]\n",
      "loss: 0.823846  [ 6464/74412]\n",
      "loss: 0.821251  [12864/74412]\n",
      "loss: 0.774488  [19264/74412]\n",
      "loss: 0.898694  [25664/74412]\n",
      "loss: 0.937340  [32064/74412]\n",
      "loss: 0.924007  [38464/74412]\n",
      "loss: 0.689537  [44864/74412]\n",
      "loss: 0.882315  [51264/74412]\n",
      "loss: 0.574566  [57664/74412]\n",
      "loss: 0.983276  [64064/74412]\n",
      "loss: 0.729437  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.993188 \n",
      "\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "loss: 1.206394  [   64/74412]\n",
      "loss: 0.824136  [ 6464/74412]\n",
      "loss: 0.820903  [12864/74412]\n",
      "loss: 0.774767  [19264/74412]\n",
      "loss: 0.897691  [25664/74412]\n",
      "loss: 0.937268  [32064/74412]\n",
      "loss: 0.922844  [38464/74412]\n",
      "loss: 0.690151  [44864/74412]\n",
      "loss: 0.881141  [51264/74412]\n",
      "loss: 0.573951  [57664/74412]\n",
      "loss: 0.983484  [64064/74412]\n",
      "loss: 0.729126  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.992789 \n",
      "\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "loss: 1.207090  [   64/74412]\n",
      "loss: 0.823839  [ 6464/74412]\n",
      "loss: 0.820005  [12864/74412]\n",
      "loss: 0.773771  [19264/74412]\n",
      "loss: 0.898334  [25664/74412]\n",
      "loss: 0.935784  [32064/74412]\n",
      "loss: 0.923546  [38464/74412]\n",
      "loss: 0.689015  [44864/74412]\n",
      "loss: 0.881543  [51264/74412]\n",
      "loss: 0.572727  [57664/74412]\n",
      "loss: 0.982674  [64064/74412]\n",
      "loss: 0.728326  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.993201 \n",
      "\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "loss: 1.205945  [   64/74412]\n",
      "loss: 0.823899  [ 6464/74412]\n",
      "loss: 0.820110  [12864/74412]\n",
      "loss: 0.773165  [19264/74412]\n",
      "loss: 0.897015  [25664/74412]\n",
      "loss: 0.935662  [32064/74412]\n",
      "loss: 0.921077  [38464/74412]\n",
      "loss: 0.689414  [44864/74412]\n",
      "loss: 0.880587  [51264/74412]\n",
      "loss: 0.573989  [57664/74412]\n",
      "loss: 0.982419  [64064/74412]\n",
      "loss: 0.728387  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.991561 \n",
      "\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "loss: 1.203125  [   64/74412]\n",
      "loss: 0.823746  [ 6464/74412]\n",
      "loss: 0.819282  [12864/74412]\n",
      "loss: 0.773188  [19264/74412]\n",
      "loss: 0.896259  [25664/74412]\n",
      "loss: 0.935216  [32064/74412]\n",
      "loss: 0.920105  [38464/74412]\n",
      "loss: 0.688182  [44864/74412]\n",
      "loss: 0.879207  [51264/74412]\n",
      "loss: 0.571825  [57664/74412]\n",
      "loss: 0.981698  [64064/74412]\n",
      "loss: 0.727550  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.992037 \n",
      "\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "loss: 1.204409  [   64/74412]\n",
      "loss: 0.822987  [ 6464/74412]\n",
      "loss: 0.819193  [12864/74412]\n",
      "loss: 0.772326  [19264/74412]\n",
      "loss: 0.896090  [25664/74412]\n",
      "loss: 0.934208  [32064/74412]\n",
      "loss: 0.920652  [38464/74412]\n",
      "loss: 0.688232  [44864/74412]\n",
      "loss: 0.878275  [51264/74412]\n",
      "loss: 0.571648  [57664/74412]\n",
      "loss: 0.981881  [64064/74412]\n",
      "loss: 0.727284  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.991418 \n",
      "\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "loss: 1.202841  [   64/74412]\n",
      "loss: 0.823126  [ 6464/74412]\n",
      "loss: 0.819163  [12864/74412]\n",
      "loss: 0.771480  [19264/74412]\n",
      "loss: 0.896509  [25664/74412]\n",
      "loss: 0.934032  [32064/74412]\n",
      "loss: 0.919926  [38464/74412]\n",
      "loss: 0.688561  [44864/74412]\n",
      "loss: 0.877533  [51264/74412]\n",
      "loss: 0.571450  [57664/74412]\n",
      "loss: 0.981469  [64064/74412]\n",
      "loss: 0.728316  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.991428 \n",
      "\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "loss: 1.203612  [   64/74412]\n",
      "loss: 0.822638  [ 6464/74412]\n",
      "loss: 0.819212  [12864/74412]\n",
      "loss: 0.771110  [19264/74412]\n",
      "loss: 0.896058  [25664/74412]\n",
      "loss: 0.934297  [32064/74412]\n",
      "loss: 0.920060  [38464/74412]\n",
      "loss: 0.687302  [44864/74412]\n",
      "loss: 0.877395  [51264/74412]\n",
      "loss: 0.571075  [57664/74412]\n",
      "loss: 0.980160  [64064/74412]\n",
      "loss: 0.727587  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.991496 \n",
      "\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "loss: 1.202544  [   64/74412]\n",
      "loss: 0.822596  [ 6464/74412]\n",
      "loss: 0.818956  [12864/74412]\n",
      "loss: 0.770514  [19264/74412]\n",
      "loss: 0.894821  [25664/74412]\n",
      "loss: 0.933850  [32064/74412]\n",
      "loss: 0.918094  [38464/74412]\n",
      "loss: 0.687518  [44864/74412]\n",
      "loss: 0.876263  [51264/74412]\n",
      "loss: 0.570622  [57664/74412]\n",
      "loss: 0.980721  [64064/74412]\n",
      "loss: 0.727139  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.991233 \n",
      "\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "loss: 1.202652  [   64/74412]\n",
      "loss: 0.821617  [ 6464/74412]\n",
      "loss: 0.818408  [12864/74412]\n",
      "loss: 0.771301  [19264/74412]\n",
      "loss: 0.896312  [25664/74412]\n",
      "loss: 0.932666  [32064/74412]\n",
      "loss: 0.919118  [38464/74412]\n",
      "loss: 0.686807  [44864/74412]\n",
      "loss: 0.876695  [51264/74412]\n",
      "loss: 0.570291  [57664/74412]\n",
      "loss: 0.978171  [64064/74412]\n",
      "loss: 0.727029  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.991601 \n",
      "\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "loss: 1.202509  [   64/74412]\n",
      "loss: 0.820988  [ 6464/74412]\n",
      "loss: 0.818591  [12864/74412]\n",
      "loss: 0.770280  [19264/74412]\n",
      "loss: 0.895460  [25664/74412]\n",
      "loss: 0.932451  [32064/74412]\n",
      "loss: 0.917643  [38464/74412]\n",
      "loss: 0.685809  [44864/74412]\n",
      "loss: 0.874549  [51264/74412]\n",
      "loss: 0.568845  [57664/74412]\n",
      "loss: 0.977837  [64064/74412]\n",
      "loss: 0.726203  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.990798 \n",
      "\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "loss: 1.201609  [   64/74412]\n",
      "loss: 0.820829  [ 6464/74412]\n",
      "loss: 0.818844  [12864/74412]\n",
      "loss: 0.771011  [19264/74412]\n",
      "loss: 0.894303  [25664/74412]\n",
      "loss: 0.931865  [32064/74412]\n",
      "loss: 0.918541  [38464/74412]\n",
      "loss: 0.685404  [44864/74412]\n",
      "loss: 0.874595  [51264/74412]\n",
      "loss: 0.569055  [57664/74412]\n",
      "loss: 0.977939  [64064/74412]\n",
      "loss: 0.725719  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.991111 \n",
      "\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "loss: 1.200771  [   64/74412]\n",
      "loss: 0.820402  [ 6464/74412]\n",
      "loss: 0.818137  [12864/74412]\n",
      "loss: 0.770360  [19264/74412]\n",
      "loss: 0.893820  [25664/74412]\n",
      "loss: 0.929899  [32064/74412]\n",
      "loss: 0.918261  [38464/74412]\n",
      "loss: 0.684677  [44864/74412]\n",
      "loss: 0.874115  [51264/74412]\n",
      "loss: 0.568679  [57664/74412]\n",
      "loss: 0.976918  [64064/74412]\n",
      "loss: 0.726008  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.989567 \n",
      "\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "loss: 1.199435  [   64/74412]\n",
      "loss: 0.819712  [ 6464/74412]\n",
      "loss: 0.817972  [12864/74412]\n",
      "loss: 0.769898  [19264/74412]\n",
      "loss: 0.893655  [25664/74412]\n",
      "loss: 0.930866  [32064/74412]\n",
      "loss: 0.917081  [38464/74412]\n",
      "loss: 0.684177  [44864/74412]\n",
      "loss: 0.873546  [51264/74412]\n",
      "loss: 0.568722  [57664/74412]\n",
      "loss: 0.976781  [64064/74412]\n",
      "loss: 0.725639  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.989484 \n",
      "\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "loss: 1.199963  [   64/74412]\n",
      "loss: 0.818930  [ 6464/74412]\n",
      "loss: 0.817423  [12864/74412]\n",
      "loss: 0.769324  [19264/74412]\n",
      "loss: 0.892325  [25664/74412]\n",
      "loss: 0.929867  [32064/74412]\n",
      "loss: 0.916223  [38464/74412]\n",
      "loss: 0.683810  [44864/74412]\n",
      "loss: 0.873685  [51264/74412]\n",
      "loss: 0.569115  [57664/74412]\n",
      "loss: 0.975037  [64064/74412]\n",
      "loss: 0.724772  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.988940 \n",
      "\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "loss: 1.197873  [   64/74412]\n",
      "loss: 0.817723  [ 6464/74412]\n",
      "loss: 0.821193  [12864/74412]\n",
      "loss: 0.769191  [19264/74412]\n",
      "loss: 0.892652  [25664/74412]\n",
      "loss: 0.929023  [32064/74412]\n",
      "loss: 0.916958  [38464/74412]\n",
      "loss: 0.683318  [44864/74412]\n",
      "loss: 0.873208  [51264/74412]\n",
      "loss: 0.568655  [57664/74412]\n",
      "loss: 0.975539  [64064/74412]\n",
      "loss: 0.724798  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.988426 \n",
      "\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "loss: 1.197075  [   64/74412]\n",
      "loss: 0.818315  [ 6464/74412]\n",
      "loss: 0.820629  [12864/74412]\n",
      "loss: 0.767578  [19264/74412]\n",
      "loss: 0.891137  [25664/74412]\n",
      "loss: 0.929440  [32064/74412]\n",
      "loss: 0.916043  [38464/74412]\n",
      "loss: 0.682354  [44864/74412]\n",
      "loss: 0.872609  [51264/74412]\n",
      "loss: 0.567084  [57664/74412]\n",
      "loss: 0.976214  [64064/74412]\n",
      "loss: 0.724057  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.987913 \n",
      "\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "loss: 1.196687  [   64/74412]\n",
      "loss: 0.817241  [ 6464/74412]\n",
      "loss: 0.817292  [12864/74412]\n",
      "loss: 0.768149  [19264/74412]\n",
      "loss: 0.891746  [25664/74412]\n",
      "loss: 0.928133  [32064/74412]\n",
      "loss: 0.914420  [38464/74412]\n",
      "loss: 0.682299  [44864/74412]\n",
      "loss: 0.872325  [51264/74412]\n",
      "loss: 0.567581  [57664/74412]\n",
      "loss: 0.975265  [64064/74412]\n",
      "loss: 0.723843  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.987853 \n",
      "\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "loss: 1.197090  [   64/74412]\n",
      "loss: 0.816926  [ 6464/74412]\n",
      "loss: 0.816438  [12864/74412]\n",
      "loss: 0.768122  [19264/74412]\n",
      "loss: 0.891180  [25664/74412]\n",
      "loss: 0.927856  [32064/74412]\n",
      "loss: 0.913366  [38464/74412]\n",
      "loss: 0.681440  [44864/74412]\n",
      "loss: 0.872688  [51264/74412]\n",
      "loss: 0.566846  [57664/74412]\n",
      "loss: 0.975145  [64064/74412]\n",
      "loss: 0.724255  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.987152 \n",
      "\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "loss: 1.195446  [   64/74412]\n",
      "loss: 0.816852  [ 6464/74412]\n",
      "loss: 0.820151  [12864/74412]\n",
      "loss: 0.767647  [19264/74412]\n",
      "loss: 0.890757  [25664/74412]\n",
      "loss: 0.927577  [32064/74412]\n",
      "loss: 0.915762  [38464/74412]\n",
      "loss: 0.681039  [44864/74412]\n",
      "loss: 0.871949  [51264/74412]\n",
      "loss: 0.566006  [57664/74412]\n",
      "loss: 0.974542  [64064/74412]\n",
      "loss: 0.723353  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.986589 \n",
      "\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "loss: 1.193764  [   64/74412]\n",
      "loss: 0.816224  [ 6464/74412]\n",
      "loss: 0.819810  [12864/74412]\n",
      "loss: 0.766390  [19264/74412]\n",
      "loss: 0.891079  [25664/74412]\n",
      "loss: 0.926957  [32064/74412]\n",
      "loss: 0.913755  [38464/74412]\n",
      "loss: 0.680210  [44864/74412]\n",
      "loss: 0.871810  [51264/74412]\n",
      "loss: 0.566476  [57664/74412]\n",
      "loss: 0.973804  [64064/74412]\n",
      "loss: 0.724033  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.986542 \n",
      "\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "loss: 1.193750  [   64/74412]\n",
      "loss: 0.815917  [ 6464/74412]\n",
      "loss: 0.815776  [12864/74412]\n",
      "loss: 0.766416  [19264/74412]\n",
      "loss: 0.889534  [25664/74412]\n",
      "loss: 0.926576  [32064/74412]\n",
      "loss: 0.912925  [38464/74412]\n",
      "loss: 0.679568  [44864/74412]\n",
      "loss: 0.871262  [51264/74412]\n",
      "loss: 0.566266  [57664/74412]\n",
      "loss: 0.972366  [64064/74412]\n",
      "loss: 0.722778  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.985652 \n",
      "\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "loss: 1.191950  [   64/74412]\n",
      "loss: 0.815334  [ 6464/74412]\n",
      "loss: 0.819154  [12864/74412]\n",
      "loss: 0.766309  [19264/74412]\n",
      "loss: 0.888383  [25664/74412]\n",
      "loss: 0.927025  [32064/74412]\n",
      "loss: 0.913166  [38464/74412]\n",
      "loss: 0.679081  [44864/74412]\n",
      "loss: 0.871176  [51264/74412]\n",
      "loss: 0.566211  [57664/74412]\n",
      "loss: 0.972248  [64064/74412]\n",
      "loss: 0.722775  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.985865 \n",
      "\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "loss: 1.192134  [   64/74412]\n",
      "loss: 0.813309  [ 6464/74412]\n",
      "loss: 0.818982  [12864/74412]\n",
      "loss: 0.765672  [19264/74412]\n",
      "loss: 0.888529  [25664/74412]\n",
      "loss: 0.925435  [32064/74412]\n",
      "loss: 0.913781  [38464/74412]\n",
      "loss: 0.678945  [44864/74412]\n",
      "loss: 0.870857  [51264/74412]\n",
      "loss: 0.566303  [57664/74412]\n",
      "loss: 0.972453  [64064/74412]\n",
      "loss: 0.722242  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.984458 \n",
      "\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "loss: 1.190951  [   64/74412]\n",
      "loss: 0.813542  [ 6464/74412]\n",
      "loss: 0.818583  [12864/74412]\n",
      "loss: 0.765739  [19264/74412]\n",
      "loss: 0.889027  [25664/74412]\n",
      "loss: 0.925080  [32064/74412]\n",
      "loss: 0.911654  [38464/74412]\n",
      "loss: 0.680210  [44864/74412]\n",
      "loss: 0.869987  [51264/74412]\n",
      "loss: 0.565319  [57664/74412]\n",
      "loss: 0.972002  [64064/74412]\n",
      "loss: 0.721933  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.985232 \n",
      "\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "loss: 1.189906  [   64/74412]\n",
      "loss: 0.812073  [ 6464/74412]\n",
      "loss: 0.818440  [12864/74412]\n",
      "loss: 0.765394  [19264/74412]\n",
      "loss: 0.888182  [25664/74412]\n",
      "loss: 0.924722  [32064/74412]\n",
      "loss: 0.912523  [38464/74412]\n",
      "loss: 0.676436  [44864/74412]\n",
      "loss: 0.870329  [51264/74412]\n",
      "loss: 0.565339  [57664/74412]\n",
      "loss: 0.970698  [64064/74412]\n",
      "loss: 0.721158  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.984262 \n",
      "\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "loss: 1.188556  [   64/74412]\n",
      "loss: 0.812150  [ 6464/74412]\n",
      "loss: 0.818464  [12864/74412]\n",
      "loss: 0.764787  [19264/74412]\n",
      "loss: 0.888045  [25664/74412]\n",
      "loss: 0.924333  [32064/74412]\n",
      "loss: 0.911261  [38464/74412]\n",
      "loss: 0.677382  [44864/74412]\n",
      "loss: 0.869732  [51264/74412]\n",
      "loss: 0.565174  [57664/74412]\n",
      "loss: 0.971482  [64064/74412]\n",
      "loss: 0.720998  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.984326 \n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "loss: 1.188382  [   64/74412]\n",
      "loss: 0.811330  [ 6464/74412]\n",
      "loss: 0.818262  [12864/74412]\n",
      "loss: 0.764238  [19264/74412]\n",
      "loss: 0.888402  [25664/74412]\n",
      "loss: 0.924250  [32064/74412]\n",
      "loss: 0.911757  [38464/74412]\n",
      "loss: 0.677434  [44864/74412]\n",
      "loss: 0.868838  [51264/74412]\n",
      "loss: 0.564756  [57664/74412]\n",
      "loss: 0.971418  [64064/74412]\n",
      "loss: 0.719855  [70464/74412]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.982999 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c7f8a",
   "metadata": {},
   "source": [
    "Best: 60.7% after 300 epochs, could improve more\n",
    "75.7% after 1000 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80978cc",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.968220 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(valid_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c8314",
   "metadata": {},
   "source": [
    "Result for validation set: 60.6% - 300 epochs\n",
    "75,9% - 1000 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d25e66",
   "metadata": {},
   "source": [
    "# Parameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d980415",
   "metadata": {},
   "source": [
    "Parameter to test: Learning Rate, Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f4a2d6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3a0cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "434f5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNModel import NNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "53a55130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear(in_features=106, out_features=250, bias=True), ReLU(), Linear(in_features=250, out_features=164, bias=True), ReLU(), Linear(in_features=164, out_features=164, bias=True), ReLU(), Linear(in_features=164, out_features=104, bias=True), ReLU()]\n",
      "Step 0\n",
      "Best Params, Batch Size: 64, Learning Rate: 0.010, Acc: 12.5\n",
      "{'learning_rate': 0.01, 'batch_size': 64}\n",
      "[Linear(in_features=106, out_features=250, bias=True), ReLU(), Linear(in_features=250, out_features=164, bias=True), ReLU(), Linear(in_features=164, out_features=164, bias=True), ReLU(), Linear(in_features=164, out_features=104, bias=True), ReLU()]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m init_param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m64\u001b[39m}\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# nnmodel.grid_search(dict_param, train_ds, test_ds, epochs=1)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mnnmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/src/NNModel.py:130\u001b[0m, in \u001b[0;36mNNModel.local_search\u001b[0;34m(self, init_param, train_ds, test_ds, epochs, steps)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_param)\n\u001b[1;32m    129\u001b[0m neighbor_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneighbor_hood(best_param)\n\u001b[0;32m--> 130\u001b[0m params, acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(acc \u001b[38;5;241m>\u001b[39m best_acc):\n\u001b[1;32m    133\u001b[0m     best_acc \u001b[38;5;241m=\u001b[39m acc\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/src/NNModel.py:98\u001b[0m, in \u001b[0;36mNNModel.grid_search\u001b[0;34m(self, dict_param, train_ds, test_ds, epochs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m---> 98\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(acc \u001b[38;5;241m>\u001b[39m best_acc):\n\u001b[1;32m    100\u001b[0m     best_param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/src/NNModel.py:82\u001b[0m, in \u001b[0;36mNNModel.run\u001b[0;34m(self, train_dataloader, test_dataloader, epochs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_dataloader, test_dataloader, epochs):\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 82\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m         acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest(test_dataloader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m acc\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/src/NNModel.py:59\u001b[0m, in \u001b[0;36mNNModel.train\u001b[0;34m(self, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     58\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 59\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/torch/optim/sgd.py:75\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     71\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 75\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/torch/optim/sgd.py:220\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 220\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/TU/2023W/ML/ML-NN-Parameter-Search/.venv/lib/python3.11/site-packages/torch/optim/sgd.py:263\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m         d_p \u001b[38;5;241m=\u001b[39m buf\n\u001b[0;32m--> 263\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer = [len(X_ttrain[0]), 250, 164, 164, 104]\n",
    "nnmodel = NNModel(layer, device, target=104, features=len(X_ttrain[0]))\n",
    "dict_param = {\"learning_rate\": [0.001, 0.01, 0.05], \"batch_size\": [32, 64, 128]}\n",
    "init_param = {\"learning_rate\": 0.01, \"batch_size\": 64}\n",
    "\n",
    "# nnmodel.grid_search(dict_param, train_ds, test_ds, epochs=1)\n",
    "nnmodel.local_search(init_param, train_ds, test_ds, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
