{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdeff493",
   "metadata": {},
   "source": [
    "# Load Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d6d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4215c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_bag = pd.read_csv('../data/beer_valid.csv', index_col='index')\n",
    "y_valid = pd.read_csv('../data/beer_target_valid.csv', index_col='index')\n",
    "X_train_bag = pd.read_csv('../data/beer_train.csv', index_col='index')\n",
    "y_train = pd.read_csv('../data/beer_target_train.csv', index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbd9b8f4-408c-4b73-b383-5cccda612759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(523583, 918)\n",
      "(523583, 1)\n",
      "(1063030, 918)\n",
      "(1063030, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_valid_bag.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_train_bag.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2757d3",
   "metadata": {},
   "source": [
    "# Find Solution for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4afda021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a7ff094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ttrain, X_test, y_ttrain, y_test = train_test_split(X_train_bag.values, y_train.values, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eef3a48d-9566-408f-9e70-f5024e802299",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m y_ttrain \u001b[38;5;241m=\u001b[39m y_ttrain\u001b[38;5;241m.\u001b[39mreshape(y_ttrain\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      2\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mreshape(y_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m y_valid \u001b[38;5;241m=\u001b[39m \u001b[43my_valid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n\u001b[1;32m      4\u001b[0m y_valid \u001b[38;5;241m=\u001b[39m y_valid\u001b[38;5;241m.\u001b[39mreshape(y_valid\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "y_ttrain = y_ttrain.reshape(y_ttrain.shape[0])\n",
    "y_test = y_test.reshape(y_test.shape[0])\n",
    "y_valid = y_valid.values\n",
    "y_valid = y_valid.reshape(y_valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_ttrain)\n",
    "print(y_ttrain)\n",
    "print(y_ttrain.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e8573",
   "metadata": {},
   "source": [
    "## Build torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a580683",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.any(np.isnan(X_ttrain))\n",
    "assert not np.any(np.isnan(y_ttrain))\n",
    "assert not np.any(np.isnan(X_test))\n",
    "assert not np.any(np.isnan(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# convert a df to tensor to be used in pytorch\n",
    "def X_to_tensor(df):\n",
    "    return torch.from_numpy(df).float().to(device)\n",
    "\n",
    "def y_to_tensor(df):\n",
    "    return torch.from_numpy(df).long().to(device)\n",
    "\n",
    "X_train_tensor = X_to_tensor(X_ttrain)\n",
    "y_train_tensor = y_to_tensor(y_ttrain)\n",
    "\n",
    "X_test_tensor = X_to_tensor(X_test)\n",
    "y_test_tensor = y_to_tensor(y_test)\n",
    "\n",
    "X_valid_tensor = X_to_tensor(X_valid_bag.values)\n",
    "y_valid_tensor = y_to_tensor(y_valid)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "valid_ds = TensorDataset(X_valid_tensor, y_valid_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea004d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_ttrain\n",
    "del X_test\n",
    "\n",
    "del y_ttrain\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "46cc9573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([1280, 918])\n",
      "Shape of y: torch.Size([1280]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1280\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(valid_ds, batch_size=len(valid_ds))\n",
    "\n",
    "for XX, yy in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {XX.shape}\")\n",
    "    print(f\"Shape of y: {yy.shape} {yy.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16463da9",
   "metadata": {},
   "source": [
    "## Creating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0c38c825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=918, out_features=250, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=250, out_features=164, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=164, out_features=164, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=164, out_features=104, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(len(train_ds[0][0]), 250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 164),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(164, 164),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(164, 104)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5300a4",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e95387a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b07e9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (XX, yy) in enumerate(dataloader):\n",
    "        XX, yy = XX.to(device), yy.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(XX)\n",
    "        loss = loss_fn(pred, yy)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss.item()\n",
    "        train_loss += loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            current = (batch + 1) * len(XX)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return test_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7a613680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for XX, yy in dataloader:\n",
    "            XX, yy = XX.to(device), yy.to(device)\n",
    "            pred = model(XX)\n",
    "            test_loss += loss_fn(pred, yy).item()\n",
    "            # correct += (pred.argmax(1) == yy).type(torch.float).sum().item()\n",
    "            correct += f1_score(yy, pred.argmax(1), average='weighted')\n",
    "    test_loss /= num_batches\n",
    "    correct /= num_batches\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "20a02e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4.651258  [ 1280/744121]\n",
      "loss: 4.590582  [129280/744121]\n",
      "loss: 4.317019  [257280/744121]\n",
      "loss: 4.137877  [385280/744121]\n",
      "loss: 4.086305  [513280/744121]\n",
      "loss: 4.127110  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 4.100077 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 4.085429  [ 1280/744121]\n",
      "loss: 4.158247  [129280/744121]\n",
      "loss: 4.107521  [257280/744121]\n",
      "loss: 4.067199  [385280/744121]\n",
      "loss: 4.017262  [513280/744121]\n",
      "loss: 4.048603  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 2.8%, Avg loss: 3.987358 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.967106  [ 1280/744121]\n",
      "loss: 4.003461  [129280/744121]\n",
      "loss: 3.889530  [257280/744121]\n",
      "loss: 3.830221  [385280/744121]\n",
      "loss: 3.732576  [513280/744121]\n",
      "loss: 3.799212  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 4.4%, Avg loss: 3.739028 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.723816  [ 1280/744121]\n",
      "loss: 3.792524  [129280/744121]\n",
      "loss: 3.719739  [257280/744121]\n",
      "loss: 3.705720  [385280/744121]\n",
      "loss: 3.608862  [513280/744121]\n",
      "loss: 3.691026  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 4.8%, Avg loss: 3.641111 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.626948  [ 1280/744121]\n",
      "loss: 3.699675  [129280/744121]\n",
      "loss: 3.634194  [257280/744121]\n",
      "loss: 3.615425  [385280/744121]\n",
      "loss: 3.506358  [513280/744121]\n",
      "loss: 3.576427  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 5.3%, Avg loss: 3.513479 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.497752  [ 1280/744121]\n",
      "loss: 3.546732  [129280/744121]\n",
      "loss: 3.523425  [257280/744121]\n",
      "loss: 3.451212  [385280/744121]\n",
      "loss: 3.411854  [513280/744121]\n",
      "loss: 3.433885  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 8.6%, Avg loss: 3.330039 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.314739  [ 1280/744121]\n",
      "loss: 3.364493  [129280/744121]\n",
      "loss: 3.353103  [257280/744121]\n",
      "loss: 3.377033  [385280/744121]\n",
      "loss: 3.193870  [513280/744121]\n",
      "loss: 3.385867  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 12.2%, Avg loss: 3.161570 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.152917  [ 1280/744121]\n",
      "loss: 3.309064  [129280/744121]\n",
      "loss: 3.289496  [257280/744121]\n",
      "loss: 3.201091  [385280/744121]\n",
      "loss: 3.125447  [513280/744121]\n",
      "loss: 3.272403  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 13.8%, Avg loss: 3.054012 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.049296  [ 1280/744121]\n",
      "loss: 3.176075  [129280/744121]\n",
      "loss: 3.130597  [257280/744121]\n",
      "loss: 3.037693  [385280/744121]\n",
      "loss: 3.020436  [513280/744121]\n",
      "loss: 3.009485  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 16.8%, Avg loss: 2.936798 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.933580  [ 1280/744121]\n",
      "loss: 2.973369  [129280/744121]\n",
      "loss: 3.027072  [257280/744121]\n",
      "loss: 2.892781  [385280/744121]\n",
      "loss: 2.771535  [513280/744121]\n",
      "loss: 2.869144  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 3.325247 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 3.354917  [ 1280/744121]\n",
      "loss: 2.868921  [129280/744121]\n",
      "loss: 2.916892  [257280/744121]\n",
      "loss: 2.701389  [385280/744121]\n",
      "loss: 2.689718  [513280/744121]\n",
      "loss: 2.761976  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 24.7%, Avg loss: 2.688850 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.722952  [ 1280/744121]\n",
      "loss: 2.715496  [129280/744121]\n",
      "loss: 2.949215  [257280/744121]\n",
      "loss: 2.622086  [385280/744121]\n",
      "loss: 2.634042  [513280/744121]\n",
      "loss: 2.635727  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 21.1%, Avg loss: 2.668414 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.674315  [ 1280/744121]\n",
      "loss: 2.732476  [129280/744121]\n",
      "loss: 2.894578  [257280/744121]\n",
      "loss: 2.495389  [385280/744121]\n",
      "loss: 2.705320  [513280/744121]\n",
      "loss: 2.579160  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 20.4%, Avg loss: 2.767659 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.780513  [ 1280/744121]\n",
      "loss: 2.848512  [129280/744121]\n",
      "loss: 2.569334  [257280/744121]\n",
      "loss: 2.583691  [385280/744121]\n",
      "loss: 2.408296  [513280/744121]\n",
      "loss: 2.494708  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 22.7%, Avg loss: 2.680668 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.691935  [ 1280/744121]\n",
      "loss: 2.632731  [129280/744121]\n",
      "loss: 2.591770  [257280/744121]\n",
      "loss: 2.439298  [385280/744121]\n",
      "loss: 2.449450  [513280/744121]\n",
      "loss: 2.625123  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 27.3%, Avg loss: 2.494498 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.518387  [ 1280/744121]\n",
      "loss: 2.472095  [129280/744121]\n",
      "loss: 2.445551  [257280/744121]\n",
      "loss: 2.376589  [385280/744121]\n",
      "loss: 2.953111  [513280/744121]\n",
      "loss: 2.563820  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 2.439932 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.485050  [ 1280/744121]\n",
      "loss: 2.358081  [129280/744121]\n",
      "loss: 2.377005  [257280/744121]\n",
      "loss: 2.458892  [385280/744121]\n",
      "loss: 2.250321  [513280/744121]\n",
      "loss: 2.906407  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 28.0%, Avg loss: 2.448769 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.489854  [ 1280/744121]\n",
      "loss: 2.725174  [129280/744121]\n",
      "loss: 2.406743  [257280/744121]\n",
      "loss: 2.211748  [385280/744121]\n",
      "loss: 2.244361  [513280/744121]\n",
      "loss: 2.385278  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.231420 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.276401  [ 1280/744121]\n",
      "loss: 2.262106  [129280/744121]\n",
      "loss: 2.277454  [257280/744121]\n",
      "loss: 2.283778  [385280/744121]\n",
      "loss: 2.158584  [513280/744121]\n",
      "loss: 2.353807  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 32.3%, Avg loss: 2.306085 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.349395  [ 1280/744121]\n",
      "loss: 2.282625  [129280/744121]\n",
      "loss: 2.267539  [257280/744121]\n",
      "loss: 2.269363  [385280/744121]\n",
      "loss: 2.370804  [513280/744121]\n",
      "loss: 2.258930  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 2.243626 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.285863  [ 1280/744121]\n",
      "loss: 2.433129  [129280/744121]\n",
      "loss: 2.137817  [257280/744121]\n",
      "loss: 2.249376  [385280/744121]\n",
      "loss: 2.664761  [513280/744121]\n",
      "loss: 2.309013  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 2.283558 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.295968  [ 1280/744121]\n",
      "loss: 2.104220  [129280/744121]\n",
      "loss: 2.154690  [257280/744121]\n",
      "loss: 2.126766  [385280/744121]\n",
      "loss: 2.028570  [513280/744121]\n",
      "loss: 2.040407  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 2.060076 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.070236  [ 1280/744121]\n",
      "loss: 2.098030  [129280/744121]\n",
      "loss: 2.115726  [257280/744121]\n",
      "loss: 2.131565  [385280/744121]\n",
      "loss: 2.124398  [513280/744121]\n",
      "loss: 1.977930  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.039065 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.055620  [ 1280/744121]\n",
      "loss: 2.010518  [129280/744121]\n",
      "loss: 2.048215  [257280/744121]\n",
      "loss: 2.100155  [385280/744121]\n",
      "loss: 2.217132  [513280/744121]\n",
      "loss: 1.915892  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 1.875333 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.901037  [ 1280/744121]\n",
      "loss: 2.136264  [129280/744121]\n",
      "loss: 2.169703  [257280/744121]\n",
      "loss: 1.941849  [385280/744121]\n",
      "loss: 1.986589  [513280/744121]\n",
      "loss: 1.905368  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.814135 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.829452  [ 1280/744121]\n",
      "loss: 2.149472  [129280/744121]\n",
      "loss: 2.023048  [257280/744121]\n",
      "loss: 1.952145  [385280/744121]\n",
      "loss: 1.840077  [513280/744121]\n",
      "loss: 1.811937  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 1.926282 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.952361  [ 1280/744121]\n",
      "loss: 1.996789  [129280/744121]\n",
      "loss: 1.893419  [257280/744121]\n",
      "loss: 1.874622  [385280/744121]\n",
      "loss: 1.825202  [513280/744121]\n",
      "loss: 1.863714  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.692579 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.692224  [ 1280/744121]\n",
      "loss: 1.923519  [129280/744121]\n",
      "loss: 1.939359  [257280/744121]\n",
      "loss: 1.856462  [385280/744121]\n",
      "loss: 1.999287  [513280/744121]\n",
      "loss: 1.770739  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.908224 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.896492  [ 1280/744121]\n",
      "loss: 1.758042  [129280/744121]\n",
      "loss: 1.815644  [257280/744121]\n",
      "loss: 1.817104  [385280/744121]\n",
      "loss: 1.792496  [513280/744121]\n",
      "loss: 1.732216  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.703376 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.699783  [ 1280/744121]\n",
      "loss: 1.703070  [129280/744121]\n",
      "loss: 1.817430  [257280/744121]\n",
      "loss: 1.708611  [385280/744121]\n",
      "loss: 1.895383  [513280/744121]\n",
      "loss: 1.680089  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 1.670261 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.652557  [ 1280/744121]\n",
      "loss: 1.689869  [129280/744121]\n",
      "loss: 1.665245  [257280/744121]\n",
      "loss: 1.952103  [385280/744121]\n",
      "loss: 1.855299  [513280/744121]\n",
      "loss: 1.607883  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 1.800970 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.781067  [ 1280/744121]\n",
      "loss: 1.654413  [129280/744121]\n",
      "loss: 1.745339  [257280/744121]\n",
      "loss: 1.759028  [385280/744121]\n",
      "loss: 1.876523  [513280/744121]\n",
      "loss: 1.620968  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 1.661073 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.652181  [ 1280/744121]\n",
      "loss: 1.612861  [129280/744121]\n",
      "loss: 1.583827  [257280/744121]\n",
      "loss: 1.761740  [385280/744121]\n",
      "loss: 1.507503  [513280/744121]\n",
      "loss: 1.585119  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.769504 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.738033  [ 1280/744121]\n",
      "loss: 1.497411  [129280/744121]\n",
      "loss: 1.602229  [257280/744121]\n",
      "loss: 1.701464  [385280/744121]\n",
      "loss: 1.717661  [513280/744121]\n",
      "loss: 1.514126  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 1.589629 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.565957  [ 1280/744121]\n",
      "loss: 1.588109  [129280/744121]\n",
      "loss: 1.501085  [257280/744121]\n",
      "loss: 1.563298  [385280/744121]\n",
      "loss: 1.553390  [513280/744121]\n",
      "loss: 1.467806  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 1.481471 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.449361  [ 1280/744121]\n",
      "loss: 1.453676  [129280/744121]\n",
      "loss: 1.611164  [257280/744121]\n",
      "loss: 1.615068  [385280/744121]\n",
      "loss: 1.503071  [513280/744121]\n",
      "loss: 1.723131  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.438995 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.405681  [ 1280/744121]\n",
      "loss: 1.491753  [129280/744121]\n",
      "loss: 1.694549  [257280/744121]\n",
      "loss: 1.673611  [385280/744121]\n",
      "loss: 1.383284  [513280/744121]\n",
      "loss: 1.651650  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.446695 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.414980  [ 1280/744121]\n",
      "loss: 1.467780  [129280/744121]\n",
      "loss: 1.612386  [257280/744121]\n",
      "loss: 1.388851  [385280/744121]\n",
      "loss: 1.418814  [513280/744121]\n",
      "loss: 1.505705  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 1.484966 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.460597  [ 1280/744121]\n",
      "loss: 1.416899  [129280/744121]\n",
      "loss: 1.519604  [257280/744121]\n",
      "loss: 1.444154  [385280/744121]\n",
      "loss: 1.399253  [513280/744121]\n",
      "loss: 1.444656  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.534405 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.496857  [ 1280/744121]\n",
      "loss: 1.332756  [129280/744121]\n",
      "loss: 1.465597  [257280/744121]\n",
      "loss: 1.382678  [385280/744121]\n",
      "loss: 1.227466  [513280/744121]\n",
      "loss: 1.534246  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.370384 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.340230  [ 1280/744121]\n",
      "loss: 1.275710  [129280/744121]\n",
      "loss: 1.335722  [257280/744121]\n",
      "loss: 1.329085  [385280/744121]\n",
      "loss: 1.346545  [513280/744121]\n",
      "loss: 1.407589  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.353548 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.319224  [ 1280/744121]\n",
      "loss: 1.271198  [129280/744121]\n",
      "loss: 1.360277  [257280/744121]\n",
      "loss: 1.251611  [385280/744121]\n",
      "loss: 1.329273  [513280/744121]\n",
      "loss: 1.266451  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.310171 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.267611  [ 1280/744121]\n",
      "loss: 1.251838  [129280/744121]\n",
      "loss: 1.331044  [257280/744121]\n",
      "loss: 1.223629  [385280/744121]\n",
      "loss: 1.197718  [513280/744121]\n",
      "loss: 1.320634  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 1.270015 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.247894  [ 1280/744121]\n",
      "loss: 1.283576  [129280/744121]\n",
      "loss: 1.299755  [257280/744121]\n",
      "loss: 1.287490  [385280/744121]\n",
      "loss: 1.723355  [513280/744121]\n",
      "loss: 1.265820  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 1.234456 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.197347  [ 1280/744121]\n",
      "loss: 1.237251  [129280/744121]\n",
      "loss: 1.251690  [257280/744121]\n",
      "loss: 1.167704  [385280/744121]\n",
      "loss: 1.126021  [513280/744121]\n",
      "loss: 1.247207  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.305547 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.290890  [ 1280/744121]\n",
      "loss: 1.237778  [129280/744121]\n",
      "loss: 1.243236  [257280/744121]\n",
      "loss: 1.153540  [385280/744121]\n",
      "loss: 1.093450  [513280/744121]\n",
      "loss: 1.212135  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 1.161061 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.129540  [ 1280/744121]\n",
      "loss: 1.137988  [129280/744121]\n",
      "loss: 1.206682  [257280/744121]\n",
      "loss: 1.159153  [385280/744121]\n",
      "loss: 1.334928  [513280/744121]\n",
      "loss: 1.226350  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.108510 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.063457  [ 1280/744121]\n",
      "loss: 1.110949  [129280/744121]\n",
      "loss: 1.222622  [257280/744121]\n",
      "loss: 1.154064  [385280/744121]\n",
      "loss: 1.052854  [513280/744121]\n",
      "loss: 1.287485  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.235693 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.213033  [ 1280/744121]\n",
      "loss: 1.119941  [129280/744121]\n",
      "loss: 1.191893  [257280/744121]\n",
      "loss: 1.113473  [385280/744121]\n",
      "loss: 1.056479  [513280/744121]\n",
      "loss: 1.300143  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.127821 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.096814  [ 1280/744121]\n",
      "loss: 1.139087  [129280/744121]\n",
      "loss: 1.168220  [257280/744121]\n",
      "loss: 1.087456  [385280/744121]\n",
      "loss: 1.005570  [513280/744121]\n",
      "loss: 1.076306  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 1.090746 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "test_losses = []\n",
    "accs = []\n",
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    losses.append(train(train_dataloader, model, loss_fn, optimizer))\n",
    "    test_loss, acc = test(test_dataloader, model, loss_fn)\n",
    "\n",
    "    accs.append(acc)\n",
    "    test_losses.append(test_loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "afbdf1a9-b51d-41e3-bc37-c9f84b57ffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses [4.281891352532246, 4.065095214909294, 3.8438163127276495, 3.687708723176386, 3.587566351972495, 3.457934481171808, 3.3195913341856493, 3.220066019759555, 3.1029259437547925, 2.9684141349956343, 2.8289376898729515, 2.7059300941290316, 2.6310529209084526, 2.5672429607496228, 2.502283347840981, 2.4452972207282415, 2.3849764124224686, 2.328687124235933, 2.284892273932388, 2.2351583557849897, 2.182858495163344, 2.1256918827283013, 2.0576167561344265, 2.0115473100409886, 1.9689460118202, 1.924617920954203, 1.870258873270959, 1.822097195587617, 1.7748977673012776, 1.720839798040816, 1.6776668376119686, 1.641430725141899, 1.5988564878394924, 1.5611950411010034, 1.5238676454193403, 1.4791595827263246, 1.4475888882306023, 1.409057339442145, 1.380136363694758, 1.3526473266562236, 1.3213375022321223, 1.297745835944959, 1.2701060827245418, 1.2479393560042495, 1.2182026284257161, 1.1987428767574613, 1.17598285144547, 1.1567707189784426, 1.1377410856923698, 1.1169076702234262]\n",
      "test_losses [4.100076854705811, 3.987357632637024, 3.7390280866622927, 3.64111119556427, 3.513478786468506, 3.3300394144058227, 3.161569796562195, 3.054012162208557, 2.9367984886169434, 3.3252467527389524, 2.688849844932556, 2.6684139289855957, 2.7676589279174806, 2.6806683607101442, 2.4944982061386107, 2.439932448387146, 2.448769013404846, 2.2314195070266725, 2.3060847234725954, 2.243626292228699, 2.2835580205917356, 2.0600756578445436, 2.039064720153809, 1.8753325562477112, 1.8141350317001343, 1.926282425403595, 1.6925786280632018, 1.908224102973938, 1.7033761358261108, 1.670261182308197, 1.8009704012870789, 1.6610733604431152, 1.7695040049552917, 1.5896293621063233, 1.4814707202911377, 1.4389948887825013, 1.446694731235504, 1.4849659748077393, 1.5344052991867065, 1.3703835663795472, 1.353547622680664, 1.3101714444160462, 1.2700149288177491, 1.2344559707641602, 1.305547010421753, 1.161061282157898, 1.108509889125824, 1.2356929850578309, 1.1278206434249878, 1.09074600481987]\n",
      "accs [0.010437042761835338, 0.028198772394478648, 0.04423288332230629, 0.04824483682970091, 0.05302136157892598, 0.08606997070867796, 0.12167309513683514, 0.13788627300080733, 0.16794524602126631, 0.10016954650831289, 0.24723668173206476, 0.21084409179813265, 0.20415144846354674, 0.22711644441762505, 0.27318361612905323, 0.32992727519670817, 0.2799213234589873, 0.38162112315725455, 0.323141916049001, 0.3718458659490861, 0.3809212711754519, 0.4119224753942955, 0.3946186019346407, 0.4818402320592535, 0.4929450606000866, 0.4502857332717606, 0.5126926822632163, 0.4786505268508655, 0.5235129554542801, 0.5330500634886335, 0.5119495384976323, 0.5418860265466285, 0.5251655163361466, 0.5496240310753169, 0.5964406334745198, 0.6182510915441678, 0.601947760472173, 0.607266260366738, 0.6084837578667187, 0.6260800258645713, 0.6444254350771931, 0.6557599362653631, 0.6549007066391145, 0.6760857510333965, 0.6481493035830286, 0.6878842714043283, 0.7018061078701516, 0.6656607860975026, 0.6980767677151796, 0.7094270968160629]\n"
     ]
    }
   ],
   "source": [
    "print('losses', losses)\n",
    "print('test_losses', test_losses)\n",
    "print('accs', accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5c5aec59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "\n",
    "plt.plot(range(len(accs)), accs)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig(f\"../results/beer_init_nn_acc.png\", bbox_inches=\"tight\")\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(range(len(losses)), losses, label=\"Training\")\n",
    "plt.plot(range(len(test_losses)), test_losses, label=\"Test\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(f\"../results/beer_init_nn_loss.png\", bbox_inches=\"tight\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80978cc",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ea5f2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model):\n",
    "    num_batches = len(dataloader)\n",
    "    assert num_batches == 1\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for XX, yy in dataloader:\n",
    "            XX, yy = XX.to(device), yy.to(device)\n",
    "            pred = model(XX)\n",
    "            print(classification_report(yy, pred.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ba7d256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.087334 \n",
      "\n",
      "(1.0873336791992188, 0.7130123746836958)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.43      0.55      2528\n",
      "           1       0.59      0.85      0.70     10196\n",
      "           2       0.57      0.84      0.68     15084\n",
      "           3       0.85      0.44      0.58      3100\n",
      "           4       0.72      0.81      0.76      8778\n",
      "           5       0.79      0.56      0.66      3821\n",
      "           6       0.54      0.52      0.53      4123\n",
      "           7       0.91      0.64      0.75      8397\n",
      "           8       0.00      0.00      0.00       498\n",
      "           9       0.84      0.91      0.87     28284\n",
      "          10       0.67      0.76      0.72      1869\n",
      "          11       0.83      0.86      0.84     16721\n",
      "          12       0.87      0.93      0.90     38886\n",
      "          13       0.87      0.72      0.79      1325\n",
      "          14       0.72      0.82      0.77     20859\n",
      "          15       0.56      0.17      0.26      3022\n",
      "          16       0.46      0.80      0.58      7963\n",
      "          17       0.88      0.93      0.90     16743\n",
      "          18       0.74      0.83      0.78      8002\n",
      "          19       0.69      0.73      0.71     10498\n",
      "          20       0.77      0.23      0.36      5940\n",
      "          21       0.76      0.74      0.75      3790\n",
      "          22       0.98      0.34      0.50      2109\n",
      "          23       0.95      0.46      0.62      4038\n",
      "          24       0.62      0.58      0.60      6359\n",
      "          25       0.58      0.73      0.64     12398\n",
      "          26       0.44      0.55      0.49     10438\n",
      "          27       0.87      0.78      0.82      1116\n",
      "          28       0.00      0.00      0.00       337\n",
      "          29       0.73      0.07      0.13      2303\n",
      "          30       0.69      0.63      0.66       781\n",
      "          31       0.75      0.78      0.76      3799\n",
      "          32       0.00      0.00      0.00       339\n",
      "          33       0.97      0.69      0.81      1375\n",
      "          34       0.00      0.00      0.00       772\n",
      "          35       0.71      0.35      0.47      1699\n",
      "          36       0.74      0.69      0.71      4213\n",
      "          37       0.78      0.84      0.81      7251\n",
      "          38       0.92      0.40      0.55      1438\n",
      "          39       0.88      0.82      0.85      6542\n",
      "          40       0.77      0.63      0.69      2287\n",
      "          41       0.98      0.74      0.84       841\n",
      "          42       0.81      0.46      0.59      4512\n",
      "          43       0.67      0.54      0.60      2900\n",
      "          44       0.85      0.70      0.77      6549\n",
      "          45       0.90      0.21      0.34       743\n",
      "          46       0.90      0.45      0.60      5263\n",
      "          47       0.72      0.49      0.58      7736\n",
      "          48       0.00      0.00      0.00       253\n",
      "          49       0.79      0.54      0.64      3708\n",
      "          50       0.87      0.10      0.18       996\n",
      "          51       0.83      0.23      0.36      1580\n",
      "          52       0.79      0.20      0.32      1536\n",
      "          53       0.54      0.61      0.58      5995\n",
      "          54       0.80      0.45      0.57       903\n",
      "          55       0.85      0.65      0.74      5615\n",
      "          56       0.00      0.00      0.00       190\n",
      "          57       1.00      0.26      0.41      1669\n",
      "          58       0.99      0.44      0.61      2190\n",
      "          59       0.92      0.55      0.69      1944\n",
      "          60       0.86      0.48      0.62     11248\n",
      "          61       0.77      0.77      0.77      7226\n",
      "          62       0.00      0.00      0.00       220\n",
      "          63       0.95      0.87      0.91      1981\n",
      "          64       0.00      0.00      0.00        72\n",
      "          65       0.68      0.87      0.76      9338\n",
      "          66       0.38      0.19      0.25      3363\n",
      "          67       0.85      0.75      0.80      4217\n",
      "          68       0.89      0.66      0.76      2607\n",
      "          69       0.00      0.00      0.00       524\n",
      "          70       0.11      0.00      0.00       831\n",
      "          71       1.00      0.00      0.00       736\n",
      "          72       0.00      0.00      0.00        82\n",
      "          73       0.25      0.58      0.35      2845\n",
      "          74       0.79      0.85      0.82      3591\n",
      "          75       0.00      0.00      0.00       387\n",
      "          76       0.93      0.90      0.92      4752\n",
      "          77       0.69      0.76      0.73       365\n",
      "          78       0.65      0.79      0.71      3524\n",
      "          79       0.80      0.72      0.76      4298\n",
      "          80       0.72      0.71      0.71      2651\n",
      "          81       0.69      0.58      0.63      2630\n",
      "          82       0.56      0.89      0.69      7781\n",
      "          83       0.96      0.82      0.88      5993\n",
      "          84       0.92      0.48      0.63      4870\n",
      "          85       0.99      0.89      0.93      5053\n",
      "          86       0.97      0.65      0.78      5993\n",
      "          87       0.87      0.48      0.62      1327\n",
      "          88       0.00      0.00      0.00       130\n",
      "          89       0.88      0.85      0.87     17745\n",
      "          90       0.82      0.56      0.66      3338\n",
      "          91       0.00      0.00      0.00       343\n",
      "          92       0.40      0.88      0.55     10300\n",
      "          93       0.71      0.69      0.70      3246\n",
      "          94       0.52      0.88      0.66      5792\n",
      "          95       0.90      0.35      0.51      3024\n",
      "          96       0.84      0.61      0.71       916\n",
      "          97       0.00      0.00      0.00       947\n",
      "          98       0.97      0.74      0.84      9904\n",
      "          99       0.51      0.75      0.61      2965\n",
      "         100       0.57      0.81      0.67      3081\n",
      "         101       0.52      0.43      0.47      1271\n",
      "         102       0.64      0.70      0.67      6874\n",
      "         103       0.86      0.84      0.85     10058\n",
      "\n",
      "    accuracy                           0.72    523583\n",
      "   macro avg       0.66      0.53      0.56    523583\n",
      "weighted avg       0.75      0.72      0.71    523583\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(test(valid_dataloader, model, loss_fn))\n",
    "validate(valid_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c4567f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del loss_fn\n",
    "del optimizer\n",
    "\n",
    "del train_dataloader\n",
    "del test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d25e66",
   "metadata": {},
   "source": [
    "# Parameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d980415",
   "metadata": {},
   "source": [
    "Parameter to test: Learning Rate, Batch Size, Layer Nodes, Activation Function, Dropout \\\n",
    "Activation Function: relu, sigmoid, linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a0027-83da-4423-b4a2-b4eac4cc1c23",
   "metadata": {},
   "source": [
    "## Shrink Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d899020-ce61-4e8e-a905-caf58b2096ae",
   "metadata": {},
   "source": [
    "Use only 2% of the data for parameter testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4bd534dd-9764-4e96-9faa-2306e7c1b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_train_bag_small, _, y_train_small = train_test_split(X_train_bag, y_train, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f40982ca-6201-42f0-9a20-7e50d4e0e9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10631\n",
      "10631\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_bag_small))\n",
    "print(len(y_train_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "11c522bc-5a09-4854-bfe0-6abb472151b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10631, 918)\n",
      "         class\n",
      "index         \n",
      "1152728      9\n",
      "1107736     17\n",
      "546778      47\n",
      "528381      14\n",
      "90768       14\n",
      "...        ...\n",
      "1222870    103\n",
      "660811       9\n",
      "822156      14\n",
      "840078       9\n",
      "788215      25\n",
      "\n",
      "[10631 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bag_small.shape)\n",
    "print(y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84814142-1299-4a66-9b60-9dc27d06f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ttrain_small, X_test_small, y_ttrain_small, y_test_small = train_test_split(X_train_bag_small.values, y_train_small.values, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b200778-d09c-47f7-b3cc-7ffd6556ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ttrain_small = y_ttrain_small.reshape(y_ttrain_small.shape[0])\n",
    "y_test_small = y_test_small.reshape(y_test_small.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "62f8f2e1-fc5c-4614-9278-2eadcbcb95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small_tensor = X_to_tensor(X_ttrain_small)\n",
    "y_train_small_tensor = y_to_tensor(y_ttrain_small)\n",
    "\n",
    "X_test_small_tensor = X_to_tensor(X_test_small)\n",
    "y_test_small_tensor = y_to_tensor(y_test_small)\n",
    "\n",
    "train_small_ds = TensorDataset(X_train_small_tensor, y_train_small_tensor)\n",
    "test_small_ds = TensorDataset(X_test_small_tensor, y_test_small_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1acb003-2d45-449e-8717-70bc279b96eb",
   "metadata": {},
   "source": [
    "## Run Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4a2d6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e3a0cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d1eaf4a3-db34-4ee2-9495-9389fb08d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d9c0b67a-101e-4e7e-948d-a9ee6cd7fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_func(loc_pred, loc_y):\n",
    "    # return (loc_pred.argmax(1) == loc_y).type(torch.float).sum().item()\n",
    "    return f1_score(loc_y, loc_pred.argmax(1), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "434f5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNModel import NNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0875ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [len(train_small_ds[0][0]), 250, 164, 164, 104]\n",
    "nnmodel = NNModel(layer, device, acc_func=acc_func, loss_func=nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c25dcf-862c-4147-81b7-463f8d1aeea4",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7a2b703f-0b8e-4f0a-bb47-8973bee25b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Combination (0.001, 320) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (0.001, 640) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.1\n",
      "\n",
      "Parameter Combination (0.001, 1280) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.0\n",
      "\n",
      "Parameter Combination (0.01, 320) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.6\n",
      "\n",
      "Parameter Combination (0.01, 640) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.9\n",
      "\n",
      "Parameter Combination (0.01, 1280) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.6\n",
      "\n",
      "Early stopping at epoch: 40\n",
      "Parameter Combination (0.05, 320) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.2\n",
      "\n",
      "Parameter Combination (0.05, 640) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 1.7\n",
      "\n",
      "Parameter Combination (0.05, 1280) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 1.0\n",
      "\n",
      "Grid search took 1.3 minutes.\n",
      "{'learning_rate': 0.05, 'batch_size': 320}\n"
     ]
    }
   ],
   "source": [
    "test_layer = [[len(train_small_ds[0][0]), 250, 164, 164, 104], [len(train_small_ds[0][0]), 25, 16, 16, 104], [len(train_small_ds[0][0]), 250, 164, 104]]\n",
    "dict_param_1 = {\"learning_rate\": [0.001, 0.01, 0.05], \"batch_size\": [320, 640, 1280]}\n",
    "best, acc = nnmodel.grid_search(dict_param_1, train_small_ds, test_small_ds, epochs=50)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0e0a9683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch: 48\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 4.3\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.4\n",
      "\n",
      "Early stopping at epoch: 45\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 8.3\n",
      "\n",
      "Early stopping at epoch: 49\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.2, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 4.1\n",
      "\n",
      "Early stopping at epoch: 44\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.2, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.7\n",
      "\n",
      "Early stopping at epoch: 43\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.2, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 8.9\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.3, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 3.3\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.3, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.5\n",
      "\n",
      "Early stopping at epoch: 28\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.3, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.6\n",
      "\n",
      "Early stopping at epoch: 30\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.5, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.4\n",
      "\n",
      "Early stopping at epoch: 47\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.5, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.6\n",
      "\n",
      "Early stopping at epoch: 32\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.5, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 3.3\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.2, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.2, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.2, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.3, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.3, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.3, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Early stopping at epoch: 45\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.5, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.5, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Early stopping at epoch: 47\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.5, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Early stopping at epoch: 28\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 4.6\n",
      "\n",
      "Early stopping at epoch: 28\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 3.9\n",
      "\n",
      "Early stopping at epoch: 28\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 10.3\n",
      "\n",
      "Early stopping at epoch: 39\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.2, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 7.6\n",
      "\n",
      "Early stopping at epoch: 43\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.2, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 3.6\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.2, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 14.7\n",
      "\n",
      "Early stopping at epoch: 25\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.3, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 6.8\n",
      "\n",
      "Early stopping at epoch: 17\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.3, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.5\n",
      "\n",
      "Early stopping at epoch: 29\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.3, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 11.5\n",
      "\n",
      "Early stopping at epoch: 38\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.5, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 7.9\n",
      "\n",
      "Early stopping at epoch: 28\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.5, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.2\n",
      "\n",
      "Early stopping at epoch: 24\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.5, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 7.7\n",
      "\n",
      "Grid search took 5.6 minutes.\n",
      "{'activation': <class 'torch.nn.modules.linear.Identity'>, 'dropout': 0.2, 'layer': [918, 250, 164, 104]}\n"
     ]
    }
   ],
   "source": [
    "nnmodel.defaults[\"learning_rate\"] = best[\"learning_rate\"]\n",
    "nnmodel.defaults[\"batch_size\"] = best[\"batch_size\"]\n",
    "dict_param_2 = {\"activation\": [nn.ReLU, nn.Sigmoid, nn.Identity], \"dropout\": [0, 0.2, 0.3, 0.5], \"layer\": test_layer}\n",
    "best, acc = nnmodel.grid_search(dict_param_2, train_small_ds, test_small_ds, epochs=50)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9649b189-f6c2-474e-ae69-727398137a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel.defaults[\"activation\"] = best[\"activation\"]\n",
    "nnmodel.defaults[\"dropout\"] = best[\"dropout\"]\n",
    "nnmodel.defaults[\"layer\"] = best[\"layer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "695931a5-40ef-4ec5-a824-3929a4a32860",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel.defaults = {'learning_rate': 0.05, 'batch_size': 320, 'layer': [918, 250, 164, 104], 'activation': nn.Identity, 'dropout': 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8c2b1c7f-9ab9-43f6-bd54-790ea1b38ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'batch_size': 320, 'layer': [918, 250, 164, 104], 'activation': <class 'torch.nn.modules.linear.Identity'>, 'dropout': 0.2}\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4.659156  [  320/744121]\n",
      "loss: 436.894220  [32320/744121]\n",
      "loss: 855.936353  [64320/744121]\n",
      "loss: 1264.658367  [96320/744121]\n",
      "loss: 1666.793379  [128320/744121]\n",
      "loss: 2061.762254  [160320/744121]\n",
      "loss: 2452.023403  [192320/744121]\n",
      "loss: 2835.468275  [224320/744121]\n",
      "loss: 3214.234700  [256320/744121]\n",
      "loss: 3588.991687  [288320/744121]\n",
      "loss: 3956.158355  [320320/744121]\n",
      "loss: 4316.220711  [352320/744121]\n",
      "loss: 4672.891688  [384320/744121]\n",
      "loss: 5021.162419  [416320/744121]\n",
      "loss: 5367.060753  [448320/744121]\n",
      "loss: 5706.399689  [480320/744121]\n",
      "loss: 6042.238549  [512320/744121]\n",
      "loss: 6374.064678  [544320/744121]\n",
      "loss: 6702.711900  [576320/744121]\n",
      "loss: 7026.497931  [608320/744121]\n",
      "loss: 7348.562871  [640320/744121]\n",
      "loss: 7665.578415  [672320/744121]\n",
      "loss: 7982.876078  [704320/744121]\n",
      "loss: 8295.899903  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 29.6%, Avg loss: 2.749563 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.150975  [  320/744121]\n",
      "loss: 311.960259  [32320/744121]\n",
      "loss: 618.571052  [64320/744121]\n",
      "loss: 923.251068  [96320/744121]\n",
      "loss: 1226.954159  [128320/744121]\n",
      "loss: 1526.833965  [160320/744121]\n",
      "loss: 1824.629219  [192320/744121]\n",
      "loss: 2118.470031  [224320/744121]\n",
      "loss: 2411.913928  [256320/744121]\n",
      "loss: 2704.423791  [288320/744121]\n",
      "loss: 2992.746284  [320320/744121]\n",
      "loss: 3283.578877  [352320/744121]\n",
      "loss: 3568.086270  [384320/744121]\n",
      "loss: 3849.750922  [416320/744121]\n",
      "loss: 4131.547163  [448320/744121]\n",
      "loss: 4408.383186  [480320/744121]\n",
      "loss: 4684.313555  [512320/744121]\n",
      "loss: 4958.986028  [544320/744121]\n",
      "loss: 5231.125142  [576320/744121]\n",
      "loss: 5500.019282  [608320/744121]\n",
      "loss: 5768.434936  [640320/744121]\n",
      "loss: 6034.134576  [672320/744121]\n",
      "loss: 6299.475568  [704320/744121]\n",
      "loss: 6562.744570  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 2.195627 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.628056  [  320/744121]\n",
      "loss: 260.320909  [32320/744121]\n",
      "loss: 517.576765  [64320/744121]\n",
      "loss: 776.887220  [96320/744121]\n",
      "loss: 1034.193627  [128320/744121]\n",
      "loss: 1287.716510  [160320/744121]\n",
      "loss: 1540.648338  [192320/744121]\n",
      "loss: 1792.270275  [224320/744121]\n",
      "loss: 2041.193168  [256320/744121]\n",
      "loss: 2290.175275  [288320/744121]\n",
      "loss: 2534.760939  [320320/744121]\n",
      "loss: 2781.044883  [352320/744121]\n",
      "loss: 3024.338396  [384320/744121]\n",
      "loss: 3267.619348  [416320/744121]\n",
      "loss: 3510.157071  [448320/744121]\n",
      "loss: 3749.518553  [480320/744121]\n",
      "loss: 3989.329541  [512320/744121]\n",
      "loss: 4225.427279  [544320/744121]\n",
      "loss: 4464.740802  [576320/744121]\n",
      "loss: 4701.257307  [608320/744121]\n",
      "loss: 4933.874719  [640320/744121]\n",
      "loss: 5164.634459  [672320/744121]\n",
      "loss: 5397.098853  [704320/744121]\n",
      "loss: 5627.321749  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.856090 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.527749  [  320/744121]\n",
      "loss: 235.361355  [32320/744121]\n",
      "loss: 460.157202  [64320/744121]\n",
      "loss: 687.700781  [96320/744121]\n",
      "loss: 915.165164  [128320/744121]\n",
      "loss: 1138.962033  [160320/744121]\n",
      "loss: 1365.398202  [192320/744121]\n",
      "loss: 1588.744828  [224320/744121]\n",
      "loss: 1812.342625  [256320/744121]\n",
      "loss: 2034.557339  [288320/744121]\n",
      "loss: 2256.580796  [320320/744121]\n",
      "loss: 2479.439098  [352320/744121]\n",
      "loss: 2697.344111  [384320/744121]\n",
      "loss: 2920.171057  [416320/744121]\n",
      "loss: 3140.358675  [448320/744121]\n",
      "loss: 3358.472246  [480320/744121]\n",
      "loss: 3577.825919  [512320/744121]\n",
      "loss: 3790.604648  [544320/744121]\n",
      "loss: 4006.755154  [576320/744121]\n",
      "loss: 4218.455217  [608320/744121]\n",
      "loss: 4431.602122  [640320/744121]\n",
      "loss: 4644.253008  [672320/744121]\n",
      "loss: 4858.282398  [704320/744121]\n",
      "loss: 5070.871136  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.571227 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.092526  [  320/744121]\n",
      "loss: 212.456393  [32320/744121]\n",
      "loss: 422.548051  [64320/744121]\n",
      "loss: 633.387920  [96320/744121]\n",
      "loss: 841.866632  [128320/744121]\n",
      "loss: 1050.854172  [160320/744121]\n",
      "loss: 1258.121761  [192320/744121]\n",
      "loss: 1468.214015  [224320/744121]\n",
      "loss: 1673.557133  [256320/744121]\n",
      "loss: 1880.378500  [288320/744121]\n",
      "loss: 2084.407556  [320320/744121]\n",
      "loss: 2289.519034  [352320/744121]\n",
      "loss: 2493.054919  [384320/744121]\n",
      "loss: 2699.329098  [416320/744121]\n",
      "loss: 2902.153645  [448320/744121]\n",
      "loss: 3107.068219  [480320/744121]\n",
      "loss: 3309.324892  [512320/744121]\n",
      "loss: 3512.226634  [544320/744121]\n",
      "loss: 3712.771403  [576320/744121]\n",
      "loss: 3915.324820  [608320/744121]\n",
      "loss: 4116.096566  [640320/744121]\n",
      "loss: 4316.418885  [672320/744121]\n",
      "loss: 4515.467232  [704320/744121]\n",
      "loss: 4715.278419  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.402566 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.148897  [  320/744121]\n",
      "loss: 200.891894  [32320/744121]\n",
      "loss: 396.200561  [64320/744121]\n",
      "loss: 595.499958  [96320/744121]\n",
      "loss: 793.642801  [128320/744121]\n",
      "loss: 989.978166  [160320/744121]\n",
      "loss: 1187.012218  [192320/744121]\n",
      "loss: 1384.506774  [224320/744121]\n",
      "loss: 1579.052317  [256320/744121]\n",
      "loss: 1776.418086  [288320/744121]\n",
      "loss: 1969.460867  [320320/744121]\n",
      "loss: 2163.674348  [352320/744121]\n",
      "loss: 2355.510491  [384320/744121]\n",
      "loss: 2549.561175  [416320/744121]\n",
      "loss: 2745.138031  [448320/744121]\n",
      "loss: 2940.241396  [480320/744121]\n",
      "loss: 3128.553247  [512320/744121]\n",
      "loss: 3320.106082  [544320/744121]\n",
      "loss: 3513.595558  [576320/744121]\n",
      "loss: 3704.389848  [608320/744121]\n",
      "loss: 3897.790978  [640320/744121]\n",
      "loss: 4088.191900  [672320/744121]\n",
      "loss: 4279.212468  [704320/744121]\n",
      "loss: 4472.778768  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 1.234600 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.774217  [  320/744121]\n",
      "loss: 190.947918  [32320/744121]\n",
      "loss: 378.585300  [64320/744121]\n",
      "loss: 568.609456  [96320/744121]\n",
      "loss: 756.504478  [128320/744121]\n",
      "loss: 943.965744  [160320/744121]\n",
      "loss: 1135.591131  [192320/744121]\n",
      "loss: 1323.527623  [224320/744121]\n",
      "loss: 1510.838430  [256320/744121]\n",
      "loss: 1697.882454  [288320/744121]\n",
      "loss: 1883.643722  [320320/744121]\n",
      "loss: 2070.355786  [352320/744121]\n",
      "loss: 2257.117453  [384320/744121]\n",
      "loss: 2444.023039  [416320/744121]\n",
      "loss: 2630.295847  [448320/744121]\n",
      "loss: 2814.849484  [480320/744121]\n",
      "loss: 2995.789999  [512320/744121]\n",
      "loss: 3181.633062  [544320/744121]\n",
      "loss: 3365.995282  [576320/744121]\n",
      "loss: 3548.791173  [608320/744121]\n",
      "loss: 3734.753619  [640320/744121]\n",
      "loss: 3920.348755  [672320/744121]\n",
      "loss: 4102.701933  [704320/744121]\n",
      "loss: 4287.911925  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 1.160638 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.817006  [  320/744121]\n",
      "loss: 185.777797  [32320/744121]\n",
      "loss: 366.117629  [64320/744121]\n",
      "loss: 547.079239  [96320/744121]\n",
      "loss: 729.403970  [128320/744121]\n",
      "loss: 912.756758  [160320/744121]\n",
      "loss: 1095.622112  [192320/744121]\n",
      "loss: 1277.568425  [224320/744121]\n",
      "loss: 1455.964236  [256320/744121]\n",
      "loss: 1637.418834  [288320/744121]\n",
      "loss: 1821.261193  [320320/744121]\n",
      "loss: 1999.496893  [352320/744121]\n",
      "loss: 2177.520786  [384320/744121]\n",
      "loss: 2358.706604  [416320/744121]\n",
      "loss: 2541.144682  [448320/744121]\n",
      "loss: 2724.108883  [480320/744121]\n",
      "loss: 2904.127211  [512320/744121]\n",
      "loss: 3082.874738  [544320/744121]\n",
      "loss: 3263.285299  [576320/744121]\n",
      "loss: 3443.983912  [608320/744121]\n",
      "loss: 3622.236208  [640320/744121]\n",
      "loss: 3800.513378  [672320/744121]\n",
      "loss: 3976.730705  [704320/744121]\n",
      "loss: 4157.080989  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 1.115655 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.015419  [  320/744121]\n",
      "loss: 180.369657  [32320/744121]\n",
      "loss: 356.493066  [64320/744121]\n",
      "loss: 535.163709  [96320/744121]\n",
      "loss: 713.023264  [128320/744121]\n",
      "loss: 888.662567  [160320/744121]\n",
      "loss: 1066.396752  [192320/744121]\n",
      "loss: 1245.634472  [224320/744121]\n",
      "loss: 1420.700657  [256320/744121]\n",
      "loss: 1600.487459  [288320/744121]\n",
      "loss: 1776.105958  [320320/744121]\n",
      "loss: 1949.636478  [352320/744121]\n",
      "loss: 2125.144799  [384320/744121]\n",
      "loss: 2300.721838  [416320/744121]\n",
      "loss: 2476.427596  [448320/744121]\n",
      "loss: 2652.180532  [480320/744121]\n",
      "loss: 2825.252235  [512320/744121]\n",
      "loss: 3000.339587  [544320/744121]\n",
      "loss: 3175.226056  [576320/744121]\n",
      "loss: 3348.877365  [608320/744121]\n",
      "loss: 3522.271751  [640320/744121]\n",
      "loss: 3695.103589  [672320/744121]\n",
      "loss: 3868.494068  [704320/744121]\n",
      "loss: 4044.956014  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 1.077999 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.857369  [  320/744121]\n",
      "loss: 172.006659  [32320/744121]\n",
      "loss: 344.676223  [64320/744121]\n",
      "loss: 519.264124  [96320/744121]\n",
      "loss: 691.722032  [128320/744121]\n",
      "loss: 863.262795  [160320/744121]\n",
      "loss: 1035.735046  [192320/744121]\n",
      "loss: 1209.280011  [224320/744121]\n",
      "loss: 1380.834721  [256320/744121]\n",
      "loss: 1553.138814  [288320/744121]\n",
      "loss: 1725.967836  [320320/744121]\n",
      "loss: 1895.323441  [352320/744121]\n",
      "loss: 2067.402955  [384320/744121]\n",
      "loss: 2236.983514  [416320/744121]\n",
      "loss: 2409.721695  [448320/744121]\n",
      "loss: 2585.736577  [480320/744121]\n",
      "loss: 2758.024700  [512320/744121]\n",
      "loss: 2929.025591  [544320/744121]\n",
      "loss: 3100.292724  [576320/744121]\n",
      "loss: 3270.914137  [608320/744121]\n",
      "loss: 3442.470224  [640320/744121]\n",
      "loss: 3612.998775  [672320/744121]\n",
      "loss: 3781.331289  [704320/744121]\n",
      "loss: 3952.112229  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.988605 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.860869  [  320/744121]\n",
      "loss: 171.542746  [32320/744121]\n",
      "loss: 339.675238  [64320/744121]\n",
      "loss: 508.702812  [96320/744121]\n",
      "loss: 678.880861  [128320/744121]\n",
      "loss: 848.549750  [160320/744121]\n",
      "loss: 1017.850393  [192320/744121]\n",
      "loss: 1184.695837  [224320/744121]\n",
      "loss: 1352.077511  [256320/744121]\n",
      "loss: 1521.718819  [288320/744121]\n",
      "loss: 1690.235845  [320320/744121]\n",
      "loss: 1856.189891  [352320/744121]\n",
      "loss: 2026.302129  [384320/744121]\n",
      "loss: 2195.532687  [416320/744121]\n",
      "loss: 2364.836331  [448320/744121]\n",
      "loss: 2534.255383  [480320/744121]\n",
      "loss: 2701.515049  [512320/744121]\n",
      "loss: 2868.947346  [544320/744121]\n",
      "loss: 3036.718473  [576320/744121]\n",
      "loss: 3204.375686  [608320/744121]\n",
      "loss: 3372.414187  [640320/744121]\n",
      "loss: 3538.268314  [672320/744121]\n",
      "loss: 3704.941423  [704320/744121]\n",
      "loss: 3874.784293  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.959925 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.817829  [  320/744121]\n",
      "loss: 168.446811  [32320/744121]\n",
      "loss: 335.932941  [64320/744121]\n",
      "loss: 502.411847  [96320/744121]\n",
      "loss: 667.312960  [128320/744121]\n",
      "loss: 832.939069  [160320/744121]\n",
      "loss: 998.524863  [192320/744121]\n",
      "loss: 1165.996113  [224320/744121]\n",
      "loss: 1330.606428  [256320/744121]\n",
      "loss: 1496.971666  [288320/744121]\n",
      "loss: 1661.109514  [320320/744121]\n",
      "loss: 1826.603629  [352320/744121]\n",
      "loss: 1991.994990  [384320/744121]\n",
      "loss: 2156.410163  [416320/744121]\n",
      "loss: 2320.229397  [448320/744121]\n",
      "loss: 2486.697904  [480320/744121]\n",
      "loss: 2650.940857  [512320/744121]\n",
      "loss: 2815.140222  [544320/744121]\n",
      "loss: 2980.469756  [576320/744121]\n",
      "loss: 3145.266949  [608320/744121]\n",
      "loss: 3309.422208  [640320/744121]\n",
      "loss: 3474.130205  [672320/744121]\n",
      "loss: 3638.551235  [704320/744121]\n",
      "loss: 3805.974799  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.932273 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.705344  [  320/744121]\n",
      "loss: 165.442343  [32320/744121]\n",
      "loss: 326.611740  [64320/744121]\n",
      "loss: 492.826870  [96320/744121]\n",
      "loss: 656.891359  [128320/744121]\n",
      "loss: 820.379399  [160320/744121]\n",
      "loss: 983.895100  [192320/744121]\n",
      "loss: 1147.083482  [224320/744121]\n",
      "loss: 1310.197940  [256320/744121]\n",
      "loss: 1472.976769  [288320/744121]\n",
      "loss: 1633.844789  [320320/744121]\n",
      "loss: 1794.683683  [352320/744121]\n",
      "loss: 1959.119059  [384320/744121]\n",
      "loss: 2120.159445  [416320/744121]\n",
      "loss: 2284.969005  [448320/744121]\n",
      "loss: 2449.532082  [480320/744121]\n",
      "loss: 2610.928464  [512320/744121]\n",
      "loss: 2772.569867  [544320/744121]\n",
      "loss: 2935.918290  [576320/744121]\n",
      "loss: 3098.185789  [608320/744121]\n",
      "loss: 3259.523409  [640320/744121]\n",
      "loss: 3422.130162  [672320/744121]\n",
      "loss: 3583.981575  [704320/744121]\n",
      "loss: 3747.683792  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.943418 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.789362  [  320/744121]\n",
      "loss: 159.900909  [32320/744121]\n",
      "loss: 321.341246  [64320/744121]\n",
      "loss: 482.899245  [96320/744121]\n",
      "loss: 643.607016  [128320/744121]\n",
      "loss: 805.745372  [160320/744121]\n",
      "loss: 967.797134  [192320/744121]\n",
      "loss: 1129.308258  [224320/744121]\n",
      "loss: 1287.860881  [256320/744121]\n",
      "loss: 1449.382051  [288320/744121]\n",
      "loss: 1610.391958  [320320/744121]\n",
      "loss: 1769.905151  [352320/744121]\n",
      "loss: 1929.728507  [384320/744121]\n",
      "loss: 2091.398945  [416320/744121]\n",
      "loss: 2252.750180  [448320/744121]\n",
      "loss: 2414.181995  [480320/744121]\n",
      "loss: 2574.223662  [512320/744121]\n",
      "loss: 2732.412809  [544320/744121]\n",
      "loss: 2892.532993  [576320/744121]\n",
      "loss: 3050.913653  [608320/744121]\n",
      "loss: 3208.720518  [640320/744121]\n",
      "loss: 3367.501814  [672320/744121]\n",
      "loss: 3526.839057  [704320/744121]\n",
      "loss: 3686.893712  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.909336 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.693699  [  320/744121]\n",
      "loss: 160.795981  [32320/744121]\n",
      "loss: 319.769075  [64320/744121]\n",
      "loss: 480.690796  [96320/744121]\n",
      "loss: 640.422568  [128320/744121]\n",
      "loss: 801.231042  [160320/744121]\n",
      "loss: 959.167485  [192320/744121]\n",
      "loss: 1117.688004  [224320/744121]\n",
      "loss: 1277.290852  [256320/744121]\n",
      "loss: 1440.113727  [288320/744121]\n",
      "loss: 1597.695960  [320320/744121]\n",
      "loss: 1755.275906  [352320/744121]\n",
      "loss: 1914.166490  [384320/744121]\n",
      "loss: 2071.243273  [416320/744121]\n",
      "loss: 2230.367759  [448320/744121]\n",
      "loss: 2391.595647  [480320/744121]\n",
      "loss: 2548.684271  [512320/744121]\n",
      "loss: 2706.729770  [544320/744121]\n",
      "loss: 2865.623686  [576320/744121]\n",
      "loss: 3023.773246  [608320/744121]\n",
      "loss: 3180.721785  [640320/744121]\n",
      "loss: 3339.199573  [672320/744121]\n",
      "loss: 3497.245964  [704320/744121]\n",
      "loss: 3657.558573  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.892868 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.939390  [  320/744121]\n",
      "loss: 159.062626  [32320/744121]\n",
      "loss: 316.467671  [64320/744121]\n",
      "loss: 474.239474  [96320/744121]\n",
      "loss: 631.813017  [128320/744121]\n",
      "loss: 790.277346  [160320/744121]\n",
      "loss: 946.798351  [192320/744121]\n",
      "loss: 1101.829432  [224320/744121]\n",
      "loss: 1259.048197  [256320/744121]\n",
      "loss: 1417.212929  [288320/744121]\n",
      "loss: 1574.250873  [320320/744121]\n",
      "loss: 1729.879703  [352320/744121]\n",
      "loss: 1887.964518  [384320/744121]\n",
      "loss: 2047.705954  [416320/744121]\n",
      "loss: 2205.354166  [448320/744121]\n",
      "loss: 2363.600619  [480320/744121]\n",
      "loss: 2519.099000  [512320/744121]\n",
      "loss: 2678.736919  [544320/744121]\n",
      "loss: 2835.773838  [576320/744121]\n",
      "loss: 2990.394950  [608320/744121]\n",
      "loss: 3148.783676  [640320/744121]\n",
      "loss: 3303.818333  [672320/744121]\n",
      "loss: 3460.045519  [704320/744121]\n",
      "loss: 3618.050760  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.871485 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.601432  [  320/744121]\n",
      "loss: 157.750721  [32320/744121]\n",
      "loss: 313.319691  [64320/744121]\n",
      "loss: 470.187852  [96320/744121]\n",
      "loss: 625.411995  [128320/744121]\n",
      "loss: 782.817235  [160320/744121]\n",
      "loss: 939.732029  [192320/744121]\n",
      "loss: 1095.426975  [224320/744121]\n",
      "loss: 1250.122291  [256320/744121]\n",
      "loss: 1407.137181  [288320/744121]\n",
      "loss: 1562.889946  [320320/744121]\n",
      "loss: 1717.451752  [352320/744121]\n",
      "loss: 1873.160704  [384320/744121]\n",
      "loss: 2026.987963  [416320/744121]\n",
      "loss: 2182.549291  [448320/744121]\n",
      "loss: 2338.800368  [480320/744121]\n",
      "loss: 2493.747231  [512320/744121]\n",
      "loss: 2648.697188  [544320/744121]\n",
      "loss: 2804.060695  [576320/744121]\n",
      "loss: 2959.217492  [608320/744121]\n",
      "loss: 3114.193509  [640320/744121]\n",
      "loss: 3269.169666  [672320/744121]\n",
      "loss: 3423.941430  [704320/744121]\n",
      "loss: 3579.629653  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.837765 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.679035  [  320/744121]\n",
      "loss: 157.075390  [32320/744121]\n",
      "loss: 312.501134  [64320/744121]\n",
      "loss: 469.605462  [96320/744121]\n",
      "loss: 625.624256  [128320/744121]\n",
      "loss: 782.698728  [160320/744121]\n",
      "loss: 938.362937  [192320/744121]\n",
      "loss: 1093.772018  [224320/744121]\n",
      "loss: 1247.807066  [256320/744121]\n",
      "loss: 1401.285175  [288320/744121]\n",
      "loss: 1554.607969  [320320/744121]\n",
      "loss: 1707.749042  [352320/744121]\n",
      "loss: 1864.129527  [384320/744121]\n",
      "loss: 2018.925879  [416320/744121]\n",
      "loss: 2174.478260  [448320/744121]\n",
      "loss: 2329.596951  [480320/744121]\n",
      "loss: 2483.559697  [512320/744121]\n",
      "loss: 2639.083425  [544320/744121]\n",
      "loss: 2792.435472  [576320/744121]\n",
      "loss: 2947.524996  [608320/744121]\n",
      "loss: 3101.968892  [640320/744121]\n",
      "loss: 3255.918064  [672320/744121]\n",
      "loss: 3407.865463  [704320/744121]\n",
      "loss: 3562.792637  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.857562 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.645040  [  320/744121]\n",
      "loss: 154.179440  [32320/744121]\n",
      "loss: 308.442562  [64320/744121]\n",
      "loss: 461.783474  [96320/744121]\n",
      "loss: 617.661137  [128320/744121]\n",
      "loss: 772.624441  [160320/744121]\n",
      "loss: 930.473694  [192320/744121]\n",
      "loss: 1083.470055  [224320/744121]\n",
      "loss: 1235.840159  [256320/744121]\n",
      "loss: 1390.633428  [288320/744121]\n",
      "loss: 1541.844468  [320320/744121]\n",
      "loss: 1692.520240  [352320/744121]\n",
      "loss: 1847.196737  [384320/744121]\n",
      "loss: 2001.616362  [416320/744121]\n",
      "loss: 2157.237084  [448320/744121]\n",
      "loss: 2313.193895  [480320/744121]\n",
      "loss: 2465.510688  [512320/744121]\n",
      "loss: 2619.257180  [544320/744121]\n",
      "loss: 2773.150612  [576320/744121]\n",
      "loss: 2926.176435  [608320/744121]\n",
      "loss: 3079.219728  [640320/744121]\n",
      "loss: 3230.773627  [672320/744121]\n",
      "loss: 3381.271471  [704320/744121]\n",
      "loss: 3535.391904  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.822819 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.547897  [  320/744121]\n",
      "loss: 152.024037  [32320/744121]\n",
      "loss: 303.977452  [64320/744121]\n",
      "loss: 458.246551  [96320/744121]\n",
      "loss: 609.578357  [128320/744121]\n",
      "loss: 762.283743  [160320/744121]\n",
      "loss: 915.545366  [192320/744121]\n",
      "loss: 1068.737572  [224320/744121]\n",
      "loss: 1221.635119  [256320/744121]\n",
      "loss: 1373.092309  [288320/744121]\n",
      "loss: 1523.922858  [320320/744121]\n",
      "loss: 1673.429610  [352320/744121]\n",
      "loss: 1824.812921  [384320/744121]\n",
      "loss: 1978.417707  [416320/744121]\n",
      "loss: 2129.101840  [448320/744121]\n",
      "loss: 2281.432376  [480320/744121]\n",
      "loss: 2432.641209  [512320/744121]\n",
      "loss: 2583.702637  [544320/744121]\n",
      "loss: 2736.391446  [576320/744121]\n",
      "loss: 2888.594714  [608320/744121]\n",
      "loss: 3041.119947  [640320/744121]\n",
      "loss: 3192.761366  [672320/744121]\n",
      "loss: 3342.435831  [704320/744121]\n",
      "loss: 3496.952456  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.801158 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.653409  [  320/744121]\n",
      "loss: 152.848171  [32320/744121]\n",
      "loss: 302.840524  [64320/744121]\n",
      "loss: 454.587259  [96320/744121]\n",
      "loss: 606.161424  [128320/744121]\n",
      "loss: 759.525900  [160320/744121]\n",
      "loss: 912.175502  [192320/744121]\n",
      "loss: 1062.682275  [224320/744121]\n",
      "loss: 1213.696690  [256320/744121]\n",
      "loss: 1364.978894  [288320/744121]\n",
      "loss: 1512.208494  [320320/744121]\n",
      "loss: 1662.405082  [352320/744121]\n",
      "loss: 1815.089477  [384320/744121]\n",
      "loss: 1965.167832  [416320/744121]\n",
      "loss: 2116.841923  [448320/744121]\n",
      "loss: 2270.031713  [480320/744121]\n",
      "loss: 2419.562085  [512320/744121]\n",
      "loss: 2571.837775  [544320/744121]\n",
      "loss: 2722.365900  [576320/744121]\n",
      "loss: 2873.059456  [608320/744121]\n",
      "loss: 3024.555072  [640320/744121]\n",
      "loss: 3175.281070  [672320/744121]\n",
      "loss: 3323.569423  [704320/744121]\n",
      "loss: 3475.899159  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.779245 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.532725  [  320/744121]\n",
      "loss: 153.229227  [32320/744121]\n",
      "loss: 301.550844  [64320/744121]\n",
      "loss: 452.796527  [96320/744121]\n",
      "loss: 602.006457  [128320/744121]\n",
      "loss: 752.778748  [160320/744121]\n",
      "loss: 906.161182  [192320/744121]\n",
      "loss: 1055.000311  [224320/744121]\n",
      "loss: 1202.856552  [256320/744121]\n",
      "loss: 1352.632454  [288320/744121]\n",
      "loss: 1502.816318  [320320/744121]\n",
      "loss: 1649.999304  [352320/744121]\n",
      "loss: 1801.319408  [384320/744121]\n",
      "loss: 1949.778599  [416320/744121]\n",
      "loss: 2100.868734  [448320/744121]\n",
      "loss: 2250.623273  [480320/744121]\n",
      "loss: 2400.973830  [512320/744121]\n",
      "loss: 2551.101730  [544320/744121]\n",
      "loss: 2701.434571  [576320/744121]\n",
      "loss: 2850.145661  [608320/744121]\n",
      "loss: 3001.247970  [640320/744121]\n",
      "loss: 3150.290828  [672320/744121]\n",
      "loss: 3299.229862  [704320/744121]\n",
      "loss: 3451.584944  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.786950 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.683182  [  320/744121]\n",
      "loss: 150.328682  [32320/744121]\n",
      "loss: 299.475333  [64320/744121]\n",
      "loss: 451.050028  [96320/744121]\n",
      "loss: 598.978581  [128320/744121]\n",
      "loss: 748.290021  [160320/744121]\n",
      "loss: 899.029194  [192320/744121]\n",
      "loss: 1049.085790  [224320/744121]\n",
      "loss: 1198.150616  [256320/744121]\n",
      "loss: 1347.266938  [288320/744121]\n",
      "loss: 1495.663990  [320320/744121]\n",
      "loss: 1642.940507  [352320/744121]\n",
      "loss: 1792.597764  [384320/744121]\n",
      "loss: 1940.640918  [416320/744121]\n",
      "loss: 2092.173410  [448320/744121]\n",
      "loss: 2243.978962  [480320/744121]\n",
      "loss: 2393.154855  [512320/744121]\n",
      "loss: 2543.232092  [544320/744121]\n",
      "loss: 2692.165367  [576320/744121]\n",
      "loss: 2841.734439  [608320/744121]\n",
      "loss: 2990.916856  [640320/744121]\n",
      "loss: 3139.216010  [672320/744121]\n",
      "loss: 3287.667628  [704320/744121]\n",
      "loss: 3438.967454  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.797146 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.590658  [  320/744121]\n",
      "loss: 150.208863  [32320/744121]\n",
      "loss: 299.994646  [64320/744121]\n",
      "loss: 449.133342  [96320/744121]\n",
      "loss: 600.007442  [128320/744121]\n",
      "loss: 748.157262  [160320/744121]\n",
      "loss: 896.837279  [192320/744121]\n",
      "loss: 1044.706738  [224320/744121]\n",
      "loss: 1192.208593  [256320/744121]\n",
      "loss: 1341.582345  [288320/744121]\n",
      "loss: 1490.233849  [320320/744121]\n",
      "loss: 1638.524011  [352320/744121]\n",
      "loss: 1788.203157  [384320/744121]\n",
      "loss: 1935.902583  [416320/744121]\n",
      "loss: 2085.939956  [448320/744121]\n",
      "loss: 2234.655212  [480320/744121]\n",
      "loss: 2381.919622  [512320/744121]\n",
      "loss: 2530.383428  [544320/744121]\n",
      "loss: 2679.552169  [576320/744121]\n",
      "loss: 2827.147382  [608320/744121]\n",
      "loss: 2976.061415  [640320/744121]\n",
      "loss: 3123.830423  [672320/744121]\n",
      "loss: 3271.914637  [704320/744121]\n",
      "loss: 3422.629583  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.788182 \n",
      "\n",
      "Early stopping at epoch: 23\n",
      "losses [3.600023065265252, 2.849590279885847, 2.443897988041823, 2.203392536959972, 2.048834836493252, 1.9431175595711554, 1.8627344001395274, 1.8061865503585042, 1.7577859845432042, 1.7168600749887482, 1.6836142714230644, 1.6537674807928189, 1.6280999023752385, 1.602251498943142, 1.5897661162058239, 1.5726645948655218, 1.555785607953371, 1.5481493858458764, 1.5364188482714232, 1.5202296982012427, 1.5102089917157502, 1.4999738739978006, 1.494551736839045, 1.4875063062329181]\n",
      "test_losses [2.74956303398492, 2.1956270584012705, 1.8560901269271834, 1.5712265907343077, 1.4025663900279712, 1.2346003745479832, 1.1606378928947831, 1.115655464537763, 1.0779986362997722, 0.988604577816836, 0.959925207773685, 0.9322732291824242, 0.9434183138304034, 0.9093358013909703, 0.8928684531267332, 0.8714854109849232, 0.8377651733644269, 0.8575618637122266, 0.8228187769681305, 0.801157515828564, 0.7792450504054277, 0.7869504034339844, 0.7971457117419305, 0.788181650722278]\n",
      "accs [0.2958827596036704, 0.43686178442300644, 0.5237060970846292, 0.6027107345058929, 0.6522236100701702, 0.695456546010879, 0.7151007075194619, 0.7226909539082134, 0.7347321096197608, 0.7623873181289355, 0.7629878454842158, 0.7705489591676731, 0.7695740070675569, 0.7755608454122737, 0.7778403082620372, 0.7805061204285394, 0.7957850460751494, 0.7900233120453177, 0.7931067555720999, 0.8039838457645704, 0.8068875696773528, 0.8038885103832143, 0.8024836548997458, 0.8055575967473666]\n",
      "0.8055575967473666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(nnmodel.defaults)\n",
    "acc = nnmodel.run(nnmodel.defaults, train_ds, test_ds, 100, out=True, name=\"beer_grid_res\")\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c4dc2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.786533 \n",
      "\n",
      "(0.786532998085022, 0.8127010633546179)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.73      0.83      2528\n",
      "           1       0.81      0.78      0.80     10196\n",
      "           2       0.91      0.77      0.83     15084\n",
      "           3       0.95      0.63      0.76      3100\n",
      "           4       0.72      0.90      0.80      8778\n",
      "           5       0.73      0.82      0.77      3821\n",
      "           6       0.78      0.61      0.69      4123\n",
      "           7       0.77      0.84      0.80      8397\n",
      "           8       0.83      0.15      0.26       498\n",
      "           9       0.92      0.89      0.90     28284\n",
      "          10       0.96      0.90      0.93      1869\n",
      "          11       0.94      0.81      0.87     16721\n",
      "          12       0.93      0.90      0.92     38886\n",
      "          13       0.95      0.73      0.82      1325\n",
      "          14       0.63      0.91      0.75     20859\n",
      "          15       0.54      0.54      0.54      3022\n",
      "          16       0.72      0.85      0.78      7963\n",
      "          17       0.91      0.94      0.93     16743\n",
      "          18       0.85      0.86      0.86      8002\n",
      "          19       0.69      0.85      0.76     10498\n",
      "          20       0.33      0.87      0.48      5940\n",
      "          21       0.97      0.79      0.87      3790\n",
      "          22       0.92      0.55      0.69      2109\n",
      "          23       0.86      0.73      0.79      4038\n",
      "          24       0.37      0.78      0.50      6359\n",
      "          25       0.56      0.84      0.68     12398\n",
      "          26       0.94      0.41      0.57     10438\n",
      "          27       0.94      0.93      0.93      1116\n",
      "          28       0.99      0.39      0.56       337\n",
      "          29       0.73      0.68      0.70      2303\n",
      "          30       1.00      0.92      0.96       781\n",
      "          31       0.79      0.83      0.81      3799\n",
      "          32       0.89      0.05      0.09       339\n",
      "          33       0.97      0.73      0.83      1375\n",
      "          34       0.96      0.76      0.85       772\n",
      "          35       0.86      0.88      0.87      1699\n",
      "          36       0.79      0.75      0.77      4213\n",
      "          37       0.85      0.86      0.85      7251\n",
      "          38       0.82      0.75      0.78      1438\n",
      "          39       0.91      0.78      0.84      6542\n",
      "          40       0.92      0.84      0.88      2287\n",
      "          41       1.00      0.84      0.91       841\n",
      "          42       0.98      0.65      0.78      4512\n",
      "          43       0.78      0.58      0.67      2900\n",
      "          44       0.84      0.85      0.84      6549\n",
      "          45       0.88      0.48      0.62       743\n",
      "          46       0.98      0.59      0.74      5263\n",
      "          47       0.72      0.76      0.74      7736\n",
      "          48       0.00      0.00      0.00       253\n",
      "          49       0.91      0.73      0.81      3708\n",
      "          50       0.73      0.50      0.59       996\n",
      "          51       0.87      0.49      0.63      1580\n",
      "          52       0.79      0.55      0.65      1536\n",
      "          53       0.62      0.72      0.67      5995\n",
      "          54       0.74      0.62      0.67       903\n",
      "          55       0.65      0.81      0.72      5615\n",
      "          56       1.00      0.78      0.88       190\n",
      "          57       0.78      0.64      0.71      1669\n",
      "          58       0.99      0.65      0.78      2190\n",
      "          59       0.96      0.64      0.77      1944\n",
      "          60       0.82      0.70      0.76     11248\n",
      "          61       0.83      0.87      0.85      7226\n",
      "          62       0.95      0.85      0.90       220\n",
      "          63       0.91      0.93      0.92      1981\n",
      "          64       0.00      0.00      0.00        72\n",
      "          65       0.92      0.88      0.90      9338\n",
      "          66       0.87      0.40      0.55      3363\n",
      "          67       0.97      0.88      0.93      4217\n",
      "          68       0.90      0.81      0.85      2607\n",
      "          69       0.95      0.87      0.91       524\n",
      "          70       0.66      0.51      0.58       831\n",
      "          71       1.00      0.47      0.64       736\n",
      "          72       0.00      0.00      0.00        82\n",
      "          73       0.88      0.75      0.81      2845\n",
      "          74       0.96      0.85      0.90      3591\n",
      "          75       1.00      0.56      0.71       387\n",
      "          76       0.95      0.93      0.94      4752\n",
      "          77       0.80      0.86      0.83       365\n",
      "          78       0.80      0.85      0.83      3524\n",
      "          79       0.95      0.83      0.89      4298\n",
      "          80       0.87      0.76      0.82      2651\n",
      "          81       0.86      0.68      0.76      2630\n",
      "          82       0.95      0.88      0.91      7781\n",
      "          83       0.95      0.90      0.93      5993\n",
      "          84       0.74      0.89      0.81      4870\n",
      "          85       0.90      0.94      0.92      5053\n",
      "          86       0.96      0.85      0.90      5993\n",
      "          87       0.92      0.76      0.83      1327\n",
      "          88       0.00      0.00      0.00       130\n",
      "          89       0.85      0.92      0.88     17745\n",
      "          90       0.87      0.67      0.76      3338\n",
      "          91       1.00      0.88      0.94       343\n",
      "          92       0.74      0.83      0.78     10300\n",
      "          93       0.93      0.83      0.88      3246\n",
      "          94       0.95      0.86      0.90      5792\n",
      "          95       0.95      0.69      0.80      3024\n",
      "          96       0.90      0.79      0.84       916\n",
      "          97       0.98      0.27      0.42       947\n",
      "          98       0.99      0.78      0.87      9904\n",
      "          99       0.86      0.82      0.84      2965\n",
      "         100       1.00      0.81      0.89      3081\n",
      "         101       0.59      0.81      0.68      1271\n",
      "         102       0.93      0.76      0.84      6874\n",
      "         103       0.96      0.84      0.90     10058\n",
      "\n",
      "    accuracy                           0.81    523583\n",
      "   macro avg       0.83      0.72      0.75    523583\n",
      "weighted avg       0.84      0.81      0.81    523583\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc = test(valid_dataloader, nnmodel.model, nnmodel.loss_fn)\n",
    "print(acc)\n",
    "validate(valid_dataloader, nnmodel.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3ea3ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_best = nnmodel.defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc879c4b",
   "metadata": {},
   "source": [
    "## Local Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7473052a-2f62-4d08-b18c-583738e7c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel = NNModel(layer, device, acc_func=acc_func, loss_func=nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "298e14b4-5980-4b83-8c62-01ea1b0ff73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch: 48\n",
      "Step 0\n",
      "Best Params, Parameter Combination {'learning_rate': 0.05, 'batch_size': 320}\n",
      " Accuracy: 3.3\n",
      "\n",
      "Parameter Combination (0.03928, 226) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 5.4\n",
      "\n",
      "Parameter Combination (0.03928, 393) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 2.8\n",
      "\n",
      "Early stopping at epoch: 31\n",
      "Parameter Combination (0.06394, 226) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.1\n",
      "\n",
      "Parameter Combination (0.06394, 393) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 4.0\n",
      "\n",
      "Grid search took 0.9 minutes.\n",
      "Step 1\n",
      "Best Params, Parameter Combination {'learning_rate': 0.03928, 'batch_size': 226}\n",
      " Accuracy: 5.4\n",
      "\n",
      "Early stopping at epoch: 42\n",
      "Parameter Combination (0.02879, 166) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.3\n",
      "\n",
      "Parameter Combination (0.02879, 278) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.8\n",
      "\n",
      "Early stopping at epoch: 31\n",
      "Parameter Combination (0.04604, 166) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 4.1\n",
      "\n",
      "Early stopping at epoch: 45\n",
      "Parameter Combination (0.04604, 278) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.5\n",
      "\n",
      "Grid search took 1.0 minutes.\n",
      "Step 2\n",
      "Best Params, Parameter Combination {'learning_rate': 0.03928, 'batch_size': 226}\n",
      " Accuracy: 5.4\n",
      "\n",
      "Parameter Combination (0.02943, 174) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.7\n",
      "\n",
      "Parameter Combination (0.02943, 249) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.6\n",
      "\n",
      "Early stopping at epoch: 35\n",
      "Parameter Combination (0.04661, 174) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 9.9\n",
      "\n",
      "Early stopping at epoch: 39\n",
      "Parameter Combination (0.04661, 249) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 4.0\n",
      "\n",
      "Grid search took 1.0 minutes.\n",
      "Step 3\n",
      "Best Params, Parameter Combination {'learning_rate': 0.04661, 'batch_size': 174}\n",
      " Accuracy: 9.9\n",
      "\n",
      "Early stopping at epoch: 31\n",
      "Parameter Combination (0.03887, 154) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.8\n",
      "\n",
      "Parameter Combination (0.03887, 220) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 2.0\n",
      "\n",
      "Early stopping at epoch: 37\n",
      "Parameter Combination (0.05916, 154) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 5.0\n",
      "\n",
      "Early stopping at epoch: 39\n",
      "Parameter Combination (0.05916, 220) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 10.3\n",
      "\n",
      "Grid search took 1.0 minutes.\n",
      "Step 4\n",
      "Best Params, Parameter Combination {'learning_rate': 0.05916, 'batch_size': 220}\n",
      " Accuracy: 10.3\n",
      "\n",
      "Early stopping at epoch: 31\n",
      "Parameter Combination (0.04494, 170) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 7.0\n",
      "\n",
      "Early stopping at epoch: 46\n",
      "Parameter Combination (0.04494, 258) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.0\n",
      "\n",
      "Early stopping at epoch: 22\n",
      "Parameter Combination (0.07052, 170) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.8\n",
      "\n",
      "Early stopping at epoch: 36\n",
      "Parameter Combination (0.07052, 258) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 2.6\n",
      "\n",
      "Grid search took 0.9 minutes.\n",
      "Local search took 5.0 minutes.\n",
      "{'learning_rate': 0.05916, 'batch_size': 220}\n"
     ]
    }
   ],
   "source": [
    "init_param = {\"learning_rate\": grid_best[\"learning_rate\"], \"batch_size\": grid_best[\"batch_size\"]}\n",
    "best, acc = nnmodel.local_search(init_param, train_small_ds, test_small_ds, steps=5, epochs=50)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "53a55130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Best Params, Parameter Combination {'layer': [918, 250, 164, 104], 'dropout': 0.2}\n",
      " Accuracy: 11.6\n",
      "\n",
      "Early stopping at epoch: 43\n",
      "Parameter Combination ([918, 202, 145, 104], 0.15407) with keys ['layer', 'dropout']\n",
      " Accuracy: 14.2\n",
      "\n",
      "Parameter Combination ([918, 202, 145, 104], 0.25395) with keys ['layer', 'dropout']\n",
      " Accuracy: 9.8\n",
      "\n",
      "Parameter Combination ([918, 309, 200, 104], 0.15407) with keys ['layer', 'dropout']\n",
      " Accuracy: 11.3\n",
      "\n",
      "Parameter Combination ([918, 309, 200, 104], 0.25395) with keys ['layer', 'dropout']\n",
      " Accuracy: 15.2\n",
      "\n",
      "Grid search took 1.0 minutes.\n",
      "Step 1\n",
      "Best Params, Parameter Combination {'layer': [918, 309, 200, 104], 'dropout': 0.25395}\n",
      " Accuracy: 15.2\n",
      "\n",
      "Parameter Combination ([918, 259, 145, 104], 0.19604) with keys ['layer', 'dropout']\n",
      " Accuracy: 10.8\n",
      "\n",
      "Early stopping at epoch: 28\n",
      "Parameter Combination ([918, 259, 145, 104], 0.31174) with keys ['layer', 'dropout']\n",
      " Accuracy: 10.8\n",
      "\n",
      "Parameter Combination ([918, 383, 244, 104], 0.19604) with keys ['layer', 'dropout']\n",
      " Accuracy: 18.2\n",
      "\n",
      "Early stopping at epoch: 24\n",
      "Parameter Combination ([918, 383, 244, 104], 0.31174) with keys ['layer', 'dropout']\n",
      " Accuracy: 7.2\n",
      "\n",
      "Grid search took 1.0 minutes.\n",
      "Step 2\n",
      "Best Params, Parameter Combination {'layer': [918, 383, 244, 104], 'dropout': 0.19604}\n",
      " Accuracy: 18.2\n",
      "\n",
      "Early stopping at epoch: 49\n",
      "Parameter Combination ([918, 316, 196, 104], 0.13805) with keys ['layer', 'dropout']\n",
      " Accuracy: 12.7\n",
      "\n",
      "Early stopping at epoch: 47\n",
      "Parameter Combination ([918, 316, 196, 104], 0.23525) with keys ['layer', 'dropout']\n",
      " Accuracy: 18.4\n",
      "\n",
      "Early stopping at epoch: 42\n",
      "Parameter Combination ([918, 492, 308, 104], 0.13805) with keys ['layer', 'dropout']\n",
      " Accuracy: 11.6\n",
      "\n",
      "Early stopping at epoch: 47\n",
      "Parameter Combination ([918, 492, 308, 104], 0.23525) with keys ['layer', 'dropout']\n",
      " Accuracy: 17.7\n",
      "\n",
      "Grid search took 1.2 minutes.\n",
      "Step 3\n",
      "Best Params, Parameter Combination {'layer': [918, 316, 196, 104], 'dropout': 0.23525}\n",
      " Accuracy: 18.4\n",
      "\n",
      "Early stopping at epoch: 30\n",
      "Parameter Combination ([918, 233, 146, 104], 0.20206) with keys ['layer', 'dropout']\n",
      " Accuracy: 5.6\n",
      "\n",
      "Early stopping at epoch: 39\n",
      "Parameter Combination ([918, 233, 146, 104], 0.27797) with keys ['layer', 'dropout']\n",
      " Accuracy: 10.7\n",
      "\n",
      "Parameter Combination ([918, 357, 250, 104], 0.20206) with keys ['layer', 'dropout']\n",
      " Accuracy: 13.8\n",
      "\n",
      "Early stopping at epoch: 27\n",
      "Parameter Combination ([918, 357, 250, 104], 0.27797) with keys ['layer', 'dropout']\n",
      " Accuracy: 8.4\n",
      "\n",
      "Grid search took 0.9 minutes.\n",
      "Step 4\n",
      "Best Params, Parameter Combination {'layer': [918, 316, 196, 104], 'dropout': 0.23525}\n",
      " Accuracy: 18.4\n",
      "\n",
      "Early stopping at epoch: 45\n",
      "Parameter Combination ([918, 242, 149, 104], 0.19757) with keys ['layer', 'dropout']\n",
      " Accuracy: 11.5\n",
      "\n",
      "Parameter Combination ([918, 242, 149, 104], 0.2731) with keys ['layer', 'dropout']\n",
      " Accuracy: 10.8\n",
      "\n",
      "Parameter Combination ([918, 369, 227, 104], 0.19757) with keys ['layer', 'dropout']\n",
      " Accuracy: 14.3\n",
      "\n",
      "Early stopping at epoch: 31\n",
      "Parameter Combination ([918, 369, 227, 104], 0.2731) with keys ['layer', 'dropout']\n",
      " Accuracy: 9.2\n",
      "\n",
      "Grid search took 1.1 minutes.\n",
      "Local search took 5.5 minutes.\n"
     ]
    }
   ],
   "source": [
    "nnmodel.defaults[\"learning_rate\"] = best[\"learning_rate\"]\n",
    "nnmodel.defaults[\"batch_size\"] = best[\"batch_size\"]\n",
    "init_param = {\"layer\": grid_best[\"layer\"], \"dropout\": grid_best[\"dropout\"]}\n",
    "best, acc = nnmodel.local_search(init_param, train_small_ds, test_small_ds, steps=5, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "536b57d4-03ca-4cf3-822f-c6b03feaf7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel.defaults[\"dropout\"] = best[\"dropout\"]\n",
    "nnmodel.defaults[\"layer\"] = best[\"layer\"]\n",
    "nnmodel.defaults[\"activation\"] = grid_best[\"activation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bb806463-f695-4e3e-b7c5-252bb70be04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05916, 'batch_size': 220, 'layer': [918, 316, 196, 104], 'activation': <class 'torch.nn.modules.linear.Identity'>, 'dropout': 0.23525}\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4.633439  [  220/744121]\n",
      "loss: 434.442266  [22220/744121]\n",
      "loss: 851.367460  [44220/744121]\n",
      "loss: 1257.674482  [66220/744121]\n",
      "loss: 1657.493434  [88220/744121]\n",
      "loss: 2050.411250  [110220/744121]\n",
      "loss: 2437.781193  [132220/744121]\n",
      "loss: 2818.604261  [154220/744121]\n",
      "loss: 3192.540667  [176220/744121]\n",
      "loss: 3559.655570  [198220/744121]\n",
      "loss: 3919.524061  [220220/744121]\n",
      "loss: 4274.038711  [242220/744121]\n",
      "loss: 4624.321822  [264220/744121]\n",
      "loss: 4969.303653  [286220/744121]\n",
      "loss: 5308.135767  [308220/744121]\n",
      "loss: 5645.444393  [330220/744121]\n",
      "loss: 5979.252656  [352220/744121]\n",
      "loss: 6309.299515  [374220/744121]\n",
      "loss: 6634.927290  [396220/744121]\n",
      "loss: 6954.938415  [418220/744121]\n",
      "loss: 7277.023932  [440220/744121]\n",
      "loss: 7593.155709  [462220/744121]\n",
      "loss: 7909.512070  [484220/744121]\n",
      "loss: 8222.135484  [506220/744121]\n",
      "loss: 8531.153265  [528220/744121]\n",
      "loss: 8836.575409  [550220/744121]\n",
      "loss: 9141.032512  [572220/744121]\n",
      "loss: 9441.815862  [594220/744121]\n",
      "loss: 9739.797811  [616220/744121]\n",
      "loss: 10033.007189  [638220/744121]\n",
      "loss: 10329.099484  [660220/744121]\n",
      "loss: 10619.293996  [682220/744121]\n",
      "loss: 10907.522915  [704220/744121]\n",
      "loss: 11198.985705  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.442944 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.900317  [  220/744121]\n",
      "loss: 284.121270  [22220/744121]\n",
      "loss: 566.502120  [44220/744121]\n",
      "loss: 847.269233  [66220/744121]\n",
      "loss: 1123.964904  [88220/744121]\n",
      "loss: 1400.866317  [110220/744121]\n",
      "loss: 1677.933325  [132220/744121]\n",
      "loss: 1951.643295  [154220/744121]\n",
      "loss: 2225.764267  [176220/744121]\n",
      "loss: 2495.209186  [198220/744121]\n",
      "loss: 2762.272203  [220220/744121]\n",
      "loss: 3029.958424  [242220/744121]\n",
      "loss: 3295.007883  [264220/744121]\n",
      "loss: 3558.531216  [286220/744121]\n",
      "loss: 3817.637391  [308220/744121]\n",
      "loss: 4076.105847  [330220/744121]\n",
      "loss: 4336.889904  [352220/744121]\n",
      "loss: 4595.261557  [374220/744121]\n",
      "loss: 4851.957444  [396220/744121]\n",
      "loss: 5107.133535  [418220/744121]\n",
      "loss: 5362.457662  [440220/744121]\n",
      "loss: 5613.828818  [462220/744121]\n",
      "loss: 5866.578957  [484220/744121]\n",
      "loss: 6115.407097  [506220/744121]\n",
      "loss: 6364.449432  [528220/744121]\n",
      "loss: 6609.180234  [550220/744121]\n",
      "loss: 6854.278593  [572220/744121]\n",
      "loss: 7097.710849  [594220/744121]\n",
      "loss: 7339.167792  [616220/744121]\n",
      "loss: 7580.040400  [638220/744121]\n",
      "loss: 7822.011537  [660220/744121]\n",
      "loss: 8061.191496  [682220/744121]\n",
      "loss: 8301.600670  [704220/744121]\n",
      "loss: 8541.218660  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.769107 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.478739  [  220/744121]\n",
      "loss: 236.260252  [22220/744121]\n",
      "loss: 473.051180  [44220/744121]\n",
      "loss: 707.143423  [66220/744121]\n",
      "loss: 940.673311  [88220/744121]\n",
      "loss: 1177.717499  [110220/744121]\n",
      "loss: 1481.440763  [132220/744121]\n",
      "loss: 1716.425593  [154220/744121]\n",
      "loss: 1948.863896  [176220/744121]\n",
      "loss: 2180.704753  [198220/744121]\n",
      "loss: 2410.918308  [220220/744121]\n",
      "loss: 2638.871221  [242220/744121]\n",
      "loss: 2864.854687  [264220/744121]\n",
      "loss: 3090.685490  [286220/744121]\n",
      "loss: 3315.814164  [308220/744121]\n",
      "loss: 3539.772818  [330220/744121]\n",
      "loss: 3765.308547  [352220/744121]\n",
      "loss: 3990.138010  [374220/744121]\n",
      "loss: 4215.629176  [396220/744121]\n",
      "loss: 4440.049632  [418220/744121]\n",
      "loss: 4662.135878  [440220/744121]\n",
      "loss: 4886.038200  [462220/744121]\n",
      "loss: 5109.170536  [484220/744121]\n",
      "loss: 5328.933265  [506220/744121]\n",
      "loss: 5548.283043  [528220/744121]\n",
      "loss: 5767.258729  [550220/744121]\n",
      "loss: 5985.837226  [572220/744121]\n",
      "loss: 6205.595560  [594220/744121]\n",
      "loss: 6424.391969  [616220/744121]\n",
      "loss: 6637.775228  [638220/744121]\n",
      "loss: 6852.181114  [660220/744121]\n",
      "loss: 7067.063101  [682220/744121]\n",
      "loss: 7284.510553  [704220/744121]\n",
      "loss: 7500.393857  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.441718 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.078930  [  220/744121]\n",
      "loss: 217.220657  [22220/744121]\n",
      "loss: 430.235338  [44220/744121]\n",
      "loss: 643.194092  [66220/744121]\n",
      "loss: 855.005914  [88220/744121]\n",
      "loss: 1070.337320  [110220/744121]\n",
      "loss: 1283.126788  [132220/744121]\n",
      "loss: 1496.887794  [154220/744121]\n",
      "loss: 1708.147807  [176220/744121]\n",
      "loss: 1919.044420  [198220/744121]\n",
      "loss: 2131.658188  [220220/744121]\n",
      "loss: 2344.176556  [242220/744121]\n",
      "loss: 2554.925629  [264220/744121]\n",
      "loss: 2764.081932  [286220/744121]\n",
      "loss: 2972.847672  [308220/744121]\n",
      "loss: 3180.434433  [330220/744121]\n",
      "loss: 3390.019786  [352220/744121]\n",
      "loss: 3596.124862  [374220/744121]\n",
      "loss: 3805.847783  [396220/744121]\n",
      "loss: 4012.669410  [418220/744121]\n",
      "loss: 4218.638399  [440220/744121]\n",
      "loss: 4423.935837  [462220/744121]\n",
      "loss: 4634.115819  [484220/744121]\n",
      "loss: 4839.326761  [506220/744121]\n",
      "loss: 5043.573154  [528220/744121]\n",
      "loss: 5246.183942  [550220/744121]\n",
      "loss: 5451.218251  [572220/744121]\n",
      "loss: 5653.743508  [594220/744121]\n",
      "loss: 5855.940346  [616220/744121]\n",
      "loss: 6058.091816  [638220/744121]\n",
      "loss: 6261.605701  [660220/744121]\n",
      "loss: 6461.134274  [682220/744121]\n",
      "loss: 6663.379090  [704220/744121]\n",
      "loss: 6867.446804  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 1.267116 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.267612  [  220/744121]\n",
      "loss: 201.669213  [22220/744121]\n",
      "loss: 405.790997  [44220/744121]\n",
      "loss: 607.463523  [66220/744121]\n",
      "loss: 806.171943  [88220/744121]\n",
      "loss: 1007.821716  [110220/744121]\n",
      "loss: 1205.572787  [132220/744121]\n",
      "loss: 1405.452123  [154220/744121]\n",
      "loss: 1606.889515  [176220/744121]\n",
      "loss: 1807.576338  [198220/744121]\n",
      "loss: 2006.210306  [220220/744121]\n",
      "loss: 2205.980288  [242220/744121]\n",
      "loss: 2404.036709  [264220/744121]\n",
      "loss: 2601.348404  [286220/744121]\n",
      "loss: 2800.038738  [308220/744121]\n",
      "loss: 2994.678989  [330220/744121]\n",
      "loss: 3192.516199  [352220/744121]\n",
      "loss: 3391.587311  [374220/744121]\n",
      "loss: 3590.609679  [396220/744121]\n",
      "loss: 3783.572881  [418220/744121]\n",
      "loss: 3978.886867  [440220/744121]\n",
      "loss: 4174.077579  [462220/744121]\n",
      "loss: 4372.336374  [484220/744121]\n",
      "loss: 4568.989933  [506220/744121]\n",
      "loss: 4763.571472  [528220/744121]\n",
      "loss: 4956.053226  [550220/744121]\n",
      "loss: 5151.534558  [572220/744121]\n",
      "loss: 5345.746877  [594220/744121]\n",
      "loss: 5540.635314  [616220/744121]\n",
      "loss: 5735.128221  [638220/744121]\n",
      "loss: 5928.125030  [660220/744121]\n",
      "loss: 6119.402248  [682220/744121]\n",
      "loss: 6314.111799  [704220/744121]\n",
      "loss: 6510.288009  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.149983 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.030566  [  220/744121]\n",
      "loss: 195.062589  [22220/744121]\n",
      "loss: 387.550963  [44220/744121]\n",
      "loss: 577.342323  [66220/744121]\n",
      "loss: 769.284932  [88220/744121]\n",
      "loss: 962.658608  [110220/744121]\n",
      "loss: 1154.797911  [132220/744121]\n",
      "loss: 1346.467635  [154220/744121]\n",
      "loss: 1541.410303  [176220/744121]\n",
      "loss: 1732.299269  [198220/744121]\n",
      "loss: 1926.295264  [220220/744121]\n",
      "loss: 2118.181143  [242220/744121]\n",
      "loss: 2309.270744  [264220/744121]\n",
      "loss: 2497.835426  [286220/744121]\n",
      "loss: 2686.019532  [308220/744121]\n",
      "loss: 2875.490642  [330220/744121]\n",
      "loss: 3061.575576  [352220/744121]\n",
      "loss: 3249.576904  [374220/744121]\n",
      "loss: 3442.209152  [396220/744121]\n",
      "loss: 3628.374972  [418220/744121]\n",
      "loss: 3816.538058  [440220/744121]\n",
      "loss: 4006.196578  [462220/744121]\n",
      "loss: 4198.648784  [484220/744121]\n",
      "loss: 4386.156667  [506220/744121]\n",
      "loss: 4575.506544  [528220/744121]\n",
      "loss: 4762.383354  [550220/744121]\n",
      "loss: 4949.373014  [572220/744121]\n",
      "loss: 5135.478154  [594220/744121]\n",
      "loss: 5322.721633  [616220/744121]\n",
      "loss: 5509.743046  [638220/744121]\n",
      "loss: 5697.123780  [660220/744121]\n",
      "loss: 5880.939394  [682220/744121]\n",
      "loss: 6069.030110  [704220/744121]\n",
      "loss: 6258.842978  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 1.201309 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.977287  [  220/744121]\n",
      "loss: 187.332942  [22220/744121]\n",
      "loss: 372.103453  [44220/744121]\n",
      "loss: 556.812017  [66220/744121]\n",
      "loss: 742.606936  [88220/744121]\n",
      "loss: 931.401003  [110220/744121]\n",
      "loss: 1117.264283  [132220/744121]\n",
      "loss: 1304.023042  [154220/744121]\n",
      "loss: 1490.948129  [176220/744121]\n",
      "loss: 1674.755094  [198220/744121]\n",
      "loss: 1860.451506  [220220/744121]\n",
      "loss: 2045.340110  [242220/744121]\n",
      "loss: 2230.193851  [264220/744121]\n",
      "loss: 2413.073255  [286220/744121]\n",
      "loss: 2596.703385  [308220/744121]\n",
      "loss: 2779.333757  [330220/744121]\n",
      "loss: 2961.661501  [352220/744121]\n",
      "loss: 3145.737033  [374220/744121]\n",
      "loss: 3331.846465  [396220/744121]\n",
      "loss: 3513.153353  [418220/744121]\n",
      "loss: 3697.739659  [440220/744121]\n",
      "loss: 3882.246387  [462220/744121]\n",
      "loss: 4068.600361  [484220/744121]\n",
      "loss: 4249.350480  [506220/744121]\n",
      "loss: 4433.090773  [528220/744121]\n",
      "loss: 4615.292506  [550220/744121]\n",
      "loss: 4797.469579  [572220/744121]\n",
      "loss: 4979.295137  [594220/744121]\n",
      "loss: 5161.262895  [616220/744121]\n",
      "loss: 5343.000384  [638220/744121]\n",
      "loss: 5524.288258  [660220/744121]\n",
      "loss: 5706.025766  [682220/744121]\n",
      "loss: 5887.477950  [704220/744121]\n",
      "loss: 6071.073875  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 1.033563 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.888414  [  220/744121]\n",
      "loss: 182.980496  [22220/744121]\n",
      "loss: 363.509650  [44220/744121]\n",
      "loss: 545.569701  [66220/744121]\n",
      "loss: 726.139598  [88220/744121]\n",
      "loss: 909.419572  [110220/744121]\n",
      "loss: 1088.203153  [132220/744121]\n",
      "loss: 1267.067371  [154220/744121]\n",
      "loss: 1446.380200  [176220/744121]\n",
      "loss: 1625.653430  [198220/744121]\n",
      "loss: 1805.208058  [220220/744121]\n",
      "loss: 1984.201654  [242220/744121]\n",
      "loss: 2162.819277  [264220/744121]\n",
      "loss: 2343.659253  [286220/744121]\n",
      "loss: 2523.605061  [308220/744121]\n",
      "loss: 2702.014011  [330220/744121]\n",
      "loss: 2880.719976  [352220/744121]\n",
      "loss: 3059.827186  [374220/744121]\n",
      "loss: 3242.813165  [396220/744121]\n",
      "loss: 3422.439990  [418220/744121]\n",
      "loss: 3602.325921  [440220/744121]\n",
      "loss: 3780.882774  [462220/744121]\n",
      "loss: 3963.878910  [484220/744121]\n",
      "loss: 4139.550447  [506220/744121]\n",
      "loss: 4317.380807  [528220/744121]\n",
      "loss: 4493.883793  [550220/744121]\n",
      "loss: 4672.512313  [572220/744121]\n",
      "loss: 4849.304357  [594220/744121]\n",
      "loss: 5029.016443  [616220/744121]\n",
      "loss: 5205.334509  [638220/744121]\n",
      "loss: 5385.372812  [660220/744121]\n",
      "loss: 5560.990253  [682220/744121]\n",
      "loss: 5741.811568  [704220/744121]\n",
      "loss: 5921.738220  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.956300 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.801528  [  220/744121]\n",
      "loss: 179.010831  [22220/744121]\n",
      "loss: 355.357097  [44220/744121]\n",
      "loss: 531.708006  [66220/744121]\n",
      "loss: 709.687175  [88220/744121]\n",
      "loss: 892.673084  [110220/744121]\n",
      "loss: 1069.505561  [132220/744121]\n",
      "loss: 1247.225924  [154220/744121]\n",
      "loss: 1424.413919  [176220/744121]\n",
      "loss: 1599.294340  [198220/744121]\n",
      "loss: 1776.977532  [220220/744121]\n",
      "loss: 1953.144516  [242220/744121]\n",
      "loss: 2129.383568  [264220/744121]\n",
      "loss: 2306.026761  [286220/744121]\n",
      "loss: 2480.222267  [308220/744121]\n",
      "loss: 2653.898341  [330220/744121]\n",
      "loss: 2828.936042  [352220/744121]\n",
      "loss: 3006.355312  [374220/744121]\n",
      "loss: 3184.688140  [396220/744121]\n",
      "loss: 3361.692799  [418220/744121]\n",
      "loss: 3538.480389  [440220/744121]\n",
      "loss: 3717.137098  [462220/744121]\n",
      "loss: 3893.001658  [484220/744121]\n",
      "loss: 4065.104920  [506220/744121]\n",
      "loss: 4240.165609  [528220/744121]\n",
      "loss: 4416.250503  [550220/744121]\n",
      "loss: 4590.462513  [572220/744121]\n",
      "loss: 4765.407142  [594220/744121]\n",
      "loss: 4941.848967  [616220/744121]\n",
      "loss: 5114.763078  [638220/744121]\n",
      "loss: 5290.563130  [660220/744121]\n",
      "loss: 5464.882778  [682220/744121]\n",
      "loss: 5642.869098  [704220/744121]\n",
      "loss: 5818.160244  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 1.002630 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.859191  [  220/744121]\n",
      "loss: 176.080607  [22220/744121]\n",
      "loss: 350.688266  [44220/744121]\n",
      "loss: 522.830254  [66220/744121]\n",
      "loss: 697.302359  [88220/744121]\n",
      "loss: 876.087036  [110220/744121]\n",
      "loss: 1048.602937  [132220/744121]\n",
      "loss: 1223.874390  [154220/744121]\n",
      "loss: 1397.135994  [176220/744121]\n",
      "loss: 1570.804188  [198220/744121]\n",
      "loss: 1745.495025  [220220/744121]\n",
      "loss: 1921.030602  [242220/744121]\n",
      "loss: 2091.136562  [264220/744121]\n",
      "loss: 2263.986277  [286220/744121]\n",
      "loss: 2438.046428  [308220/744121]\n",
      "loss: 2608.564377  [330220/744121]\n",
      "loss: 2780.480622  [352220/744121]\n",
      "loss: 2953.361380  [374220/744121]\n",
      "loss: 3129.344451  [396220/744121]\n",
      "loss: 3301.266054  [418220/744121]\n",
      "loss: 3474.046619  [440220/744121]\n",
      "loss: 3647.117380  [462220/744121]\n",
      "loss: 3824.509300  [484220/744121]\n",
      "loss: 3995.430181  [506220/744121]\n",
      "loss: 4169.519493  [528220/744121]\n",
      "loss: 4341.889558  [550220/744121]\n",
      "loss: 4514.452268  [572220/744121]\n",
      "loss: 4687.536753  [594220/744121]\n",
      "loss: 4861.716945  [616220/744121]\n",
      "loss: 5034.038060  [638220/744121]\n",
      "loss: 5208.053871  [660220/744121]\n",
      "loss: 5377.702746  [682220/744121]\n",
      "loss: 5547.727875  [704220/744121]\n",
      "loss: 5722.819916  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1.057878 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.882211  [  220/744121]\n",
      "loss: 176.701254  [22220/744121]\n",
      "loss: 350.369706  [44220/744121]\n",
      "loss: 522.257857  [66220/744121]\n",
      "loss: 692.057639  [88220/744121]\n",
      "loss: 866.384264  [110220/744121]\n",
      "loss: 1037.982596  [132220/744121]\n",
      "loss: 1213.086834  [154220/744121]\n",
      "loss: 1386.266420  [176220/744121]\n",
      "loss: 1556.973559  [198220/744121]\n",
      "loss: 1729.426589  [220220/744121]\n",
      "loss: 1900.923424  [242220/744121]\n",
      "loss: 2070.682824  [264220/744121]\n",
      "loss: 2243.963723  [286220/744121]\n",
      "loss: 2413.974574  [308220/744121]\n",
      "loss: 2583.938880  [330220/744121]\n",
      "loss: 2753.098370  [352220/744121]\n",
      "loss: 2923.932707  [374220/744121]\n",
      "loss: 3097.367420  [396220/744121]\n",
      "loss: 3268.389794  [418220/744121]\n",
      "loss: 3438.664734  [440220/744121]\n",
      "loss: 3608.192926  [462220/744121]\n",
      "loss: 3782.369975  [484220/744121]\n",
      "loss: 3953.829291  [506220/744121]\n",
      "loss: 4123.820955  [528220/744121]\n",
      "loss: 4294.424726  [550220/744121]\n",
      "loss: 4466.586104  [572220/744121]\n",
      "loss: 4634.342072  [594220/744121]\n",
      "loss: 4803.360366  [616220/744121]\n",
      "loss: 4971.598290  [638220/744121]\n",
      "loss: 5142.944602  [660220/744121]\n",
      "loss: 5311.791050  [682220/744121]\n",
      "loss: 5479.911678  [704220/744121]\n",
      "loss: 5651.347663  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.950632 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.717872  [  220/744121]\n",
      "loss: 171.564977  [22220/744121]\n",
      "loss: 341.003268  [44220/744121]\n",
      "loss: 510.298254  [66220/744121]\n",
      "loss: 682.173378  [88220/744121]\n",
      "loss: 854.593879  [110220/744121]\n",
      "loss: 1024.559026  [132220/744121]\n",
      "loss: 1196.881200  [154220/744121]\n",
      "loss: 1367.942481  [176220/744121]\n",
      "loss: 1536.391640  [198220/744121]\n",
      "loss: 1704.429189  [220220/744121]\n",
      "loss: 1878.202744  [242220/744121]\n",
      "loss: 2048.546557  [264220/744121]\n",
      "loss: 2218.496238  [286220/744121]\n",
      "loss: 2387.127663  [308220/744121]\n",
      "loss: 2555.294667  [330220/744121]\n",
      "loss: 2722.932045  [352220/744121]\n",
      "loss: 2892.319618  [374220/744121]\n",
      "loss: 3063.463109  [396220/744121]\n",
      "loss: 3230.701904  [418220/744121]\n",
      "loss: 3400.234011  [440220/744121]\n",
      "loss: 3570.902586  [462220/744121]\n",
      "loss: 3743.359565  [484220/744121]\n",
      "loss: 3909.583884  [506220/744121]\n",
      "loss: 4079.211946  [528220/744121]\n",
      "loss: 4246.518594  [550220/744121]\n",
      "loss: 4414.434773  [572220/744121]\n",
      "loss: 4582.037363  [594220/744121]\n",
      "loss: 4752.730094  [616220/744121]\n",
      "loss: 4919.648511  [638220/744121]\n",
      "loss: 5088.503591  [660220/744121]\n",
      "loss: 5255.725236  [682220/744121]\n",
      "loss: 5424.833930  [704220/744121]\n",
      "loss: 5595.477189  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.909389 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.894185  [  220/744121]\n",
      "loss: 170.448248  [22220/744121]\n",
      "loss: 339.063862  [44220/744121]\n",
      "loss: 504.304997  [66220/744121]\n",
      "loss: 670.759960  [88220/744121]\n",
      "loss: 840.919459  [110220/744121]\n",
      "loss: 1008.437734  [132220/744121]\n",
      "loss: 1177.714474  [154220/744121]\n",
      "loss: 1345.962641  [176220/744121]\n",
      "loss: 1512.906524  [198220/744121]\n",
      "loss: 1682.421275  [220220/744121]\n",
      "loss: 1849.168553  [242220/744121]\n",
      "loss: 2016.513294  [264220/744121]\n",
      "loss: 2182.210392  [286220/744121]\n",
      "loss: 2347.862650  [308220/744121]\n",
      "loss: 2516.256324  [330220/744121]\n",
      "loss: 2681.509471  [352220/744121]\n",
      "loss: 2849.171597  [374220/744121]\n",
      "loss: 3018.369547  [396220/744121]\n",
      "loss: 3183.577291  [418220/744121]\n",
      "loss: 3351.900362  [440220/744121]\n",
      "loss: 3519.399083  [462220/744121]\n",
      "loss: 3690.299484  [484220/744121]\n",
      "loss: 3855.765050  [506220/744121]\n",
      "loss: 4023.314474  [528220/744121]\n",
      "loss: 4191.270886  [550220/744121]\n",
      "loss: 4358.584948  [572220/744121]\n",
      "loss: 4523.671697  [594220/744121]\n",
      "loss: 4691.528475  [616220/744121]\n",
      "loss: 4857.645580  [638220/744121]\n",
      "loss: 5023.477805  [660220/744121]\n",
      "loss: 5189.054549  [682220/744121]\n",
      "loss: 5356.572301  [704220/744121]\n",
      "loss: 5527.931115  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.909163 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.774300  [  220/744121]\n",
      "loss: 168.820327  [22220/744121]\n",
      "loss: 336.962916  [44220/744121]\n",
      "loss: 500.667288  [66220/744121]\n",
      "loss: 667.605600  [88220/744121]\n",
      "loss: 834.345165  [110220/744121]\n",
      "loss: 997.813736  [132220/744121]\n",
      "loss: 1166.231629  [154220/744121]\n",
      "loss: 1332.207906  [176220/744121]\n",
      "loss: 1498.786976  [198220/744121]\n",
      "loss: 1664.193894  [220220/744121]\n",
      "loss: 1830.807526  [242220/744121]\n",
      "loss: 1995.532203  [264220/744121]\n",
      "loss: 2160.264076  [286220/744121]\n",
      "loss: 2324.902661  [308220/744121]\n",
      "loss: 2490.326325  [330220/744121]\n",
      "loss: 2654.383197  [352220/744121]\n",
      "loss: 2819.775051  [374220/744121]\n",
      "loss: 2985.728965  [396220/744121]\n",
      "loss: 3150.236375  [418220/744121]\n",
      "loss: 3317.508438  [440220/744121]\n",
      "loss: 3485.021107  [462220/744121]\n",
      "loss: 3653.746454  [484220/744121]\n",
      "loss: 3818.602543  [506220/744121]\n",
      "loss: 3985.410493  [528220/744121]\n",
      "loss: 4152.902643  [550220/744121]\n",
      "loss: 4318.897445  [572220/744121]\n",
      "loss: 4484.218875  [594220/744121]\n",
      "loss: 4649.427506  [616220/744121]\n",
      "loss: 4814.638758  [638220/744121]\n",
      "loss: 4978.685765  [660220/744121]\n",
      "loss: 5142.415214  [682220/744121]\n",
      "loss: 5307.565658  [704220/744121]\n",
      "loss: 5475.239744  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.850142 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.783014  [  220/744121]\n",
      "loss: 166.720419  [22220/744121]\n",
      "loss: 331.337483  [44220/744121]\n",
      "loss: 495.642938  [66220/744121]\n",
      "loss: 660.101126  [88220/744121]\n",
      "loss: 826.819073  [110220/744121]\n",
      "loss: 988.972987  [132220/744121]\n",
      "loss: 1156.928011  [154220/744121]\n",
      "loss: 1324.631214  [176220/744121]\n",
      "loss: 1489.507649  [198220/744121]\n",
      "loss: 1654.068168  [220220/744121]\n",
      "loss: 1818.008940  [242220/744121]\n",
      "loss: 1982.528710  [264220/744121]\n",
      "loss: 2146.800348  [286220/744121]\n",
      "loss: 2310.698481  [308220/744121]\n",
      "loss: 2473.511684  [330220/744121]\n",
      "loss: 2637.107484  [352220/744121]\n",
      "loss: 2801.392438  [374220/744121]\n",
      "loss: 2966.235809  [396220/744121]\n",
      "loss: 3130.690887  [418220/744121]\n",
      "loss: 3295.965899  [440220/744121]\n",
      "loss: 3461.184983  [462220/744121]\n",
      "loss: 3627.713535  [484220/744121]\n",
      "loss: 3791.271988  [506220/744121]\n",
      "loss: 3954.992183  [528220/744121]\n",
      "loss: 4119.003968  [550220/744121]\n",
      "loss: 4282.800377  [572220/744121]\n",
      "loss: 4445.787534  [594220/744121]\n",
      "loss: 4611.710844  [616220/744121]\n",
      "loss: 4774.801299  [638220/744121]\n",
      "loss: 4937.940910  [660220/744121]\n",
      "loss: 5101.263238  [682220/744121]\n",
      "loss: 5266.129912  [704220/744121]\n",
      "loss: 5431.482902  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.892122 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.865652  [  220/744121]\n",
      "loss: 166.414207  [22220/744121]\n",
      "loss: 329.257377  [44220/744121]\n",
      "loss: 493.669677  [66220/744121]\n",
      "loss: 657.943222  [88220/744121]\n",
      "loss: 822.568400  [110220/744121]\n",
      "loss: 987.789093  [132220/744121]\n",
      "loss: 1151.441593  [154220/744121]\n",
      "loss: 1314.002704  [176220/744121]\n",
      "loss: 1478.340302  [198220/744121]\n",
      "loss: 1640.270428  [220220/744121]\n",
      "loss: 1804.644636  [242220/744121]\n",
      "loss: 1968.752748  [264220/744121]\n",
      "loss: 2131.059342  [286220/744121]\n",
      "loss: 2293.904441  [308220/744121]\n",
      "loss: 2456.767352  [330220/744121]\n",
      "loss: 2620.604568  [352220/744121]\n",
      "loss: 2784.675870  [374220/744121]\n",
      "loss: 2949.653023  [396220/744121]\n",
      "loss: 3111.035009  [418220/744121]\n",
      "loss: 3274.831713  [440220/744121]\n",
      "loss: 3438.019102  [462220/744121]\n",
      "loss: 3604.730740  [484220/744121]\n",
      "loss: 3766.118458  [506220/744121]\n",
      "loss: 3930.538822  [528220/744121]\n",
      "loss: 4091.918624  [550220/744121]\n",
      "loss: 4255.629851  [572220/744121]\n",
      "loss: 4419.759081  [594220/744121]\n",
      "loss: 4584.244057  [616220/744121]\n",
      "loss: 4744.705158  [638220/744121]\n",
      "loss: 4907.163173  [660220/744121]\n",
      "loss: 5066.584228  [682220/744121]\n",
      "loss: 5229.961420  [704220/744121]\n",
      "loss: 5395.208793  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.903735 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.830740  [  220/744121]\n",
      "loss: 166.829865  [22220/744121]\n",
      "loss: 331.985491  [44220/744121]\n",
      "loss: 494.942639  [66220/744121]\n",
      "loss: 657.717829  [88220/744121]\n",
      "loss: 820.948411  [110220/744121]\n",
      "loss: 982.286214  [132220/744121]\n",
      "loss: 1145.564184  [154220/744121]\n",
      "loss: 1310.319944  [176220/744121]\n",
      "loss: 1472.113987  [198220/744121]\n",
      "loss: 1634.493875  [220220/744121]\n",
      "loss: 1798.312253  [242220/744121]\n",
      "loss: 1960.669200  [264220/744121]\n",
      "loss: 2122.675853  [286220/744121]\n",
      "loss: 2282.974206  [308220/744121]\n",
      "loss: 2444.481493  [330220/744121]\n",
      "loss: 2607.097254  [352220/744121]\n",
      "loss: 2767.439681  [374220/744121]\n",
      "loss: 2930.807676  [396220/744121]\n",
      "loss: 3092.965286  [418220/744121]\n",
      "loss: 3257.015902  [440220/744121]\n",
      "loss: 3419.980409  [462220/744121]\n",
      "loss: 3585.635302  [484220/744121]\n",
      "loss: 3747.945578  [506220/744121]\n",
      "loss: 3910.450212  [528220/744121]\n",
      "loss: 4073.387140  [550220/744121]\n",
      "loss: 4235.682628  [572220/744121]\n",
      "loss: 4396.757325  [594220/744121]\n",
      "loss: 4562.137299  [616220/744121]\n",
      "loss: 4722.947155  [638220/744121]\n",
      "loss: 4887.273504  [660220/744121]\n",
      "loss: 5047.652533  [682220/744121]\n",
      "loss: 5209.057993  [704220/744121]\n",
      "loss: 5370.796181  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.840199 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.745325  [  220/744121]\n",
      "loss: 164.866477  [22220/744121]\n",
      "loss: 326.351631  [44220/744121]\n",
      "loss: 488.067950  [66220/744121]\n",
      "loss: 649.850502  [88220/744121]\n",
      "loss: 812.001853  [110220/744121]\n",
      "loss: 972.754759  [132220/744121]\n",
      "loss: 1135.906917  [154220/744121]\n",
      "loss: 1299.645293  [176220/744121]\n",
      "loss: 1461.837764  [198220/744121]\n",
      "loss: 1621.881830  [220220/744121]\n",
      "loss: 1783.948094  [242220/744121]\n",
      "loss: 1943.625150  [264220/744121]\n",
      "loss: 2104.252816  [286220/744121]\n",
      "loss: 2265.199298  [308220/744121]\n",
      "loss: 2423.512874  [330220/744121]\n",
      "loss: 2584.623008  [352220/744121]\n",
      "loss: 2747.980334  [374220/744121]\n",
      "loss: 2911.395527  [396220/744121]\n",
      "loss: 3072.541616  [418220/744121]\n",
      "loss: 3235.677263  [440220/744121]\n",
      "loss: 3397.606496  [462220/744121]\n",
      "loss: 3560.826318  [484220/744121]\n",
      "loss: 3720.697538  [506220/744121]\n",
      "loss: 3882.007762  [528220/744121]\n",
      "loss: 4040.888314  [550220/744121]\n",
      "loss: 4203.203785  [572220/744121]\n",
      "loss: 4363.112621  [594220/744121]\n",
      "loss: 4524.504316  [616220/744121]\n",
      "loss: 4685.604311  [638220/744121]\n",
      "loss: 4848.102095  [660220/744121]\n",
      "loss: 5007.388941  [682220/744121]\n",
      "loss: 5168.987090  [704220/744121]\n",
      "loss: 5333.159782  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.864556 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.799235  [  220/744121]\n",
      "loss: 163.463377  [22220/744121]\n",
      "loss: 326.193334  [44220/744121]\n",
      "loss: 486.134658  [66220/744121]\n",
      "loss: 649.396569  [88220/744121]\n",
      "loss: 809.643278  [110220/744121]\n",
      "loss: 969.373931  [132220/744121]\n",
      "loss: 1130.290392  [154220/744121]\n",
      "loss: 1291.854466  [176220/744121]\n",
      "loss: 1452.256667  [198220/744121]\n",
      "loss: 1611.754434  [220220/744121]\n",
      "loss: 1776.247132  [242220/744121]\n",
      "loss: 1936.118878  [264220/744121]\n",
      "loss: 2098.063601  [286220/744121]\n",
      "loss: 2259.905304  [308220/744121]\n",
      "loss: 2420.139727  [330220/744121]\n",
      "loss: 2580.813092  [352220/744121]\n",
      "loss: 2743.409989  [374220/744121]\n",
      "loss: 2907.977999  [396220/744121]\n",
      "loss: 3067.841471  [418220/744121]\n",
      "loss: 3229.889691  [440220/744121]\n",
      "loss: 3389.625596  [462220/744121]\n",
      "loss: 3552.762543  [484220/744121]\n",
      "loss: 3713.173358  [506220/744121]\n",
      "loss: 3875.257340  [528220/744121]\n",
      "loss: 4037.153018  [550220/744121]\n",
      "loss: 4197.407152  [572220/744121]\n",
      "loss: 4356.497048  [594220/744121]\n",
      "loss: 4516.917358  [616220/744121]\n",
      "loss: 4673.902509  [638220/744121]\n",
      "loss: 4836.463034  [660220/744121]\n",
      "loss: 4996.090836  [682220/744121]\n",
      "loss: 5156.172598  [704220/744121]\n",
      "loss: 5320.407907  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.793307 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.559292  [  220/744121]\n",
      "loss: 160.919334  [22220/744121]\n",
      "loss: 320.311554  [44220/744121]\n",
      "loss: 479.064828  [66220/744121]\n",
      "loss: 638.826567  [88220/744121]\n",
      "loss: 800.670483  [110220/744121]\n",
      "loss: 958.732666  [132220/744121]\n",
      "loss: 1119.169292  [154220/744121]\n",
      "loss: 1281.624521  [176220/744121]\n",
      "loss: 1442.522388  [198220/744121]\n",
      "loss: 1602.265104  [220220/744121]\n",
      "loss: 1762.152493  [242220/744121]\n",
      "loss: 1919.506824  [264220/744121]\n",
      "loss: 2077.961274  [286220/744121]\n",
      "loss: 2237.981984  [308220/744121]\n",
      "loss: 2395.420203  [330220/744121]\n",
      "loss: 2553.418899  [352220/744121]\n",
      "loss: 2713.777693  [374220/744121]\n",
      "loss: 2874.767255  [396220/744121]\n",
      "loss: 3034.728223  [418220/744121]\n",
      "loss: 3194.590101  [440220/744121]\n",
      "loss: 3355.155884  [462220/744121]\n",
      "loss: 3515.014109  [484220/744121]\n",
      "loss: 3674.467987  [506220/744121]\n",
      "loss: 3835.179082  [528220/744121]\n",
      "loss: 3992.433565  [550220/744121]\n",
      "loss: 4153.246039  [572220/744121]\n",
      "loss: 4313.828039  [594220/744121]\n",
      "loss: 4472.316057  [616220/744121]\n",
      "loss: 4630.567299  [638220/744121]\n",
      "loss: 4790.576063  [660220/744121]\n",
      "loss: 4949.826269  [682220/744121]\n",
      "loss: 5107.942840  [704220/744121]\n",
      "loss: 5269.420416  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.787623 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.005660  [  220/744121]\n",
      "loss: 160.507771  [22220/744121]\n",
      "loss: 321.301475  [44220/744121]\n",
      "loss: 479.478663  [66220/744121]\n",
      "loss: 638.343480  [88220/744121]\n",
      "loss: 798.921104  [110220/744121]\n",
      "loss: 956.436833  [132220/744121]\n",
      "loss: 1115.320516  [154220/744121]\n",
      "loss: 1276.929405  [176220/744121]\n",
      "loss: 1434.859144  [198220/744121]\n",
      "loss: 1594.976642  [220220/744121]\n",
      "loss: 1752.288379  [242220/744121]\n",
      "loss: 1909.367595  [264220/744121]\n",
      "loss: 2067.936929  [286220/744121]\n",
      "loss: 2225.831565  [308220/744121]\n",
      "loss: 2382.821227  [330220/744121]\n",
      "loss: 2542.287342  [352220/744121]\n",
      "loss: 2700.408397  [374220/744121]\n",
      "loss: 2860.392398  [396220/744121]\n",
      "loss: 3021.824965  [418220/744121]\n",
      "loss: 3180.826349  [440220/744121]\n",
      "loss: 3339.917886  [462220/744121]\n",
      "loss: 3502.254627  [484220/744121]\n",
      "loss: 3660.603078  [506220/744121]\n",
      "loss: 3819.628391  [528220/744121]\n",
      "loss: 3979.657482  [550220/744121]\n",
      "loss: 4137.504727  [572220/744121]\n",
      "loss: 4294.053669  [594220/744121]\n",
      "loss: 4452.480883  [616220/744121]\n",
      "loss: 4614.496454  [638220/744121]\n",
      "loss: 4773.763183  [660220/744121]\n",
      "loss: 4931.791740  [682220/744121]\n",
      "loss: 5089.745770  [704220/744121]\n",
      "loss: 5250.116203  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.801048 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.825294  [  220/744121]\n",
      "loss: 162.641236  [22220/744121]\n",
      "loss: 322.807678  [44220/744121]\n",
      "loss: 479.291303  [66220/744121]\n",
      "loss: 640.194244  [88220/744121]\n",
      "loss: 800.811875  [110220/744121]\n",
      "loss: 958.908787  [132220/744121]\n",
      "loss: 1117.342160  [154220/744121]\n",
      "loss: 1276.858838  [176220/744121]\n",
      "loss: 1435.676015  [198220/744121]\n",
      "loss: 1594.230960  [220220/744121]\n",
      "loss: 1754.539122  [242220/744121]\n",
      "loss: 1913.322546  [264220/744121]\n",
      "loss: 2071.426332  [286220/744121]\n",
      "loss: 2228.352878  [308220/744121]\n",
      "loss: 2382.777205  [330220/744121]\n",
      "loss: 2538.727021  [352220/744121]\n",
      "loss: 2696.234718  [374220/744121]\n",
      "loss: 2856.889635  [396220/744121]\n",
      "loss: 3014.286249  [418220/744121]\n",
      "loss: 3172.594156  [440220/744121]\n",
      "loss: 3330.778406  [462220/744121]\n",
      "loss: 3491.288262  [484220/744121]\n",
      "loss: 3650.141832  [506220/744121]\n",
      "loss: 3808.290988  [528220/744121]\n",
      "loss: 3965.375915  [550220/744121]\n",
      "loss: 4124.296967  [572220/744121]\n",
      "loss: 4283.566227  [594220/744121]\n",
      "loss: 4442.314668  [616220/744121]\n",
      "loss: 4601.352781  [638220/744121]\n",
      "loss: 4759.119802  [660220/744121]\n",
      "loss: 4915.671952  [682220/744121]\n",
      "loss: 5072.723484  [704220/744121]\n",
      "loss: 5232.393138  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.799825 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.559977  [  220/744121]\n",
      "loss: 161.049734  [22220/744121]\n",
      "loss: 320.622717  [44220/744121]\n",
      "loss: 477.706085  [66220/744121]\n",
      "loss: 635.136059  [88220/744121]\n",
      "loss: 794.466803  [110220/744121]\n",
      "loss: 951.663977  [132220/744121]\n",
      "loss: 1109.364992  [154220/744121]\n",
      "loss: 1266.929632  [176220/744121]\n",
      "loss: 1425.075763  [198220/744121]\n",
      "loss: 1581.756725  [220220/744121]\n",
      "loss: 1743.292381  [242220/744121]\n",
      "loss: 1899.631949  [264220/744121]\n",
      "loss: 2056.822750  [286220/744121]\n",
      "loss: 2213.512145  [308220/744121]\n",
      "loss: 2369.506283  [330220/744121]\n",
      "loss: 2525.538231  [352220/744121]\n",
      "loss: 2681.632581  [374220/744121]\n",
      "loss: 2838.804520  [396220/744121]\n",
      "loss: 2995.185231  [418220/744121]\n",
      "loss: 3155.646891  [440220/744121]\n",
      "loss: 3312.212694  [462220/744121]\n",
      "loss: 3471.481366  [484220/744121]\n",
      "loss: 3628.904021  [506220/744121]\n",
      "loss: 3786.840599  [528220/744121]\n",
      "loss: 3941.139879  [550220/744121]\n",
      "loss: 4098.637652  [572220/744121]\n",
      "loss: 4256.904996  [594220/744121]\n",
      "loss: 4415.528824  [616220/744121]\n",
      "loss: 4571.666513  [638220/744121]\n",
      "loss: 4730.957505  [660220/744121]\n",
      "loss: 4886.318767  [682220/744121]\n",
      "loss: 5045.263720  [704220/744121]\n",
      "loss: 5204.236811  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.794489 \n",
      "\n",
      "Early stopping at epoch: 22\n",
      "losses [3.380164178842065, 2.5830568063903976, 2.2696209502156726, 2.0795433154093454, 1.9706833609595935, 1.8954651844138546, 1.8387895494109245, 1.7935582865916022, 1.7625047548825734, 1.7334231626624508, 1.7121189114894524, 1.6948991741895887, 1.6747456889796672, 1.6587036357558835, 1.6458249207329476, 1.634489723186397, 1.6274474369433023, 1.6153143120347626, 1.6118642634764742, 1.5963132969557163, 1.590008414676124, 1.584796281527599, 1.5770895988885516]\n",
      "test_losses [2.4429443732623395, 1.769107478076014, 1.4417177043289975, 1.26711595514725, 1.1499833692764414, 1.20130876598687, 1.0335629554452568, 0.9562999813310031, 1.0026304051382788, 1.0578775102105633, 0.9506323423056767, 0.9093891132700033, 0.9091633872328133, 0.850142036306447, 0.8921222205408689, 0.9037353752399313, 0.8401990850218412, 0.864556349269275, 0.7933070587289744, 0.787623092223858, 0.8010476236918876, 0.7998254047591111, 0.7944889641219172]\n",
      "accs [0.3907628925042131, 0.5754105851777195, 0.6616048830662618, 0.6906831074894089, 0.7209953122251743, 0.7064824485283548, 0.7557992508660126, 0.7705750202685473, 0.7689486664964573, 0.7497531950180439, 0.7775676253735351, 0.7808177960696986, 0.7886990160204432, 0.7913460604658835, 0.7771038934608488, 0.7796005256431523, 0.7905782210454094, 0.7840100577930755, 0.8030222467669076, 0.8041262798464635, 0.8000992726767837, 0.8013935131644877, 0.8067488123931434]\n",
      "0.8067488123931434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(nnmodel.defaults)\n",
    "acc = nnmodel.run(nnmodel.defaults, train_ds, test_ds, 100, out=True, name=\"beer_local_res\")\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fc5d8bce-d298-4f7e-b22f-62cd394c76f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.791916 \n",
      "\n",
      "(0.791915774345398, 0.8142797309804871)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.80      0.83      2528\n",
      "           1       0.54      0.92      0.68     10196\n",
      "           2       0.92      0.72      0.81     15084\n",
      "           3       0.75      0.78      0.77      3100\n",
      "           4       0.83      0.87      0.85      8778\n",
      "           5       0.83      0.79      0.81      3821\n",
      "           6       0.83      0.63      0.71      4123\n",
      "           7       0.87      0.79      0.83      8397\n",
      "           8       0.39      0.50      0.44       498\n",
      "           9       0.93      0.88      0.90     28284\n",
      "          10       0.97      0.89      0.93      1869\n",
      "          11       0.92      0.84      0.88     16721\n",
      "          12       0.94      0.88      0.91     38886\n",
      "          13       0.93      0.75      0.83      1325\n",
      "          14       0.86      0.82      0.84     20859\n",
      "          15       0.51      0.59      0.55      3022\n",
      "          16       0.58      0.88      0.70      7963\n",
      "          17       0.78      0.95      0.86     16743\n",
      "          18       0.84      0.85      0.84      8002\n",
      "          19       0.63      0.88      0.73     10498\n",
      "          20       0.23      0.87      0.36      5940\n",
      "          21       0.97      0.78      0.86      3790\n",
      "          22       0.94      0.54      0.69      2109\n",
      "          23       0.92      0.71      0.80      4038\n",
      "          24       0.82      0.55      0.66      6359\n",
      "          25       0.74      0.72      0.73     12398\n",
      "          26       0.94      0.46      0.62     10438\n",
      "          27       0.72      0.94      0.82      1116\n",
      "          28       1.00      0.85      0.92       337\n",
      "          29       0.76      0.69      0.72      2303\n",
      "          30       0.99      0.94      0.96       781\n",
      "          31       0.90      0.80      0.85      3799\n",
      "          32       0.60      0.27      0.38       339\n",
      "          33       0.94      0.78      0.85      1375\n",
      "          34       0.92      0.79      0.85       772\n",
      "          35       0.93      0.82      0.87      1699\n",
      "          36       0.86      0.75      0.80      4213\n",
      "          37       0.76      0.90      0.82      7251\n",
      "          38       0.90      0.74      0.81      1438\n",
      "          39       0.79      0.90      0.84      6542\n",
      "          40       0.92      0.86      0.89      2287\n",
      "          41       0.90      0.96      0.93       841\n",
      "          42       1.00      0.60      0.75      4512\n",
      "          43       0.84      0.63      0.72      2900\n",
      "          44       0.77      0.89      0.82      6549\n",
      "          45       0.89      0.44      0.58       743\n",
      "          46       0.92      0.67      0.78      5263\n",
      "          47       0.85      0.67      0.75      7736\n",
      "          48       0.00      0.00      0.00       253\n",
      "          49       0.93      0.69      0.79      3708\n",
      "          50       0.95      0.34      0.50       996\n",
      "          51       0.88      0.42      0.56      1580\n",
      "          52       0.77      0.59      0.67      1536\n",
      "          53       0.73      0.60      0.66      5995\n",
      "          54       0.78      0.50      0.61       903\n",
      "          55       0.90      0.74      0.82      5615\n",
      "          56       1.00      0.89      0.94       190\n",
      "          57       0.84      0.56      0.67      1669\n",
      "          58       0.97      0.75      0.85      2190\n",
      "          59       0.98      0.68      0.80      1944\n",
      "          60       0.60      0.81      0.69     11248\n",
      "          61       0.84      0.85      0.84      7226\n",
      "          62       0.95      0.95      0.95       220\n",
      "          63       1.00      0.92      0.95      1981\n",
      "          64       0.00      0.00      0.00        72\n",
      "          65       0.86      0.88      0.87      9338\n",
      "          66       0.65      0.49      0.56      3363\n",
      "          67       0.77      0.95      0.85      4217\n",
      "          68       0.90      0.83      0.86      2607\n",
      "          69       0.95      0.81      0.87       524\n",
      "          70       0.98      0.43      0.60       831\n",
      "          71       0.97      0.47      0.63       736\n",
      "          72       0.57      0.10      0.17        82\n",
      "          73       0.95      0.72      0.82      2845\n",
      "          74       0.81      0.92      0.86      3591\n",
      "          75       1.00      0.56      0.71       387\n",
      "          76       0.95      0.92      0.93      4752\n",
      "          77       0.79      0.88      0.83       365\n",
      "          78       0.98      0.73      0.83      3524\n",
      "          79       0.95      0.83      0.88      4298\n",
      "          80       0.86      0.79      0.82      2651\n",
      "          81       0.69      0.76      0.73      2630\n",
      "          82       0.96      0.88      0.92      7781\n",
      "          83       0.98      0.90      0.94      5993\n",
      "          84       0.86      0.82      0.84      4870\n",
      "          85       0.97      0.93      0.95      5053\n",
      "          86       0.94      0.89      0.91      5993\n",
      "          87       0.98      0.71      0.82      1327\n",
      "          88       0.00      0.00      0.00       130\n",
      "          89       0.86      0.93      0.89     17745\n",
      "          90       0.80      0.75      0.77      3338\n",
      "          91       1.00      0.89      0.94       343\n",
      "          92       0.82      0.81      0.81     10300\n",
      "          93       0.73      0.88      0.80      3246\n",
      "          94       0.96      0.85      0.90      5792\n",
      "          95       0.95      0.66      0.78      3024\n",
      "          96       0.89      0.79      0.84       916\n",
      "          97       0.92      0.47      0.62       947\n",
      "          98       0.88      0.89      0.89      9904\n",
      "          99       0.91      0.80      0.85      2965\n",
      "         100       0.64      0.92      0.75      3081\n",
      "         101       0.78      0.75      0.77      1271\n",
      "         102       0.94      0.74      0.83      6874\n",
      "         103       0.94      0.86      0.90     10058\n",
      "\n",
      "    accuracy                           0.81    523583\n",
      "   macro avg       0.83      0.73      0.76    523583\n",
      "weighted avg       0.84      0.81      0.81    523583\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc = test(valid_dataloader, nnmodel.model, nnmodel.loss_fn)\n",
    "print(acc)\n",
    "validate(valid_dataloader, nnmodel.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a80e73",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8662d3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest took 6.2 minutes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "rf = RandomForestClassifier(n_estimators=20, max_features=100, random_state=42)  \n",
    "\n",
    "rf.fit(X_train_bag, y_train['class'])\n",
    "end = time.time()\n",
    "print(f\"Random Forest took {round((end - start)/60, 1)} minutes.\")\n",
    "y_prediction = rf.predict(X_valid_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c8b19014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9619831048754448\n",
      "F1-Score: 0.9619926208541042\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94      2528\n",
      "           1       0.90      0.95      0.93     10196\n",
      "           2       0.95      0.97      0.96     15084\n",
      "           3       0.95      0.93      0.94      3100\n",
      "           4       0.98      0.98      0.98      8778\n",
      "           5       0.96      0.96      0.96      3821\n",
      "           6       0.91      0.93      0.92      4123\n",
      "           7       0.97      0.97      0.97      8397\n",
      "           8       0.99      0.94      0.96       498\n",
      "           9       0.98      0.99      0.98     28284\n",
      "          10       0.99      0.98      0.98      1869\n",
      "          11       0.98      0.98      0.98     16721\n",
      "          12       0.98      0.99      0.98     38886\n",
      "          13       0.99      0.97      0.98      1325\n",
      "          14       0.96      0.97      0.96     20859\n",
      "          15       0.91      0.92      0.91      3022\n",
      "          16       0.96      0.96      0.96      7963\n",
      "          17       0.98      0.99      0.98     16743\n",
      "          18       0.96      0.97      0.96      8002\n",
      "          19       0.98      0.98      0.98     10498\n",
      "          20       0.95      0.96      0.96      5940\n",
      "          21       0.99      0.97      0.98      3790\n",
      "          22       0.94      0.90      0.92      2109\n",
      "          23       0.93      0.95      0.94      4038\n",
      "          24       0.93      0.92      0.93      6359\n",
      "          25       0.96      0.95      0.96     12398\n",
      "          26       0.94      0.95      0.95     10438\n",
      "          27       0.98      0.98      0.98      1116\n",
      "          28       0.97      0.97      0.97       337\n",
      "          29       0.94      0.95      0.95      2303\n",
      "          30       1.00      0.99      0.99       781\n",
      "          31       0.96      0.96      0.96      3799\n",
      "          32       0.95      0.87      0.91       339\n",
      "          33       0.95      0.91      0.93      1375\n",
      "          34       0.96      0.92      0.94       772\n",
      "          35       0.97      0.97      0.97      1699\n",
      "          36       0.92      0.89      0.91      4213\n",
      "          37       0.98      0.98      0.98      7251\n",
      "          38       0.94      0.92      0.93      1438\n",
      "          39       0.99      0.99      0.99      6542\n",
      "          40       0.97      0.95      0.96      2287\n",
      "          41       1.00      0.99      0.99       841\n",
      "          42       0.98      0.98      0.98      4512\n",
      "          43       0.89      0.91      0.90      2900\n",
      "          44       0.97      0.97      0.97      6549\n",
      "          45       0.93      0.87      0.90       743\n",
      "          46       0.98      0.95      0.96      5263\n",
      "          47       0.95      0.94      0.94      7736\n",
      "          48       0.92      0.77      0.84       253\n",
      "          49       0.97      0.94      0.96      3708\n",
      "          50       0.90      0.85      0.87       996\n",
      "          51       0.92      0.93      0.92      1580\n",
      "          52       0.90      0.88      0.89      1536\n",
      "          53       0.85      0.89      0.87      5995\n",
      "          54       0.87      0.90      0.89       903\n",
      "          55       0.96      0.96      0.96      5615\n",
      "          56       0.99      0.97      0.98       190\n",
      "          57       0.95      0.95      0.95      1669\n",
      "          58       0.97      0.96      0.97      2190\n",
      "          59       0.95      0.95      0.95      1944\n",
      "          60       0.95      0.94      0.94     11248\n",
      "          61       0.93      0.96      0.94      7226\n",
      "          62       0.99      0.98      0.98       220\n",
      "          63       0.98      0.97      0.97      1981\n",
      "          64       0.71      0.68      0.70        72\n",
      "          65       0.96      0.96      0.96      9338\n",
      "          66       0.93      0.91      0.92      3363\n",
      "          67       0.98      0.97      0.98      4217\n",
      "          68       0.98      0.95      0.96      2607\n",
      "          69       0.96      0.95      0.95       524\n",
      "          70       0.78      0.77      0.77       831\n",
      "          71       0.95      0.89      0.92       736\n",
      "          72       0.83      0.89      0.86        82\n",
      "          73       0.92      0.91      0.91      2845\n",
      "          74       0.97      0.97      0.97      3591\n",
      "          75       0.90      0.91      0.91       387\n",
      "          76       0.98      0.98      0.98      4752\n",
      "          77       0.96      0.95      0.95       365\n",
      "          78       0.96      0.95      0.96      3524\n",
      "          79       0.98      0.98      0.98      4298\n",
      "          80       0.93      0.92      0.92      2651\n",
      "          81       0.92      0.91      0.91      2630\n",
      "          82       0.96      0.96      0.96      7781\n",
      "          83       0.98      0.98      0.98      5993\n",
      "          84       0.98      0.97      0.98      4870\n",
      "          85       0.99      0.99      0.99      5053\n",
      "          86       0.98      0.98      0.98      5993\n",
      "          87       0.96      0.94      0.95      1327\n",
      "          88       0.60      0.67      0.64       130\n",
      "          89       0.98      0.98      0.98     17745\n",
      "          90       0.97      0.96      0.97      3338\n",
      "          91       0.99      0.97      0.98       343\n",
      "          92       0.96      0.97      0.96     10300\n",
      "          93       0.96      0.95      0.95      3246\n",
      "          94       0.98      0.98      0.98      5792\n",
      "          95       0.98      0.95      0.96      3024\n",
      "          96       0.97      0.92      0.94       916\n",
      "          97       0.90      0.87      0.88       947\n",
      "          98       0.98      0.98      0.98      9904\n",
      "          99       0.92      0.92      0.92      2965\n",
      "         100       0.99      0.98      0.98      3081\n",
      "         101       0.98      0.96      0.97      1271\n",
      "         102       0.98      0.97      0.97      6874\n",
      "         103       0.97      0.96      0.97     10058\n",
      "\n",
      "    accuracy                           0.96    523583\n",
      "   macro avg       0.95      0.94      0.94    523583\n",
      "weighted avg       0.96      0.96      0.96    523583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_valid, y_prediction)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "f1 = f1_score(y_valid, y_prediction, average='weighted')\n",
    "print(f'F1-Score: {f1}')\n",
    "\n",
    "print(classification_report(y_valid, y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb26501b-2fde-4fee-9268-ee0de06cd15a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
