{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdeff493",
   "metadata": {},
   "source": [
    "# Load Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d6d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4215c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_bag = pd.read_csv('../data/beer_valid.csv', index_col='index')\n",
    "y_valid = pd.read_csv('../data/beer_target_valid.csv', index_col='index')\n",
    "X_train_bag = pd.read_csv('../data/beer_train.csv', index_col='index')\n",
    "y_train = pd.read_csv('../data/beer_target_train.csv', index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd9b8f4-408c-4b73-b383-5cccda612759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(523583, 918)\n",
      "(523583, 1)\n",
      "(1063030, 918)\n",
      "(1063030, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_valid_bag.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_train_bag.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2757d3",
   "metadata": {},
   "source": [
    "# Find Solution for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4afda021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7ff094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ttrain, X_test, y_ttrain, y_test = train_test_split(X_train_bag.values, y_train.values, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef3a48d-9566-408f-9e70-f5024e802299",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ttrain = y_ttrain.reshape(y_ttrain.shape[0])\n",
    "y_test = y_test.reshape(y_test.shape[0])\n",
    "y_valid = y_valid.values\n",
    "y_valid = y_valid.reshape(y_valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e8e5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.5 3.5 3.  ... 0.  0.  0. ]\n",
      " [2.  2.5 2.5 ... 0.  0.  0. ]\n",
      " [4.  4.  3.5 ... 0.  0.  0. ]\n",
      " ...\n",
      " [4.  4.5 4.  ... 0.  0.  0. ]\n",
      " [5.  4.5 5.  ... 0.  0.  0. ]\n",
      " [4.  4.5 4.  ... 0.  0.  0. ]]\n",
      "[ 80  54  17 ... 100  26  12]\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "print(X_ttrain)\n",
    "print(y_ttrain)\n",
    "print(y_ttrain.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e8573",
   "metadata": {},
   "source": [
    "## Build torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a580683",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.any(np.isnan(X_ttrain))\n",
    "assert not np.any(np.isnan(y_ttrain))\n",
    "assert not np.any(np.isnan(X_test))\n",
    "assert not np.any(np.isnan(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1b970c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# convert a df to tensor to be used in pytorch\n",
    "def X_to_tensor(df):\n",
    "    return torch.from_numpy(df).float().to(device)\n",
    "\n",
    "def y_to_tensor(df):\n",
    "    return torch.from_numpy(df).long().to(device)\n",
    "\n",
    "X_train_tensor = X_to_tensor(X_ttrain)\n",
    "y_train_tensor = y_to_tensor(y_ttrain)\n",
    "\n",
    "X_test_tensor = X_to_tensor(X_test)\n",
    "y_test_tensor = y_to_tensor(y_test)\n",
    "\n",
    "X_valid_tensor = X_to_tensor(X_valid_bag.values)\n",
    "y_valid_tensor = y_to_tensor(y_valid)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "valid_ds = TensorDataset(X_valid_tensor, y_valid_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ea004d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_ttrain\n",
    "del X_test\n",
    "\n",
    "del y_ttrain\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46cc9573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([1280, 918])\n",
      "Shape of y: torch.Size([1280]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1280\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(valid_ds, batch_size=len(valid_ds))\n",
    "\n",
    "for XX, yy in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {XX.shape}\")\n",
    "    print(f\"Shape of y: {yy.shape} {yy.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16463da9",
   "metadata": {},
   "source": [
    "## Creating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0c38c825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=918, out_features=250, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=250, out_features=164, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=164, out_features=164, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=164, out_features=104, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(len(train_ds[0][0]), 250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 164),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(164, 164),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(164, 104)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5300a4",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e95387a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b07e9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (XX, yy) in enumerate(dataloader):\n",
    "        XX, yy = XX.to(device), yy.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(XX)\n",
    "        loss = loss_fn(pred, yy)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss.item()\n",
    "        train_loss += loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            current = (batch + 1) * len(XX)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return test_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a613680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for XX, yy in dataloader:\n",
    "            XX, yy = XX.to(device), yy.to(device)\n",
    "            pred = model(XX)\n",
    "            test_loss += loss_fn(pred, yy).item()\n",
    "            # correct += (pred.argmax(1) == yy).type(torch.float).sum().item()\n",
    "            correct += f1_score(yy, pred.argmax(1), average='weighted')\n",
    "    test_loss /= num_batches\n",
    "    correct /= num_batches\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "20a02e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4.651258  [ 1280/744121]\n",
      "loss: 4.590582  [129280/744121]\n",
      "loss: 4.317019  [257280/744121]\n",
      "loss: 4.137877  [385280/744121]\n",
      "loss: 4.086305  [513280/744121]\n",
      "loss: 4.127110  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 4.100077 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 4.085429  [ 1280/744121]\n",
      "loss: 4.158247  [129280/744121]\n",
      "loss: 4.107521  [257280/744121]\n",
      "loss: 4.067199  [385280/744121]\n",
      "loss: 4.017262  [513280/744121]\n",
      "loss: 4.048603  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 2.8%, Avg loss: 3.987358 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.967106  [ 1280/744121]\n",
      "loss: 4.003461  [129280/744121]\n",
      "loss: 3.889530  [257280/744121]\n",
      "loss: 3.830221  [385280/744121]\n",
      "loss: 3.732576  [513280/744121]\n",
      "loss: 3.799212  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 4.4%, Avg loss: 3.739028 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.723816  [ 1280/744121]\n",
      "loss: 3.792524  [129280/744121]\n",
      "loss: 3.719739  [257280/744121]\n",
      "loss: 3.705720  [385280/744121]\n",
      "loss: 3.608862  [513280/744121]\n",
      "loss: 3.691026  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 4.8%, Avg loss: 3.641111 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.626948  [ 1280/744121]\n",
      "loss: 3.699675  [129280/744121]\n",
      "loss: 3.634194  [257280/744121]\n",
      "loss: 3.615425  [385280/744121]\n",
      "loss: 3.506358  [513280/744121]\n",
      "loss: 3.576427  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 5.3%, Avg loss: 3.513479 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.497752  [ 1280/744121]\n",
      "loss: 3.546732  [129280/744121]\n",
      "loss: 3.523425  [257280/744121]\n",
      "loss: 3.451212  [385280/744121]\n",
      "loss: 3.411854  [513280/744121]\n",
      "loss: 3.433885  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 8.6%, Avg loss: 3.330039 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.314739  [ 1280/744121]\n",
      "loss: 3.364493  [129280/744121]\n",
      "loss: 3.353103  [257280/744121]\n",
      "loss: 3.377033  [385280/744121]\n",
      "loss: 3.193870  [513280/744121]\n",
      "loss: 3.385867  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 12.2%, Avg loss: 3.161570 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.152917  [ 1280/744121]\n",
      "loss: 3.309064  [129280/744121]\n",
      "loss: 3.289496  [257280/744121]\n",
      "loss: 3.201091  [385280/744121]\n",
      "loss: 3.125447  [513280/744121]\n",
      "loss: 3.272403  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 13.8%, Avg loss: 3.054012 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.049296  [ 1280/744121]\n",
      "loss: 3.176075  [129280/744121]\n",
      "loss: 3.130597  [257280/744121]\n",
      "loss: 3.037693  [385280/744121]\n",
      "loss: 3.020436  [513280/744121]\n",
      "loss: 3.009485  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 16.8%, Avg loss: 2.936798 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.933580  [ 1280/744121]\n",
      "loss: 2.973369  [129280/744121]\n",
      "loss: 3.027072  [257280/744121]\n",
      "loss: 2.892781  [385280/744121]\n",
      "loss: 2.771535  [513280/744121]\n",
      "loss: 2.869144  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 3.325247 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 3.354917  [ 1280/744121]\n",
      "loss: 2.868921  [129280/744121]\n",
      "loss: 2.916892  [257280/744121]\n",
      "loss: 2.701389  [385280/744121]\n",
      "loss: 2.689718  [513280/744121]\n",
      "loss: 2.761976  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 24.7%, Avg loss: 2.688850 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.722952  [ 1280/744121]\n",
      "loss: 2.715496  [129280/744121]\n",
      "loss: 2.949215  [257280/744121]\n",
      "loss: 2.622086  [385280/744121]\n",
      "loss: 2.634042  [513280/744121]\n",
      "loss: 2.635727  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 21.1%, Avg loss: 2.668414 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.674315  [ 1280/744121]\n",
      "loss: 2.732476  [129280/744121]\n",
      "loss: 2.894578  [257280/744121]\n",
      "loss: 2.495389  [385280/744121]\n",
      "loss: 2.705320  [513280/744121]\n",
      "loss: 2.579160  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 20.4%, Avg loss: 2.767659 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.780513  [ 1280/744121]\n",
      "loss: 2.848512  [129280/744121]\n",
      "loss: 2.569334  [257280/744121]\n",
      "loss: 2.583691  [385280/744121]\n",
      "loss: 2.408296  [513280/744121]\n",
      "loss: 2.494708  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 22.7%, Avg loss: 2.680668 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.691935  [ 1280/744121]\n",
      "loss: 2.632731  [129280/744121]\n",
      "loss: 2.591770  [257280/744121]\n",
      "loss: 2.439298  [385280/744121]\n",
      "loss: 2.449450  [513280/744121]\n",
      "loss: 2.625123  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 27.3%, Avg loss: 2.494498 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.518387  [ 1280/744121]\n",
      "loss: 2.472095  [129280/744121]\n",
      "loss: 2.445551  [257280/744121]\n",
      "loss: 2.376589  [385280/744121]\n",
      "loss: 2.953111  [513280/744121]\n",
      "loss: 2.563820  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 2.439932 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.485050  [ 1280/744121]\n",
      "loss: 2.358081  [129280/744121]\n",
      "loss: 2.377005  [257280/744121]\n",
      "loss: 2.458892  [385280/744121]\n",
      "loss: 2.250321  [513280/744121]\n",
      "loss: 2.906407  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 28.0%, Avg loss: 2.448769 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.489854  [ 1280/744121]\n",
      "loss: 2.725174  [129280/744121]\n",
      "loss: 2.406743  [257280/744121]\n",
      "loss: 2.211748  [385280/744121]\n",
      "loss: 2.244361  [513280/744121]\n",
      "loss: 2.385278  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.231420 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.276401  [ 1280/744121]\n",
      "loss: 2.262106  [129280/744121]\n",
      "loss: 2.277454  [257280/744121]\n",
      "loss: 2.283778  [385280/744121]\n",
      "loss: 2.158584  [513280/744121]\n",
      "loss: 2.353807  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 32.3%, Avg loss: 2.306085 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.349395  [ 1280/744121]\n",
      "loss: 2.282625  [129280/744121]\n",
      "loss: 2.267539  [257280/744121]\n",
      "loss: 2.269363  [385280/744121]\n",
      "loss: 2.370804  [513280/744121]\n",
      "loss: 2.258930  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 2.243626 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.285863  [ 1280/744121]\n",
      "loss: 2.433129  [129280/744121]\n",
      "loss: 2.137817  [257280/744121]\n",
      "loss: 2.249376  [385280/744121]\n",
      "loss: 2.664761  [513280/744121]\n",
      "loss: 2.309013  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 2.283558 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.295968  [ 1280/744121]\n",
      "loss: 2.104220  [129280/744121]\n",
      "loss: 2.154690  [257280/744121]\n",
      "loss: 2.126766  [385280/744121]\n",
      "loss: 2.028570  [513280/744121]\n",
      "loss: 2.040407  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 2.060076 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.070236  [ 1280/744121]\n",
      "loss: 2.098030  [129280/744121]\n",
      "loss: 2.115726  [257280/744121]\n",
      "loss: 2.131565  [385280/744121]\n",
      "loss: 2.124398  [513280/744121]\n",
      "loss: 1.977930  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.039065 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.055620  [ 1280/744121]\n",
      "loss: 2.010518  [129280/744121]\n",
      "loss: 2.048215  [257280/744121]\n",
      "loss: 2.100155  [385280/744121]\n",
      "loss: 2.217132  [513280/744121]\n",
      "loss: 1.915892  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 1.875333 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.901037  [ 1280/744121]\n",
      "loss: 2.136264  [129280/744121]\n",
      "loss: 2.169703  [257280/744121]\n",
      "loss: 1.941849  [385280/744121]\n",
      "loss: 1.986589  [513280/744121]\n",
      "loss: 1.905368  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.814135 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.829452  [ 1280/744121]\n",
      "loss: 2.149472  [129280/744121]\n",
      "loss: 2.023048  [257280/744121]\n",
      "loss: 1.952145  [385280/744121]\n",
      "loss: 1.840077  [513280/744121]\n",
      "loss: 1.811937  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 1.926282 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.952361  [ 1280/744121]\n",
      "loss: 1.996789  [129280/744121]\n",
      "loss: 1.893419  [257280/744121]\n",
      "loss: 1.874622  [385280/744121]\n",
      "loss: 1.825202  [513280/744121]\n",
      "loss: 1.863714  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.692579 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.692224  [ 1280/744121]\n",
      "loss: 1.923519  [129280/744121]\n",
      "loss: 1.939359  [257280/744121]\n",
      "loss: 1.856462  [385280/744121]\n",
      "loss: 1.999287  [513280/744121]\n",
      "loss: 1.770739  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.908224 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.896492  [ 1280/744121]\n",
      "loss: 1.758042  [129280/744121]\n",
      "loss: 1.815644  [257280/744121]\n",
      "loss: 1.817104  [385280/744121]\n",
      "loss: 1.792496  [513280/744121]\n",
      "loss: 1.732216  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.703376 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.699783  [ 1280/744121]\n",
      "loss: 1.703070  [129280/744121]\n",
      "loss: 1.817430  [257280/744121]\n",
      "loss: 1.708611  [385280/744121]\n",
      "loss: 1.895383  [513280/744121]\n",
      "loss: 1.680089  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 1.670261 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.652557  [ 1280/744121]\n",
      "loss: 1.689869  [129280/744121]\n",
      "loss: 1.665245  [257280/744121]\n",
      "loss: 1.952103  [385280/744121]\n",
      "loss: 1.855299  [513280/744121]\n",
      "loss: 1.607883  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 1.800970 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.781067  [ 1280/744121]\n",
      "loss: 1.654413  [129280/744121]\n",
      "loss: 1.745339  [257280/744121]\n",
      "loss: 1.759028  [385280/744121]\n",
      "loss: 1.876523  [513280/744121]\n",
      "loss: 1.620968  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 1.661073 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.652181  [ 1280/744121]\n",
      "loss: 1.612861  [129280/744121]\n",
      "loss: 1.583827  [257280/744121]\n",
      "loss: 1.761740  [385280/744121]\n",
      "loss: 1.507503  [513280/744121]\n",
      "loss: 1.585119  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.769504 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.738033  [ 1280/744121]\n",
      "loss: 1.497411  [129280/744121]\n",
      "loss: 1.602229  [257280/744121]\n",
      "loss: 1.701464  [385280/744121]\n",
      "loss: 1.717661  [513280/744121]\n",
      "loss: 1.514126  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 1.589629 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.565957  [ 1280/744121]\n",
      "loss: 1.588109  [129280/744121]\n",
      "loss: 1.501085  [257280/744121]\n",
      "loss: 1.563298  [385280/744121]\n",
      "loss: 1.553390  [513280/744121]\n",
      "loss: 1.467806  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 1.481471 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.449361  [ 1280/744121]\n",
      "loss: 1.453676  [129280/744121]\n",
      "loss: 1.611164  [257280/744121]\n",
      "loss: 1.615068  [385280/744121]\n",
      "loss: 1.503071  [513280/744121]\n",
      "loss: 1.723131  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.438995 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.405681  [ 1280/744121]\n",
      "loss: 1.491753  [129280/744121]\n",
      "loss: 1.694549  [257280/744121]\n",
      "loss: 1.673611  [385280/744121]\n",
      "loss: 1.383284  [513280/744121]\n",
      "loss: 1.651650  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.446695 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.414980  [ 1280/744121]\n",
      "loss: 1.467780  [129280/744121]\n",
      "loss: 1.612386  [257280/744121]\n",
      "loss: 1.388851  [385280/744121]\n",
      "loss: 1.418814  [513280/744121]\n",
      "loss: 1.505705  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 1.484966 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.460597  [ 1280/744121]\n",
      "loss: 1.416899  [129280/744121]\n",
      "loss: 1.519604  [257280/744121]\n",
      "loss: 1.444154  [385280/744121]\n",
      "loss: 1.399253  [513280/744121]\n",
      "loss: 1.444656  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.534405 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.496857  [ 1280/744121]\n",
      "loss: 1.332756  [129280/744121]\n",
      "loss: 1.465597  [257280/744121]\n",
      "loss: 1.382678  [385280/744121]\n",
      "loss: 1.227466  [513280/744121]\n",
      "loss: 1.534246  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.370384 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.340230  [ 1280/744121]\n",
      "loss: 1.275710  [129280/744121]\n",
      "loss: 1.335722  [257280/744121]\n",
      "loss: 1.329085  [385280/744121]\n",
      "loss: 1.346545  [513280/744121]\n",
      "loss: 1.407589  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.353548 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.319224  [ 1280/744121]\n",
      "loss: 1.271198  [129280/744121]\n",
      "loss: 1.360277  [257280/744121]\n",
      "loss: 1.251611  [385280/744121]\n",
      "loss: 1.329273  [513280/744121]\n",
      "loss: 1.266451  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.310171 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.267611  [ 1280/744121]\n",
      "loss: 1.251838  [129280/744121]\n",
      "loss: 1.331044  [257280/744121]\n",
      "loss: 1.223629  [385280/744121]\n",
      "loss: 1.197718  [513280/744121]\n",
      "loss: 1.320634  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 1.270015 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.247894  [ 1280/744121]\n",
      "loss: 1.283576  [129280/744121]\n",
      "loss: 1.299755  [257280/744121]\n",
      "loss: 1.287490  [385280/744121]\n",
      "loss: 1.723355  [513280/744121]\n",
      "loss: 1.265820  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 1.234456 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.197347  [ 1280/744121]\n",
      "loss: 1.237251  [129280/744121]\n",
      "loss: 1.251690  [257280/744121]\n",
      "loss: 1.167704  [385280/744121]\n",
      "loss: 1.126021  [513280/744121]\n",
      "loss: 1.247207  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.305547 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.290890  [ 1280/744121]\n",
      "loss: 1.237778  [129280/744121]\n",
      "loss: 1.243236  [257280/744121]\n",
      "loss: 1.153540  [385280/744121]\n",
      "loss: 1.093450  [513280/744121]\n",
      "loss: 1.212135  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 1.161061 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.129540  [ 1280/744121]\n",
      "loss: 1.137988  [129280/744121]\n",
      "loss: 1.206682  [257280/744121]\n",
      "loss: 1.159153  [385280/744121]\n",
      "loss: 1.334928  [513280/744121]\n",
      "loss: 1.226350  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.108510 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.063457  [ 1280/744121]\n",
      "loss: 1.110949  [129280/744121]\n",
      "loss: 1.222622  [257280/744121]\n",
      "loss: 1.154064  [385280/744121]\n",
      "loss: 1.052854  [513280/744121]\n",
      "loss: 1.287485  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.235693 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.213033  [ 1280/744121]\n",
      "loss: 1.119941  [129280/744121]\n",
      "loss: 1.191893  [257280/744121]\n",
      "loss: 1.113473  [385280/744121]\n",
      "loss: 1.056479  [513280/744121]\n",
      "loss: 1.300143  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.127821 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.096814  [ 1280/744121]\n",
      "loss: 1.139087  [129280/744121]\n",
      "loss: 1.168220  [257280/744121]\n",
      "loss: 1.087456  [385280/744121]\n",
      "loss: 1.005570  [513280/744121]\n",
      "loss: 1.076306  [641280/744121]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 1.090746 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "test_losses = []\n",
    "accs = []\n",
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    losses.append(train(train_dataloader, model, loss_fn, optimizer))\n",
    "    test_loss, acc = test(test_dataloader, model, loss_fn)\n",
    "\n",
    "    accs.append(acc)\n",
    "    test_losses.append(test_loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "afbdf1a9-b51d-41e3-bc37-c9f84b57ffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses [4.281891352532246, 4.065095214909294, 3.8438163127276495, 3.687708723176386, 3.587566351972495, 3.457934481171808, 3.3195913341856493, 3.220066019759555, 3.1029259437547925, 2.9684141349956343, 2.8289376898729515, 2.7059300941290316, 2.6310529209084526, 2.5672429607496228, 2.502283347840981, 2.4452972207282415, 2.3849764124224686, 2.328687124235933, 2.284892273932388, 2.2351583557849897, 2.182858495163344, 2.1256918827283013, 2.0576167561344265, 2.0115473100409886, 1.9689460118202, 1.924617920954203, 1.870258873270959, 1.822097195587617, 1.7748977673012776, 1.720839798040816, 1.6776668376119686, 1.641430725141899, 1.5988564878394924, 1.5611950411010034, 1.5238676454193403, 1.4791595827263246, 1.4475888882306023, 1.409057339442145, 1.380136363694758, 1.3526473266562236, 1.3213375022321223, 1.297745835944959, 1.2701060827245418, 1.2479393560042495, 1.2182026284257161, 1.1987428767574613, 1.17598285144547, 1.1567707189784426, 1.1377410856923698, 1.1169076702234262]\n",
      "test_losses [4.100076854705811, 3.987357632637024, 3.7390280866622927, 3.64111119556427, 3.513478786468506, 3.3300394144058227, 3.161569796562195, 3.054012162208557, 2.9367984886169434, 3.3252467527389524, 2.688849844932556, 2.6684139289855957, 2.7676589279174806, 2.6806683607101442, 2.4944982061386107, 2.439932448387146, 2.448769013404846, 2.2314195070266725, 2.3060847234725954, 2.243626292228699, 2.2835580205917356, 2.0600756578445436, 2.039064720153809, 1.8753325562477112, 1.8141350317001343, 1.926282425403595, 1.6925786280632018, 1.908224102973938, 1.7033761358261108, 1.670261182308197, 1.8009704012870789, 1.6610733604431152, 1.7695040049552917, 1.5896293621063233, 1.4814707202911377, 1.4389948887825013, 1.446694731235504, 1.4849659748077393, 1.5344052991867065, 1.3703835663795472, 1.353547622680664, 1.3101714444160462, 1.2700149288177491, 1.2344559707641602, 1.305547010421753, 1.161061282157898, 1.108509889125824, 1.2356929850578309, 1.1278206434249878, 1.09074600481987]\n",
      "accs [0.010437042761835338, 0.028198772394478648, 0.04423288332230629, 0.04824483682970091, 0.05302136157892598, 0.08606997070867796, 0.12167309513683514, 0.13788627300080733, 0.16794524602126631, 0.10016954650831289, 0.24723668173206476, 0.21084409179813265, 0.20415144846354674, 0.22711644441762505, 0.27318361612905323, 0.32992727519670817, 0.2799213234589873, 0.38162112315725455, 0.323141916049001, 0.3718458659490861, 0.3809212711754519, 0.4119224753942955, 0.3946186019346407, 0.4818402320592535, 0.4929450606000866, 0.4502857332717606, 0.5126926822632163, 0.4786505268508655, 0.5235129554542801, 0.5330500634886335, 0.5119495384976323, 0.5418860265466285, 0.5251655163361466, 0.5496240310753169, 0.5964406334745198, 0.6182510915441678, 0.601947760472173, 0.607266260366738, 0.6084837578667187, 0.6260800258645713, 0.6444254350771931, 0.6557599362653631, 0.6549007066391145, 0.6760857510333965, 0.6481493035830286, 0.6878842714043283, 0.7018061078701516, 0.6656607860975026, 0.6980767677151796, 0.7094270968160629]\n"
     ]
    }
   ],
   "source": [
    "print('losses', losses)\n",
    "print('test_losses', test_losses)\n",
    "print('accs', accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5c5aec59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "\n",
    "plt.plot(range(len(accs)), accs)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig(f\"../results/beer_init_nn_acc.png\", bbox_inches=\"tight\")\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(range(len(losses)), losses, label=\"Training\")\n",
    "plt.plot(range(len(test_losses)), test_losses, label=\"Test\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(f\"../results/beer_init_nn_loss.png\", bbox_inches=\"tight\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80978cc",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea5f2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model):\n",
    "    num_batches = len(dataloader)\n",
    "    assert num_batches == 1\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for XX, yy in dataloader:\n",
    "            XX, yy = XX.to(device), yy.to(device)\n",
    "            pred = model(XX)\n",
    "            print(classification_report(yy, pred.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ba7d256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.087334 \n",
      "\n",
      "(1.0873336791992188, 0.7130123746836958)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.43      0.55      2528\n",
      "           1       0.59      0.85      0.70     10196\n",
      "           2       0.57      0.84      0.68     15084\n",
      "           3       0.85      0.44      0.58      3100\n",
      "           4       0.72      0.81      0.76      8778\n",
      "           5       0.79      0.56      0.66      3821\n",
      "           6       0.54      0.52      0.53      4123\n",
      "           7       0.91      0.64      0.75      8397\n",
      "           8       0.00      0.00      0.00       498\n",
      "           9       0.84      0.91      0.87     28284\n",
      "          10       0.67      0.76      0.72      1869\n",
      "          11       0.83      0.86      0.84     16721\n",
      "          12       0.87      0.93      0.90     38886\n",
      "          13       0.87      0.72      0.79      1325\n",
      "          14       0.72      0.82      0.77     20859\n",
      "          15       0.56      0.17      0.26      3022\n",
      "          16       0.46      0.80      0.58      7963\n",
      "          17       0.88      0.93      0.90     16743\n",
      "          18       0.74      0.83      0.78      8002\n",
      "          19       0.69      0.73      0.71     10498\n",
      "          20       0.77      0.23      0.36      5940\n",
      "          21       0.76      0.74      0.75      3790\n",
      "          22       0.98      0.34      0.50      2109\n",
      "          23       0.95      0.46      0.62      4038\n",
      "          24       0.62      0.58      0.60      6359\n",
      "          25       0.58      0.73      0.64     12398\n",
      "          26       0.44      0.55      0.49     10438\n",
      "          27       0.87      0.78      0.82      1116\n",
      "          28       0.00      0.00      0.00       337\n",
      "          29       0.73      0.07      0.13      2303\n",
      "          30       0.69      0.63      0.66       781\n",
      "          31       0.75      0.78      0.76      3799\n",
      "          32       0.00      0.00      0.00       339\n",
      "          33       0.97      0.69      0.81      1375\n",
      "          34       0.00      0.00      0.00       772\n",
      "          35       0.71      0.35      0.47      1699\n",
      "          36       0.74      0.69      0.71      4213\n",
      "          37       0.78      0.84      0.81      7251\n",
      "          38       0.92      0.40      0.55      1438\n",
      "          39       0.88      0.82      0.85      6542\n",
      "          40       0.77      0.63      0.69      2287\n",
      "          41       0.98      0.74      0.84       841\n",
      "          42       0.81      0.46      0.59      4512\n",
      "          43       0.67      0.54      0.60      2900\n",
      "          44       0.85      0.70      0.77      6549\n",
      "          45       0.90      0.21      0.34       743\n",
      "          46       0.90      0.45      0.60      5263\n",
      "          47       0.72      0.49      0.58      7736\n",
      "          48       0.00      0.00      0.00       253\n",
      "          49       0.79      0.54      0.64      3708\n",
      "          50       0.87      0.10      0.18       996\n",
      "          51       0.83      0.23      0.36      1580\n",
      "          52       0.79      0.20      0.32      1536\n",
      "          53       0.54      0.61      0.58      5995\n",
      "          54       0.80      0.45      0.57       903\n",
      "          55       0.85      0.65      0.74      5615\n",
      "          56       0.00      0.00      0.00       190\n",
      "          57       1.00      0.26      0.41      1669\n",
      "          58       0.99      0.44      0.61      2190\n",
      "          59       0.92      0.55      0.69      1944\n",
      "          60       0.86      0.48      0.62     11248\n",
      "          61       0.77      0.77      0.77      7226\n",
      "          62       0.00      0.00      0.00       220\n",
      "          63       0.95      0.87      0.91      1981\n",
      "          64       0.00      0.00      0.00        72\n",
      "          65       0.68      0.87      0.76      9338\n",
      "          66       0.38      0.19      0.25      3363\n",
      "          67       0.85      0.75      0.80      4217\n",
      "          68       0.89      0.66      0.76      2607\n",
      "          69       0.00      0.00      0.00       524\n",
      "          70       0.11      0.00      0.00       831\n",
      "          71       1.00      0.00      0.00       736\n",
      "          72       0.00      0.00      0.00        82\n",
      "          73       0.25      0.58      0.35      2845\n",
      "          74       0.79      0.85      0.82      3591\n",
      "          75       0.00      0.00      0.00       387\n",
      "          76       0.93      0.90      0.92      4752\n",
      "          77       0.69      0.76      0.73       365\n",
      "          78       0.65      0.79      0.71      3524\n",
      "          79       0.80      0.72      0.76      4298\n",
      "          80       0.72      0.71      0.71      2651\n",
      "          81       0.69      0.58      0.63      2630\n",
      "          82       0.56      0.89      0.69      7781\n",
      "          83       0.96      0.82      0.88      5993\n",
      "          84       0.92      0.48      0.63      4870\n",
      "          85       0.99      0.89      0.93      5053\n",
      "          86       0.97      0.65      0.78      5993\n",
      "          87       0.87      0.48      0.62      1327\n",
      "          88       0.00      0.00      0.00       130\n",
      "          89       0.88      0.85      0.87     17745\n",
      "          90       0.82      0.56      0.66      3338\n",
      "          91       0.00      0.00      0.00       343\n",
      "          92       0.40      0.88      0.55     10300\n",
      "          93       0.71      0.69      0.70      3246\n",
      "          94       0.52      0.88      0.66      5792\n",
      "          95       0.90      0.35      0.51      3024\n",
      "          96       0.84      0.61      0.71       916\n",
      "          97       0.00      0.00      0.00       947\n",
      "          98       0.97      0.74      0.84      9904\n",
      "          99       0.51      0.75      0.61      2965\n",
      "         100       0.57      0.81      0.67      3081\n",
      "         101       0.52      0.43      0.47      1271\n",
      "         102       0.64      0.70      0.67      6874\n",
      "         103       0.86      0.84      0.85     10058\n",
      "\n",
      "    accuracy                           0.72    523583\n",
      "   macro avg       0.66      0.53      0.56    523583\n",
      "weighted avg       0.75      0.72      0.71    523583\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(test(valid_dataloader, model, loss_fn))\n",
    "validate(valid_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c4567f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del loss_fn\n",
    "del optimizer\n",
    "\n",
    "del train_dataloader\n",
    "del test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d25e66",
   "metadata": {},
   "source": [
    "# Parameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d980415",
   "metadata": {},
   "source": [
    "Parameter to test: Learning Rate, Batch Size, Layer Nodes, Activation Function, Dropout \\\n",
    "Activation Function: relu, sigmoid, linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a0027-83da-4423-b4a2-b4eac4cc1c23",
   "metadata": {},
   "source": [
    "## Shrink Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d899020-ce61-4e8e-a905-caf58b2096ae",
   "metadata": {},
   "source": [
    "Use only 1% of the data for parameter testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bd534dd-9764-4e96-9faa-2306e7c1b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_train_bag_small, _, y_train_small = train_test_split(X_train_bag, y_train, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f40982ca-6201-42f0-9a20-7e50d4e0e9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10631\n",
      "10631\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_bag_small))\n",
    "print(len(y_train_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11c522bc-5a09-4854-bfe0-6abb472151b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10631, 918)\n",
      "         class\n",
      "index         \n",
      "1152728      9\n",
      "1107736     17\n",
      "546778      47\n",
      "528381      14\n",
      "90768       14\n",
      "...        ...\n",
      "1222870    103\n",
      "660811       9\n",
      "822156      14\n",
      "840078       9\n",
      "788215      25\n",
      "\n",
      "[10631 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bag_small.shape)\n",
    "print(y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84814142-1299-4a66-9b60-9dc27d06f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ttrain_small, X_test_small, y_ttrain_small, y_test_small = train_test_split(X_train_bag_small.values, y_train_small.values, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b200778-d09c-47f7-b3cc-7ffd6556ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ttrain_small = y_ttrain_small.reshape(y_ttrain_small.shape[0])\n",
    "y_test_small = y_test_small.reshape(y_test_small.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62f8f2e1-fc5c-4614-9278-2eadcbcb95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small_tensor = X_to_tensor(X_ttrain_small)\n",
    "y_train_small_tensor = y_to_tensor(y_ttrain_small)\n",
    "\n",
    "X_test_small_tensor = X_to_tensor(X_test_small)\n",
    "y_test_small_tensor = y_to_tensor(y_test_small)\n",
    "\n",
    "train_small_ds = TensorDataset(X_train_small_tensor, y_train_small_tensor)\n",
    "test_small_ds = TensorDataset(X_test_small_tensor, y_test_small_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1acb003-2d45-449e-8717-70bc279b96eb",
   "metadata": {},
   "source": [
    "## Run Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4a2d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3a0cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1eaf4a3-db34-4ee2-9495-9389fb08d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9c0b67a-101e-4e7e-948d-a9ee6cd7fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_func(loc_pred, loc_y):\n",
    "    # return (loc_pred.argmax(1) == loc_y).type(torch.float).sum().item()\n",
    "    return f1_score(loc_y, loc_pred.argmax(1), average='weighted') * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "434f5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNModel import NNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0875ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [len(train_small_ds[0][0]), 250, 164, 164, 104]\n",
    "nnmodel = NNModel(layer, device, acc_func=acc_func, loss_func=nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c25dcf-862c-4147-81b7-463f8d1aeea4",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7a2b703f-0b8e-4f0a-bb47-8973bee25b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Combination (0.001, 320) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (0.001, 640) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.1\n",
      "\n",
      "Parameter Combination (0.001, 1280) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.0\n",
      "\n",
      "Parameter Combination (0.01, 320) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.6\n",
      "\n",
      "Parameter Combination (0.01, 640) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.9\n",
      "\n",
      "Parameter Combination (0.01, 1280) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.6\n",
      "\n",
      "Early stopping at epoch: 40\n",
      "Parameter Combination (0.05, 320) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.2\n",
      "\n",
      "Parameter Combination (0.05, 640) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 1.7\n",
      "\n",
      "Parameter Combination (0.05, 1280) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 1.0\n",
      "\n",
      "Grid search took 1.3 minutes.\n",
      "{'learning_rate': 0.05, 'batch_size': 320}\n"
     ]
    }
   ],
   "source": [
    "test_layer = [[len(train_small_ds[0][0]), 250, 164, 164, 104], [len(train_small_ds[0][0]), 25, 16, 16, 104], [len(train_small_ds[0][0]), 250, 164, 104]]\n",
    "dict_param_1 = {\"learning_rate\": [0.001, 0.01, 0.05], \"batch_size\": [320, 640, 1280]}\n",
    "best, acc = nnmodel.grid_search(dict_param_1, train_small_ds, test_small_ds, epochs=50)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0e0a9683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch: 48\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 4.3\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.4\n",
      "\n",
      "Early stopping at epoch: 45\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 8.3\n",
      "\n",
      "Early stopping at epoch: 49\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.2, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 4.1\n",
      "\n",
      "Early stopping at epoch: 44\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.2, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.7\n",
      "\n",
      "Early stopping at epoch: 43\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.2, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 8.9\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.3, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 3.3\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.3, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.5\n",
      "\n",
      "Early stopping at epoch: 28\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.3, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.6\n",
      "\n",
      "Early stopping at epoch: 30\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.5, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.4\n",
      "\n",
      "Early stopping at epoch: 47\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.5, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.6\n",
      "\n",
      "Early stopping at epoch: 32\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.5, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 3.3\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.2, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.2, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.2, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.3, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.3, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.3, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Early stopping at epoch: 45\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.5, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.5, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Early stopping at epoch: 47\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.5, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.8\n",
      "\n",
      "Early stopping at epoch: 28\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 4.6\n",
      "\n",
      "Early stopping at epoch: 28\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 3.9\n",
      "\n",
      "Early stopping at epoch: 28\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 10.3\n",
      "\n",
      "Early stopping at epoch: 39\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.2, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 7.6\n",
      "\n",
      "Early stopping at epoch: 43\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.2, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 3.6\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.2, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 14.7\n",
      "\n",
      "Early stopping at epoch: 25\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.3, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 6.8\n",
      "\n",
      "Early stopping at epoch: 17\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.3, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.5\n",
      "\n",
      "Early stopping at epoch: 29\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.3, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 11.5\n",
      "\n",
      "Early stopping at epoch: 38\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.5, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 7.9\n",
      "\n",
      "Early stopping at epoch: 28\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.5, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.2\n",
      "\n",
      "Early stopping at epoch: 24\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.5, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 7.7\n",
      "\n",
      "Grid search took 5.6 minutes.\n",
      "{'activation': <class 'torch.nn.modules.linear.Identity'>, 'dropout': 0.2, 'layer': [918, 250, 164, 104]}\n"
     ]
    }
   ],
   "source": [
    "nnmodel.defaults[\"learning_rate\"] = best[\"learning_rate\"]\n",
    "nnmodel.defaults[\"batch_size\"] = best[\"batch_size\"]\n",
    "dict_param_2 = {\"activation\": [nn.ReLU, nn.Sigmoid, nn.Identity], \"dropout\": [0, 0.2, 0.3, 0.5], \"layer\": test_layer}\n",
    "best, acc = nnmodel.grid_search(dict_param_2, train_small_ds, test_small_ds, epochs=50)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9649b189-f6c2-474e-ae69-727398137a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel.defaults[\"activation\"] = best[\"activation\"]\n",
    "nnmodel.defaults[\"dropout\"] = best[\"dropout\"]\n",
    "nnmodel.defaults[\"layer\"] = best[\"layer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "695931a5-40ef-4ec5-a824-3929a4a32860",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel.defaults = {'learning_rate': 0.05, 'batch_size': 320, 'layer': [918, 250, 164, 104], 'activation': nn.Identity, 'dropout': 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8c2b1c7f-9ab9-43f6-bd54-790ea1b38ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'batch_size': 320, 'layer': [918, 250, 164, 104], 'activation': <class 'torch.nn.modules.linear.Identity'>, 'dropout': 0.2}\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4.659156  [  320/744121]\n",
      "loss: 436.894220  [32320/744121]\n",
      "loss: 855.936353  [64320/744121]\n",
      "loss: 1264.658367  [96320/744121]\n",
      "loss: 1666.793379  [128320/744121]\n",
      "loss: 2061.762254  [160320/744121]\n",
      "loss: 2452.023403  [192320/744121]\n",
      "loss: 2835.468275  [224320/744121]\n",
      "loss: 3214.234700  [256320/744121]\n",
      "loss: 3588.991687  [288320/744121]\n",
      "loss: 3956.158355  [320320/744121]\n",
      "loss: 4316.220711  [352320/744121]\n",
      "loss: 4672.891688  [384320/744121]\n",
      "loss: 5021.162419  [416320/744121]\n",
      "loss: 5367.060753  [448320/744121]\n",
      "loss: 5706.399689  [480320/744121]\n",
      "loss: 6042.238549  [512320/744121]\n",
      "loss: 6374.064678  [544320/744121]\n",
      "loss: 6702.711900  [576320/744121]\n",
      "loss: 7026.497931  [608320/744121]\n",
      "loss: 7348.562871  [640320/744121]\n",
      "loss: 7665.578415  [672320/744121]\n",
      "loss: 7982.876078  [704320/744121]\n",
      "loss: 8295.899903  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 29.6%, Avg loss: 2.749563 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.150975  [  320/744121]\n",
      "loss: 311.960259  [32320/744121]\n",
      "loss: 618.571052  [64320/744121]\n",
      "loss: 923.251068  [96320/744121]\n",
      "loss: 1226.954159  [128320/744121]\n",
      "loss: 1526.833965  [160320/744121]\n",
      "loss: 1824.629219  [192320/744121]\n",
      "loss: 2118.470031  [224320/744121]\n",
      "loss: 2411.913928  [256320/744121]\n",
      "loss: 2704.423791  [288320/744121]\n",
      "loss: 2992.746284  [320320/744121]\n",
      "loss: 3283.578877  [352320/744121]\n",
      "loss: 3568.086270  [384320/744121]\n",
      "loss: 3849.750922  [416320/744121]\n",
      "loss: 4131.547163  [448320/744121]\n",
      "loss: 4408.383186  [480320/744121]\n",
      "loss: 4684.313555  [512320/744121]\n",
      "loss: 4958.986028  [544320/744121]\n",
      "loss: 5231.125142  [576320/744121]\n",
      "loss: 5500.019282  [608320/744121]\n",
      "loss: 5768.434936  [640320/744121]\n",
      "loss: 6034.134576  [672320/744121]\n",
      "loss: 6299.475568  [704320/744121]\n",
      "loss: 6562.744570  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 2.195627 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.628056  [  320/744121]\n",
      "loss: 260.320909  [32320/744121]\n",
      "loss: 517.576765  [64320/744121]\n",
      "loss: 776.887220  [96320/744121]\n",
      "loss: 1034.193627  [128320/744121]\n",
      "loss: 1287.716510  [160320/744121]\n",
      "loss: 1540.648338  [192320/744121]\n",
      "loss: 1792.270275  [224320/744121]\n",
      "loss: 2041.193168  [256320/744121]\n",
      "loss: 2290.175275  [288320/744121]\n",
      "loss: 2534.760939  [320320/744121]\n",
      "loss: 2781.044883  [352320/744121]\n",
      "loss: 3024.338396  [384320/744121]\n",
      "loss: 3267.619348  [416320/744121]\n",
      "loss: 3510.157071  [448320/744121]\n",
      "loss: 3749.518553  [480320/744121]\n",
      "loss: 3989.329541  [512320/744121]\n",
      "loss: 4225.427279  [544320/744121]\n",
      "loss: 4464.740802  [576320/744121]\n",
      "loss: 4701.257307  [608320/744121]\n",
      "loss: 4933.874719  [640320/744121]\n",
      "loss: 5164.634459  [672320/744121]\n",
      "loss: 5397.098853  [704320/744121]\n",
      "loss: 5627.321749  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.856090 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.527749  [  320/744121]\n",
      "loss: 235.361355  [32320/744121]\n",
      "loss: 460.157202  [64320/744121]\n",
      "loss: 687.700781  [96320/744121]\n",
      "loss: 915.165164  [128320/744121]\n",
      "loss: 1138.962033  [160320/744121]\n",
      "loss: 1365.398202  [192320/744121]\n",
      "loss: 1588.744828  [224320/744121]\n",
      "loss: 1812.342625  [256320/744121]\n",
      "loss: 2034.557339  [288320/744121]\n",
      "loss: 2256.580796  [320320/744121]\n",
      "loss: 2479.439098  [352320/744121]\n",
      "loss: 2697.344111  [384320/744121]\n",
      "loss: 2920.171057  [416320/744121]\n",
      "loss: 3140.358675  [448320/744121]\n",
      "loss: 3358.472246  [480320/744121]\n",
      "loss: 3577.825919  [512320/744121]\n",
      "loss: 3790.604648  [544320/744121]\n",
      "loss: 4006.755154  [576320/744121]\n",
      "loss: 4218.455217  [608320/744121]\n",
      "loss: 4431.602122  [640320/744121]\n",
      "loss: 4644.253008  [672320/744121]\n",
      "loss: 4858.282398  [704320/744121]\n",
      "loss: 5070.871136  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.571227 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.092526  [  320/744121]\n",
      "loss: 212.456393  [32320/744121]\n",
      "loss: 422.548051  [64320/744121]\n",
      "loss: 633.387920  [96320/744121]\n",
      "loss: 841.866632  [128320/744121]\n",
      "loss: 1050.854172  [160320/744121]\n",
      "loss: 1258.121761  [192320/744121]\n",
      "loss: 1468.214015  [224320/744121]\n",
      "loss: 1673.557133  [256320/744121]\n",
      "loss: 1880.378500  [288320/744121]\n",
      "loss: 2084.407556  [320320/744121]\n",
      "loss: 2289.519034  [352320/744121]\n",
      "loss: 2493.054919  [384320/744121]\n",
      "loss: 2699.329098  [416320/744121]\n",
      "loss: 2902.153645  [448320/744121]\n",
      "loss: 3107.068219  [480320/744121]\n",
      "loss: 3309.324892  [512320/744121]\n",
      "loss: 3512.226634  [544320/744121]\n",
      "loss: 3712.771403  [576320/744121]\n",
      "loss: 3915.324820  [608320/744121]\n",
      "loss: 4116.096566  [640320/744121]\n",
      "loss: 4316.418885  [672320/744121]\n",
      "loss: 4515.467232  [704320/744121]\n",
      "loss: 4715.278419  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.402566 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.148897  [  320/744121]\n",
      "loss: 200.891894  [32320/744121]\n",
      "loss: 396.200561  [64320/744121]\n",
      "loss: 595.499958  [96320/744121]\n",
      "loss: 793.642801  [128320/744121]\n",
      "loss: 989.978166  [160320/744121]\n",
      "loss: 1187.012218  [192320/744121]\n",
      "loss: 1384.506774  [224320/744121]\n",
      "loss: 1579.052317  [256320/744121]\n",
      "loss: 1776.418086  [288320/744121]\n",
      "loss: 1969.460867  [320320/744121]\n",
      "loss: 2163.674348  [352320/744121]\n",
      "loss: 2355.510491  [384320/744121]\n",
      "loss: 2549.561175  [416320/744121]\n",
      "loss: 2745.138031  [448320/744121]\n",
      "loss: 2940.241396  [480320/744121]\n",
      "loss: 3128.553247  [512320/744121]\n",
      "loss: 3320.106082  [544320/744121]\n",
      "loss: 3513.595558  [576320/744121]\n",
      "loss: 3704.389848  [608320/744121]\n",
      "loss: 3897.790978  [640320/744121]\n",
      "loss: 4088.191900  [672320/744121]\n",
      "loss: 4279.212468  [704320/744121]\n",
      "loss: 4472.778768  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 1.234600 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.774217  [  320/744121]\n",
      "loss: 190.947918  [32320/744121]\n",
      "loss: 378.585300  [64320/744121]\n",
      "loss: 568.609456  [96320/744121]\n",
      "loss: 756.504478  [128320/744121]\n",
      "loss: 943.965744  [160320/744121]\n",
      "loss: 1135.591131  [192320/744121]\n",
      "loss: 1323.527623  [224320/744121]\n",
      "loss: 1510.838430  [256320/744121]\n",
      "loss: 1697.882454  [288320/744121]\n",
      "loss: 1883.643722  [320320/744121]\n",
      "loss: 2070.355786  [352320/744121]\n",
      "loss: 2257.117453  [384320/744121]\n",
      "loss: 2444.023039  [416320/744121]\n",
      "loss: 2630.295847  [448320/744121]\n",
      "loss: 2814.849484  [480320/744121]\n",
      "loss: 2995.789999  [512320/744121]\n",
      "loss: 3181.633062  [544320/744121]\n",
      "loss: 3365.995282  [576320/744121]\n",
      "loss: 3548.791173  [608320/744121]\n",
      "loss: 3734.753619  [640320/744121]\n",
      "loss: 3920.348755  [672320/744121]\n",
      "loss: 4102.701933  [704320/744121]\n",
      "loss: 4287.911925  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 1.160638 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.817006  [  320/744121]\n",
      "loss: 185.777797  [32320/744121]\n",
      "loss: 366.117629  [64320/744121]\n",
      "loss: 547.079239  [96320/744121]\n",
      "loss: 729.403970  [128320/744121]\n",
      "loss: 912.756758  [160320/744121]\n",
      "loss: 1095.622112  [192320/744121]\n",
      "loss: 1277.568425  [224320/744121]\n",
      "loss: 1455.964236  [256320/744121]\n",
      "loss: 1637.418834  [288320/744121]\n",
      "loss: 1821.261193  [320320/744121]\n",
      "loss: 1999.496893  [352320/744121]\n",
      "loss: 2177.520786  [384320/744121]\n",
      "loss: 2358.706604  [416320/744121]\n",
      "loss: 2541.144682  [448320/744121]\n",
      "loss: 2724.108883  [480320/744121]\n",
      "loss: 2904.127211  [512320/744121]\n",
      "loss: 3082.874738  [544320/744121]\n",
      "loss: 3263.285299  [576320/744121]\n",
      "loss: 3443.983912  [608320/744121]\n",
      "loss: 3622.236208  [640320/744121]\n",
      "loss: 3800.513378  [672320/744121]\n",
      "loss: 3976.730705  [704320/744121]\n",
      "loss: 4157.080989  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 1.115655 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.015419  [  320/744121]\n",
      "loss: 180.369657  [32320/744121]\n",
      "loss: 356.493066  [64320/744121]\n",
      "loss: 535.163709  [96320/744121]\n",
      "loss: 713.023264  [128320/744121]\n",
      "loss: 888.662567  [160320/744121]\n",
      "loss: 1066.396752  [192320/744121]\n",
      "loss: 1245.634472  [224320/744121]\n",
      "loss: 1420.700657  [256320/744121]\n",
      "loss: 1600.487459  [288320/744121]\n",
      "loss: 1776.105958  [320320/744121]\n",
      "loss: 1949.636478  [352320/744121]\n",
      "loss: 2125.144799  [384320/744121]\n",
      "loss: 2300.721838  [416320/744121]\n",
      "loss: 2476.427596  [448320/744121]\n",
      "loss: 2652.180532  [480320/744121]\n",
      "loss: 2825.252235  [512320/744121]\n",
      "loss: 3000.339587  [544320/744121]\n",
      "loss: 3175.226056  [576320/744121]\n",
      "loss: 3348.877365  [608320/744121]\n",
      "loss: 3522.271751  [640320/744121]\n",
      "loss: 3695.103589  [672320/744121]\n",
      "loss: 3868.494068  [704320/744121]\n",
      "loss: 4044.956014  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 1.077999 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.857369  [  320/744121]\n",
      "loss: 172.006659  [32320/744121]\n",
      "loss: 344.676223  [64320/744121]\n",
      "loss: 519.264124  [96320/744121]\n",
      "loss: 691.722032  [128320/744121]\n",
      "loss: 863.262795  [160320/744121]\n",
      "loss: 1035.735046  [192320/744121]\n",
      "loss: 1209.280011  [224320/744121]\n",
      "loss: 1380.834721  [256320/744121]\n",
      "loss: 1553.138814  [288320/744121]\n",
      "loss: 1725.967836  [320320/744121]\n",
      "loss: 1895.323441  [352320/744121]\n",
      "loss: 2067.402955  [384320/744121]\n",
      "loss: 2236.983514  [416320/744121]\n",
      "loss: 2409.721695  [448320/744121]\n",
      "loss: 2585.736577  [480320/744121]\n",
      "loss: 2758.024700  [512320/744121]\n",
      "loss: 2929.025591  [544320/744121]\n",
      "loss: 3100.292724  [576320/744121]\n",
      "loss: 3270.914137  [608320/744121]\n",
      "loss: 3442.470224  [640320/744121]\n",
      "loss: 3612.998775  [672320/744121]\n",
      "loss: 3781.331289  [704320/744121]\n",
      "loss: 3952.112229  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.988605 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.860869  [  320/744121]\n",
      "loss: 171.542746  [32320/744121]\n",
      "loss: 339.675238  [64320/744121]\n",
      "loss: 508.702812  [96320/744121]\n",
      "loss: 678.880861  [128320/744121]\n",
      "loss: 848.549750  [160320/744121]\n",
      "loss: 1017.850393  [192320/744121]\n",
      "loss: 1184.695837  [224320/744121]\n",
      "loss: 1352.077511  [256320/744121]\n",
      "loss: 1521.718819  [288320/744121]\n",
      "loss: 1690.235845  [320320/744121]\n",
      "loss: 1856.189891  [352320/744121]\n",
      "loss: 2026.302129  [384320/744121]\n",
      "loss: 2195.532687  [416320/744121]\n",
      "loss: 2364.836331  [448320/744121]\n",
      "loss: 2534.255383  [480320/744121]\n",
      "loss: 2701.515049  [512320/744121]\n",
      "loss: 2868.947346  [544320/744121]\n",
      "loss: 3036.718473  [576320/744121]\n",
      "loss: 3204.375686  [608320/744121]\n",
      "loss: 3372.414187  [640320/744121]\n",
      "loss: 3538.268314  [672320/744121]\n",
      "loss: 3704.941423  [704320/744121]\n",
      "loss: 3874.784293  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.959925 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.817829  [  320/744121]\n",
      "loss: 168.446811  [32320/744121]\n",
      "loss: 335.932941  [64320/744121]\n",
      "loss: 502.411847  [96320/744121]\n",
      "loss: 667.312960  [128320/744121]\n",
      "loss: 832.939069  [160320/744121]\n",
      "loss: 998.524863  [192320/744121]\n",
      "loss: 1165.996113  [224320/744121]\n",
      "loss: 1330.606428  [256320/744121]\n",
      "loss: 1496.971666  [288320/744121]\n",
      "loss: 1661.109514  [320320/744121]\n",
      "loss: 1826.603629  [352320/744121]\n",
      "loss: 1991.994990  [384320/744121]\n",
      "loss: 2156.410163  [416320/744121]\n",
      "loss: 2320.229397  [448320/744121]\n",
      "loss: 2486.697904  [480320/744121]\n",
      "loss: 2650.940857  [512320/744121]\n",
      "loss: 2815.140222  [544320/744121]\n",
      "loss: 2980.469756  [576320/744121]\n",
      "loss: 3145.266949  [608320/744121]\n",
      "loss: 3309.422208  [640320/744121]\n",
      "loss: 3474.130205  [672320/744121]\n",
      "loss: 3638.551235  [704320/744121]\n",
      "loss: 3805.974799  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.932273 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.705344  [  320/744121]\n",
      "loss: 165.442343  [32320/744121]\n",
      "loss: 326.611740  [64320/744121]\n",
      "loss: 492.826870  [96320/744121]\n",
      "loss: 656.891359  [128320/744121]\n",
      "loss: 820.379399  [160320/744121]\n",
      "loss: 983.895100  [192320/744121]\n",
      "loss: 1147.083482  [224320/744121]\n",
      "loss: 1310.197940  [256320/744121]\n",
      "loss: 1472.976769  [288320/744121]\n",
      "loss: 1633.844789  [320320/744121]\n",
      "loss: 1794.683683  [352320/744121]\n",
      "loss: 1959.119059  [384320/744121]\n",
      "loss: 2120.159445  [416320/744121]\n",
      "loss: 2284.969005  [448320/744121]\n",
      "loss: 2449.532082  [480320/744121]\n",
      "loss: 2610.928464  [512320/744121]\n",
      "loss: 2772.569867  [544320/744121]\n",
      "loss: 2935.918290  [576320/744121]\n",
      "loss: 3098.185789  [608320/744121]\n",
      "loss: 3259.523409  [640320/744121]\n",
      "loss: 3422.130162  [672320/744121]\n",
      "loss: 3583.981575  [704320/744121]\n",
      "loss: 3747.683792  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.943418 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.789362  [  320/744121]\n",
      "loss: 159.900909  [32320/744121]\n",
      "loss: 321.341246  [64320/744121]\n",
      "loss: 482.899245  [96320/744121]\n",
      "loss: 643.607016  [128320/744121]\n",
      "loss: 805.745372  [160320/744121]\n",
      "loss: 967.797134  [192320/744121]\n",
      "loss: 1129.308258  [224320/744121]\n",
      "loss: 1287.860881  [256320/744121]\n",
      "loss: 1449.382051  [288320/744121]\n",
      "loss: 1610.391958  [320320/744121]\n",
      "loss: 1769.905151  [352320/744121]\n",
      "loss: 1929.728507  [384320/744121]\n",
      "loss: 2091.398945  [416320/744121]\n",
      "loss: 2252.750180  [448320/744121]\n",
      "loss: 2414.181995  [480320/744121]\n",
      "loss: 2574.223662  [512320/744121]\n",
      "loss: 2732.412809  [544320/744121]\n",
      "loss: 2892.532993  [576320/744121]\n",
      "loss: 3050.913653  [608320/744121]\n",
      "loss: 3208.720518  [640320/744121]\n",
      "loss: 3367.501814  [672320/744121]\n",
      "loss: 3526.839057  [704320/744121]\n",
      "loss: 3686.893712  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.909336 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.693699  [  320/744121]\n",
      "loss: 160.795981  [32320/744121]\n",
      "loss: 319.769075  [64320/744121]\n",
      "loss: 480.690796  [96320/744121]\n",
      "loss: 640.422568  [128320/744121]\n",
      "loss: 801.231042  [160320/744121]\n",
      "loss: 959.167485  [192320/744121]\n",
      "loss: 1117.688004  [224320/744121]\n",
      "loss: 1277.290852  [256320/744121]\n",
      "loss: 1440.113727  [288320/744121]\n",
      "loss: 1597.695960  [320320/744121]\n",
      "loss: 1755.275906  [352320/744121]\n",
      "loss: 1914.166490  [384320/744121]\n",
      "loss: 2071.243273  [416320/744121]\n",
      "loss: 2230.367759  [448320/744121]\n",
      "loss: 2391.595647  [480320/744121]\n",
      "loss: 2548.684271  [512320/744121]\n",
      "loss: 2706.729770  [544320/744121]\n",
      "loss: 2865.623686  [576320/744121]\n",
      "loss: 3023.773246  [608320/744121]\n",
      "loss: 3180.721785  [640320/744121]\n",
      "loss: 3339.199573  [672320/744121]\n",
      "loss: 3497.245964  [704320/744121]\n",
      "loss: 3657.558573  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.892868 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.939390  [  320/744121]\n",
      "loss: 159.062626  [32320/744121]\n",
      "loss: 316.467671  [64320/744121]\n",
      "loss: 474.239474  [96320/744121]\n",
      "loss: 631.813017  [128320/744121]\n",
      "loss: 790.277346  [160320/744121]\n",
      "loss: 946.798351  [192320/744121]\n",
      "loss: 1101.829432  [224320/744121]\n",
      "loss: 1259.048197  [256320/744121]\n",
      "loss: 1417.212929  [288320/744121]\n",
      "loss: 1574.250873  [320320/744121]\n",
      "loss: 1729.879703  [352320/744121]\n",
      "loss: 1887.964518  [384320/744121]\n",
      "loss: 2047.705954  [416320/744121]\n",
      "loss: 2205.354166  [448320/744121]\n",
      "loss: 2363.600619  [480320/744121]\n",
      "loss: 2519.099000  [512320/744121]\n",
      "loss: 2678.736919  [544320/744121]\n",
      "loss: 2835.773838  [576320/744121]\n",
      "loss: 2990.394950  [608320/744121]\n",
      "loss: 3148.783676  [640320/744121]\n",
      "loss: 3303.818333  [672320/744121]\n",
      "loss: 3460.045519  [704320/744121]\n",
      "loss: 3618.050760  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.871485 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.601432  [  320/744121]\n",
      "loss: 157.750721  [32320/744121]\n",
      "loss: 313.319691  [64320/744121]\n",
      "loss: 470.187852  [96320/744121]\n",
      "loss: 625.411995  [128320/744121]\n",
      "loss: 782.817235  [160320/744121]\n",
      "loss: 939.732029  [192320/744121]\n",
      "loss: 1095.426975  [224320/744121]\n",
      "loss: 1250.122291  [256320/744121]\n",
      "loss: 1407.137181  [288320/744121]\n",
      "loss: 1562.889946  [320320/744121]\n",
      "loss: 1717.451752  [352320/744121]\n",
      "loss: 1873.160704  [384320/744121]\n",
      "loss: 2026.987963  [416320/744121]\n",
      "loss: 2182.549291  [448320/744121]\n",
      "loss: 2338.800368  [480320/744121]\n",
      "loss: 2493.747231  [512320/744121]\n",
      "loss: 2648.697188  [544320/744121]\n",
      "loss: 2804.060695  [576320/744121]\n",
      "loss: 2959.217492  [608320/744121]\n",
      "loss: 3114.193509  [640320/744121]\n",
      "loss: 3269.169666  [672320/744121]\n",
      "loss: 3423.941430  [704320/744121]\n",
      "loss: 3579.629653  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.837765 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.679035  [  320/744121]\n",
      "loss: 157.075390  [32320/744121]\n",
      "loss: 312.501134  [64320/744121]\n",
      "loss: 469.605462  [96320/744121]\n",
      "loss: 625.624256  [128320/744121]\n",
      "loss: 782.698728  [160320/744121]\n",
      "loss: 938.362937  [192320/744121]\n",
      "loss: 1093.772018  [224320/744121]\n",
      "loss: 1247.807066  [256320/744121]\n",
      "loss: 1401.285175  [288320/744121]\n",
      "loss: 1554.607969  [320320/744121]\n",
      "loss: 1707.749042  [352320/744121]\n",
      "loss: 1864.129527  [384320/744121]\n",
      "loss: 2018.925879  [416320/744121]\n",
      "loss: 2174.478260  [448320/744121]\n",
      "loss: 2329.596951  [480320/744121]\n",
      "loss: 2483.559697  [512320/744121]\n",
      "loss: 2639.083425  [544320/744121]\n",
      "loss: 2792.435472  [576320/744121]\n",
      "loss: 2947.524996  [608320/744121]\n",
      "loss: 3101.968892  [640320/744121]\n",
      "loss: 3255.918064  [672320/744121]\n",
      "loss: 3407.865463  [704320/744121]\n",
      "loss: 3562.792637  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.857562 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.645040  [  320/744121]\n",
      "loss: 154.179440  [32320/744121]\n",
      "loss: 308.442562  [64320/744121]\n",
      "loss: 461.783474  [96320/744121]\n",
      "loss: 617.661137  [128320/744121]\n",
      "loss: 772.624441  [160320/744121]\n",
      "loss: 930.473694  [192320/744121]\n",
      "loss: 1083.470055  [224320/744121]\n",
      "loss: 1235.840159  [256320/744121]\n",
      "loss: 1390.633428  [288320/744121]\n",
      "loss: 1541.844468  [320320/744121]\n",
      "loss: 1692.520240  [352320/744121]\n",
      "loss: 1847.196737  [384320/744121]\n",
      "loss: 2001.616362  [416320/744121]\n",
      "loss: 2157.237084  [448320/744121]\n",
      "loss: 2313.193895  [480320/744121]\n",
      "loss: 2465.510688  [512320/744121]\n",
      "loss: 2619.257180  [544320/744121]\n",
      "loss: 2773.150612  [576320/744121]\n",
      "loss: 2926.176435  [608320/744121]\n",
      "loss: 3079.219728  [640320/744121]\n",
      "loss: 3230.773627  [672320/744121]\n",
      "loss: 3381.271471  [704320/744121]\n",
      "loss: 3535.391904  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.822819 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.547897  [  320/744121]\n",
      "loss: 152.024037  [32320/744121]\n",
      "loss: 303.977452  [64320/744121]\n",
      "loss: 458.246551  [96320/744121]\n",
      "loss: 609.578357  [128320/744121]\n",
      "loss: 762.283743  [160320/744121]\n",
      "loss: 915.545366  [192320/744121]\n",
      "loss: 1068.737572  [224320/744121]\n",
      "loss: 1221.635119  [256320/744121]\n",
      "loss: 1373.092309  [288320/744121]\n",
      "loss: 1523.922858  [320320/744121]\n",
      "loss: 1673.429610  [352320/744121]\n",
      "loss: 1824.812921  [384320/744121]\n",
      "loss: 1978.417707  [416320/744121]\n",
      "loss: 2129.101840  [448320/744121]\n",
      "loss: 2281.432376  [480320/744121]\n",
      "loss: 2432.641209  [512320/744121]\n",
      "loss: 2583.702637  [544320/744121]\n",
      "loss: 2736.391446  [576320/744121]\n",
      "loss: 2888.594714  [608320/744121]\n",
      "loss: 3041.119947  [640320/744121]\n",
      "loss: 3192.761366  [672320/744121]\n",
      "loss: 3342.435831  [704320/744121]\n",
      "loss: 3496.952456  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.801158 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.653409  [  320/744121]\n",
      "loss: 152.848171  [32320/744121]\n",
      "loss: 302.840524  [64320/744121]\n",
      "loss: 454.587259  [96320/744121]\n",
      "loss: 606.161424  [128320/744121]\n",
      "loss: 759.525900  [160320/744121]\n",
      "loss: 912.175502  [192320/744121]\n",
      "loss: 1062.682275  [224320/744121]\n",
      "loss: 1213.696690  [256320/744121]\n",
      "loss: 1364.978894  [288320/744121]\n",
      "loss: 1512.208494  [320320/744121]\n",
      "loss: 1662.405082  [352320/744121]\n",
      "loss: 1815.089477  [384320/744121]\n",
      "loss: 1965.167832  [416320/744121]\n",
      "loss: 2116.841923  [448320/744121]\n",
      "loss: 2270.031713  [480320/744121]\n",
      "loss: 2419.562085  [512320/744121]\n",
      "loss: 2571.837775  [544320/744121]\n",
      "loss: 2722.365900  [576320/744121]\n",
      "loss: 2873.059456  [608320/744121]\n",
      "loss: 3024.555072  [640320/744121]\n",
      "loss: 3175.281070  [672320/744121]\n",
      "loss: 3323.569423  [704320/744121]\n",
      "loss: 3475.899159  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.779245 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.532725  [  320/744121]\n",
      "loss: 153.229227  [32320/744121]\n",
      "loss: 301.550844  [64320/744121]\n",
      "loss: 452.796527  [96320/744121]\n",
      "loss: 602.006457  [128320/744121]\n",
      "loss: 752.778748  [160320/744121]\n",
      "loss: 906.161182  [192320/744121]\n",
      "loss: 1055.000311  [224320/744121]\n",
      "loss: 1202.856552  [256320/744121]\n",
      "loss: 1352.632454  [288320/744121]\n",
      "loss: 1502.816318  [320320/744121]\n",
      "loss: 1649.999304  [352320/744121]\n",
      "loss: 1801.319408  [384320/744121]\n",
      "loss: 1949.778599  [416320/744121]\n",
      "loss: 2100.868734  [448320/744121]\n",
      "loss: 2250.623273  [480320/744121]\n",
      "loss: 2400.973830  [512320/744121]\n",
      "loss: 2551.101730  [544320/744121]\n",
      "loss: 2701.434571  [576320/744121]\n",
      "loss: 2850.145661  [608320/744121]\n",
      "loss: 3001.247970  [640320/744121]\n",
      "loss: 3150.290828  [672320/744121]\n",
      "loss: 3299.229862  [704320/744121]\n",
      "loss: 3451.584944  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.786950 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.683182  [  320/744121]\n",
      "loss: 150.328682  [32320/744121]\n",
      "loss: 299.475333  [64320/744121]\n",
      "loss: 451.050028  [96320/744121]\n",
      "loss: 598.978581  [128320/744121]\n",
      "loss: 748.290021  [160320/744121]\n",
      "loss: 899.029194  [192320/744121]\n",
      "loss: 1049.085790  [224320/744121]\n",
      "loss: 1198.150616  [256320/744121]\n",
      "loss: 1347.266938  [288320/744121]\n",
      "loss: 1495.663990  [320320/744121]\n",
      "loss: 1642.940507  [352320/744121]\n",
      "loss: 1792.597764  [384320/744121]\n",
      "loss: 1940.640918  [416320/744121]\n",
      "loss: 2092.173410  [448320/744121]\n",
      "loss: 2243.978962  [480320/744121]\n",
      "loss: 2393.154855  [512320/744121]\n",
      "loss: 2543.232092  [544320/744121]\n",
      "loss: 2692.165367  [576320/744121]\n",
      "loss: 2841.734439  [608320/744121]\n",
      "loss: 2990.916856  [640320/744121]\n",
      "loss: 3139.216010  [672320/744121]\n",
      "loss: 3287.667628  [704320/744121]\n",
      "loss: 3438.967454  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.797146 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.590658  [  320/744121]\n",
      "loss: 150.208863  [32320/744121]\n",
      "loss: 299.994646  [64320/744121]\n",
      "loss: 449.133342  [96320/744121]\n",
      "loss: 600.007442  [128320/744121]\n",
      "loss: 748.157262  [160320/744121]\n",
      "loss: 896.837279  [192320/744121]\n",
      "loss: 1044.706738  [224320/744121]\n",
      "loss: 1192.208593  [256320/744121]\n",
      "loss: 1341.582345  [288320/744121]\n",
      "loss: 1490.233849  [320320/744121]\n",
      "loss: 1638.524011  [352320/744121]\n",
      "loss: 1788.203157  [384320/744121]\n",
      "loss: 1935.902583  [416320/744121]\n",
      "loss: 2085.939956  [448320/744121]\n",
      "loss: 2234.655212  [480320/744121]\n",
      "loss: 2381.919622  [512320/744121]\n",
      "loss: 2530.383428  [544320/744121]\n",
      "loss: 2679.552169  [576320/744121]\n",
      "loss: 2827.147382  [608320/744121]\n",
      "loss: 2976.061415  [640320/744121]\n",
      "loss: 3123.830423  [672320/744121]\n",
      "loss: 3271.914637  [704320/744121]\n",
      "loss: 3422.629583  [736320/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.788182 \n",
      "\n",
      "Early stopping at epoch: 23\n",
      "losses [3.600023065265252, 2.849590279885847, 2.443897988041823, 2.203392536959972, 2.048834836493252, 1.9431175595711554, 1.8627344001395274, 1.8061865503585042, 1.7577859845432042, 1.7168600749887482, 1.6836142714230644, 1.6537674807928189, 1.6280999023752385, 1.602251498943142, 1.5897661162058239, 1.5726645948655218, 1.555785607953371, 1.5481493858458764, 1.5364188482714232, 1.5202296982012427, 1.5102089917157502, 1.4999738739978006, 1.494551736839045, 1.4875063062329181]\n",
      "test_losses [2.74956303398492, 2.1956270584012705, 1.8560901269271834, 1.5712265907343077, 1.4025663900279712, 1.2346003745479832, 1.1606378928947831, 1.115655464537763, 1.0779986362997722, 0.988604577816836, 0.959925207773685, 0.9322732291824242, 0.9434183138304034, 0.9093358013909703, 0.8928684531267332, 0.8714854109849232, 0.8377651733644269, 0.8575618637122266, 0.8228187769681305, 0.801157515828564, 0.7792450504054277, 0.7869504034339844, 0.7971457117419305, 0.788181650722278]\n",
      "accs [0.2958827596036704, 0.43686178442300644, 0.5237060970846292, 0.6027107345058929, 0.6522236100701702, 0.695456546010879, 0.7151007075194619, 0.7226909539082134, 0.7347321096197608, 0.7623873181289355, 0.7629878454842158, 0.7705489591676731, 0.7695740070675569, 0.7755608454122737, 0.7778403082620372, 0.7805061204285394, 0.7957850460751494, 0.7900233120453177, 0.7931067555720999, 0.8039838457645704, 0.8068875696773528, 0.8038885103832143, 0.8024836548997458, 0.8055575967473666]\n",
      "0.8055575967473666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(nnmodel.defaults)\n",
    "acc = nnmodel.run(nnmodel.defaults, train_ds, test_ds, 100, out=True, name=\"beer_grid_res\")\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c4dc2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.786533 \n",
      "\n",
      "(0.786532998085022, 0.8127010633546179)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.73      0.83      2528\n",
      "           1       0.81      0.78      0.80     10196\n",
      "           2       0.91      0.77      0.83     15084\n",
      "           3       0.95      0.63      0.76      3100\n",
      "           4       0.72      0.90      0.80      8778\n",
      "           5       0.73      0.82      0.77      3821\n",
      "           6       0.78      0.61      0.69      4123\n",
      "           7       0.77      0.84      0.80      8397\n",
      "           8       0.83      0.15      0.26       498\n",
      "           9       0.92      0.89      0.90     28284\n",
      "          10       0.96      0.90      0.93      1869\n",
      "          11       0.94      0.81      0.87     16721\n",
      "          12       0.93      0.90      0.92     38886\n",
      "          13       0.95      0.73      0.82      1325\n",
      "          14       0.63      0.91      0.75     20859\n",
      "          15       0.54      0.54      0.54      3022\n",
      "          16       0.72      0.85      0.78      7963\n",
      "          17       0.91      0.94      0.93     16743\n",
      "          18       0.85      0.86      0.86      8002\n",
      "          19       0.69      0.85      0.76     10498\n",
      "          20       0.33      0.87      0.48      5940\n",
      "          21       0.97      0.79      0.87      3790\n",
      "          22       0.92      0.55      0.69      2109\n",
      "          23       0.86      0.73      0.79      4038\n",
      "          24       0.37      0.78      0.50      6359\n",
      "          25       0.56      0.84      0.68     12398\n",
      "          26       0.94      0.41      0.57     10438\n",
      "          27       0.94      0.93      0.93      1116\n",
      "          28       0.99      0.39      0.56       337\n",
      "          29       0.73      0.68      0.70      2303\n",
      "          30       1.00      0.92      0.96       781\n",
      "          31       0.79      0.83      0.81      3799\n",
      "          32       0.89      0.05      0.09       339\n",
      "          33       0.97      0.73      0.83      1375\n",
      "          34       0.96      0.76      0.85       772\n",
      "          35       0.86      0.88      0.87      1699\n",
      "          36       0.79      0.75      0.77      4213\n",
      "          37       0.85      0.86      0.85      7251\n",
      "          38       0.82      0.75      0.78      1438\n",
      "          39       0.91      0.78      0.84      6542\n",
      "          40       0.92      0.84      0.88      2287\n",
      "          41       1.00      0.84      0.91       841\n",
      "          42       0.98      0.65      0.78      4512\n",
      "          43       0.78      0.58      0.67      2900\n",
      "          44       0.84      0.85      0.84      6549\n",
      "          45       0.88      0.48      0.62       743\n",
      "          46       0.98      0.59      0.74      5263\n",
      "          47       0.72      0.76      0.74      7736\n",
      "          48       0.00      0.00      0.00       253\n",
      "          49       0.91      0.73      0.81      3708\n",
      "          50       0.73      0.50      0.59       996\n",
      "          51       0.87      0.49      0.63      1580\n",
      "          52       0.79      0.55      0.65      1536\n",
      "          53       0.62      0.72      0.67      5995\n",
      "          54       0.74      0.62      0.67       903\n",
      "          55       0.65      0.81      0.72      5615\n",
      "          56       1.00      0.78      0.88       190\n",
      "          57       0.78      0.64      0.71      1669\n",
      "          58       0.99      0.65      0.78      2190\n",
      "          59       0.96      0.64      0.77      1944\n",
      "          60       0.82      0.70      0.76     11248\n",
      "          61       0.83      0.87      0.85      7226\n",
      "          62       0.95      0.85      0.90       220\n",
      "          63       0.91      0.93      0.92      1981\n",
      "          64       0.00      0.00      0.00        72\n",
      "          65       0.92      0.88      0.90      9338\n",
      "          66       0.87      0.40      0.55      3363\n",
      "          67       0.97      0.88      0.93      4217\n",
      "          68       0.90      0.81      0.85      2607\n",
      "          69       0.95      0.87      0.91       524\n",
      "          70       0.66      0.51      0.58       831\n",
      "          71       1.00      0.47      0.64       736\n",
      "          72       0.00      0.00      0.00        82\n",
      "          73       0.88      0.75      0.81      2845\n",
      "          74       0.96      0.85      0.90      3591\n",
      "          75       1.00      0.56      0.71       387\n",
      "          76       0.95      0.93      0.94      4752\n",
      "          77       0.80      0.86      0.83       365\n",
      "          78       0.80      0.85      0.83      3524\n",
      "          79       0.95      0.83      0.89      4298\n",
      "          80       0.87      0.76      0.82      2651\n",
      "          81       0.86      0.68      0.76      2630\n",
      "          82       0.95      0.88      0.91      7781\n",
      "          83       0.95      0.90      0.93      5993\n",
      "          84       0.74      0.89      0.81      4870\n",
      "          85       0.90      0.94      0.92      5053\n",
      "          86       0.96      0.85      0.90      5993\n",
      "          87       0.92      0.76      0.83      1327\n",
      "          88       0.00      0.00      0.00       130\n",
      "          89       0.85      0.92      0.88     17745\n",
      "          90       0.87      0.67      0.76      3338\n",
      "          91       1.00      0.88      0.94       343\n",
      "          92       0.74      0.83      0.78     10300\n",
      "          93       0.93      0.83      0.88      3246\n",
      "          94       0.95      0.86      0.90      5792\n",
      "          95       0.95      0.69      0.80      3024\n",
      "          96       0.90      0.79      0.84       916\n",
      "          97       0.98      0.27      0.42       947\n",
      "          98       0.99      0.78      0.87      9904\n",
      "          99       0.86      0.82      0.84      2965\n",
      "         100       1.00      0.81      0.89      3081\n",
      "         101       0.59      0.81      0.68      1271\n",
      "         102       0.93      0.76      0.84      6874\n",
      "         103       0.96      0.84      0.90     10058\n",
      "\n",
      "    accuracy                           0.81    523583\n",
      "   macro avg       0.83      0.72      0.75    523583\n",
      "weighted avg       0.84      0.81      0.81    523583\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc = test(valid_dataloader, nnmodel.model, nnmodel.loss_fn)\n",
    "print(acc)\n",
    "validate(valid_dataloader, nnmodel.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3ea3ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_best = nnmodel.defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963044d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "nnmodel.defaults['learning_rate'] = 0.05\n",
    "nnmodel.defaults['layer'] = [918, 250, 164, 104]\n",
    "nnmodel.defaults['batch_size'] = 320\n",
    "nnmodel.defaults['dropout'] = 0.2\n",
    "nnmodel.defaults['activation'] = nn.Linear\n",
    "nnmodel.acc_func = acc_func\n",
    "cv_acc = nnmodel.run_cv(nnmodel.defaults, train_small_ds, test_small_ds, epochs=100, k_folds=5)\n",
    "print(cv_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42513ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/beer_grid_search_res_cv.txt', 'w+') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc879c4b",
   "metadata": {},
   "source": [
    "## Local Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7473052a-2f62-4d08-b18c-583738e7c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel = NNModel(layer, device, acc_func=acc_func, loss_func=nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "298e14b4-5980-4b83-8c62-01ea1b0ff73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch: 48\n",
      "Step 0\n",
      "Best Params, Parameter Combination {'learning_rate': 0.05, 'batch_size': 320}\n",
      " Accuracy: 3.3\n",
      "\n",
      "Parameter Combination (0.03928, 226) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 5.4\n",
      "\n",
      "Parameter Combination (0.03928, 393) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 2.8\n",
      "\n",
      "Early stopping at epoch: 31\n",
      "Parameter Combination (0.06394, 226) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.1\n",
      "\n",
      "Parameter Combination (0.06394, 393) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 4.0\n",
      "\n",
      "Grid search took 0.9 minutes.\n",
      "Step 1\n",
      "Best Params, Parameter Combination {'learning_rate': 0.03928, 'batch_size': 226}\n",
      " Accuracy: 5.4\n",
      "\n",
      "Early stopping at epoch: 42\n",
      "Parameter Combination (0.02879, 166) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.3\n",
      "\n",
      "Parameter Combination (0.02879, 278) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.8\n",
      "\n",
      "Early stopping at epoch: 31\n",
      "Parameter Combination (0.04604, 166) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 4.1\n",
      "\n",
      "Early stopping at epoch: 45\n",
      "Parameter Combination (0.04604, 278) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.5\n",
      "\n",
      "Grid search took 1.0 minutes.\n",
      "Step 2\n",
      "Best Params, Parameter Combination {'learning_rate': 0.03928, 'batch_size': 226}\n",
      " Accuracy: 5.4\n",
      "\n",
      "Parameter Combination (0.02943, 174) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.7\n",
      "\n",
      "Parameter Combination (0.02943, 249) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.6\n",
      "\n",
      "Early stopping at epoch: 35\n",
      "Parameter Combination (0.04661, 174) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 9.9\n",
      "\n",
      "Early stopping at epoch: 39\n",
      "Parameter Combination (0.04661, 249) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 4.0\n",
      "\n",
      "Grid search took 1.0 minutes.\n",
      "Step 3\n",
      "Best Params, Parameter Combination {'learning_rate': 0.04661, 'batch_size': 174}\n",
      " Accuracy: 9.9\n",
      "\n",
      "Early stopping at epoch: 31\n",
      "Parameter Combination (0.03887, 154) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.8\n",
      "\n",
      "Parameter Combination (0.03887, 220) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 2.0\n",
      "\n",
      "Early stopping at epoch: 37\n",
      "Parameter Combination (0.05916, 154) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 5.0\n",
      "\n",
      "Early stopping at epoch: 39\n",
      "Parameter Combination (0.05916, 220) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 10.3\n",
      "\n",
      "Grid search took 1.0 minutes.\n",
      "Step 4\n",
      "Best Params, Parameter Combination {'learning_rate': 0.05916, 'batch_size': 220}\n",
      " Accuracy: 10.3\n",
      "\n",
      "Early stopping at epoch: 31\n",
      "Parameter Combination (0.04494, 170) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 7.0\n",
      "\n",
      "Early stopping at epoch: 46\n",
      "Parameter Combination (0.04494, 258) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.0\n",
      "\n",
      "Early stopping at epoch: 22\n",
      "Parameter Combination (0.07052, 170) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.8\n",
      "\n",
      "Early stopping at epoch: 36\n",
      "Parameter Combination (0.07052, 258) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 2.6\n",
      "\n",
      "Grid search took 0.9 minutes.\n",
      "Local search took 5.0 minutes.\n",
      "{'learning_rate': 0.05916, 'batch_size': 220}\n"
     ]
    }
   ],
   "source": [
    "init_param = {\"learning_rate\": grid_best[\"learning_rate\"], \"batch_size\": grid_best[\"batch_size\"]}\n",
    "best, acc = nnmodel.local_search(init_param, train_small_ds, test_small_ds, steps=5, epochs=50)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "53a55130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Best Params, Parameter Combination {'layer': [918, 250, 164, 104], 'dropout': 0.2}\n",
      " Accuracy: 11.6\n",
      "\n",
      "Early stopping at epoch: 43\n",
      "Parameter Combination ([918, 202, 145, 104], 0.15407) with keys ['layer', 'dropout']\n",
      " Accuracy: 14.2\n",
      "\n",
      "Parameter Combination ([918, 202, 145, 104], 0.25395) with keys ['layer', 'dropout']\n",
      " Accuracy: 9.8\n",
      "\n",
      "Parameter Combination ([918, 309, 200, 104], 0.15407) with keys ['layer', 'dropout']\n",
      " Accuracy: 11.3\n",
      "\n",
      "Parameter Combination ([918, 309, 200, 104], 0.25395) with keys ['layer', 'dropout']\n",
      " Accuracy: 15.2\n",
      "\n",
      "Grid search took 1.0 minutes.\n",
      "Step 1\n",
      "Best Params, Parameter Combination {'layer': [918, 309, 200, 104], 'dropout': 0.25395}\n",
      " Accuracy: 15.2\n",
      "\n",
      "Parameter Combination ([918, 259, 145, 104], 0.19604) with keys ['layer', 'dropout']\n",
      " Accuracy: 10.8\n",
      "\n",
      "Early stopping at epoch: 28\n",
      "Parameter Combination ([918, 259, 145, 104], 0.31174) with keys ['layer', 'dropout']\n",
      " Accuracy: 10.8\n",
      "\n",
      "Parameter Combination ([918, 383, 244, 104], 0.19604) with keys ['layer', 'dropout']\n",
      " Accuracy: 18.2\n",
      "\n",
      "Early stopping at epoch: 24\n",
      "Parameter Combination ([918, 383, 244, 104], 0.31174) with keys ['layer', 'dropout']\n",
      " Accuracy: 7.2\n",
      "\n",
      "Grid search took 1.0 minutes.\n",
      "Step 2\n",
      "Best Params, Parameter Combination {'layer': [918, 383, 244, 104], 'dropout': 0.19604}\n",
      " Accuracy: 18.2\n",
      "\n",
      "Early stopping at epoch: 49\n",
      "Parameter Combination ([918, 316, 196, 104], 0.13805) with keys ['layer', 'dropout']\n",
      " Accuracy: 12.7\n",
      "\n",
      "Early stopping at epoch: 47\n",
      "Parameter Combination ([918, 316, 196, 104], 0.23525) with keys ['layer', 'dropout']\n",
      " Accuracy: 18.4\n",
      "\n",
      "Early stopping at epoch: 42\n",
      "Parameter Combination ([918, 492, 308, 104], 0.13805) with keys ['layer', 'dropout']\n",
      " Accuracy: 11.6\n",
      "\n",
      "Early stopping at epoch: 47\n",
      "Parameter Combination ([918, 492, 308, 104], 0.23525) with keys ['layer', 'dropout']\n",
      " Accuracy: 17.7\n",
      "\n",
      "Grid search took 1.2 minutes.\n",
      "Step 3\n",
      "Best Params, Parameter Combination {'layer': [918, 316, 196, 104], 'dropout': 0.23525}\n",
      " Accuracy: 18.4\n",
      "\n",
      "Early stopping at epoch: 30\n",
      "Parameter Combination ([918, 233, 146, 104], 0.20206) with keys ['layer', 'dropout']\n",
      " Accuracy: 5.6\n",
      "\n",
      "Early stopping at epoch: 39\n",
      "Parameter Combination ([918, 233, 146, 104], 0.27797) with keys ['layer', 'dropout']\n",
      " Accuracy: 10.7\n",
      "\n",
      "Parameter Combination ([918, 357, 250, 104], 0.20206) with keys ['layer', 'dropout']\n",
      " Accuracy: 13.8\n",
      "\n",
      "Early stopping at epoch: 27\n",
      "Parameter Combination ([918, 357, 250, 104], 0.27797) with keys ['layer', 'dropout']\n",
      " Accuracy: 8.4\n",
      "\n",
      "Grid search took 0.9 minutes.\n",
      "Step 4\n",
      "Best Params, Parameter Combination {'layer': [918, 316, 196, 104], 'dropout': 0.23525}\n",
      " Accuracy: 18.4\n",
      "\n",
      "Early stopping at epoch: 45\n",
      "Parameter Combination ([918, 242, 149, 104], 0.19757) with keys ['layer', 'dropout']\n",
      " Accuracy: 11.5\n",
      "\n",
      "Parameter Combination ([918, 242, 149, 104], 0.2731) with keys ['layer', 'dropout']\n",
      " Accuracy: 10.8\n",
      "\n",
      "Parameter Combination ([918, 369, 227, 104], 0.19757) with keys ['layer', 'dropout']\n",
      " Accuracy: 14.3\n",
      "\n",
      "Early stopping at epoch: 31\n",
      "Parameter Combination ([918, 369, 227, 104], 0.2731) with keys ['layer', 'dropout']\n",
      " Accuracy: 9.2\n",
      "\n",
      "Grid search took 1.1 minutes.\n",
      "Local search took 5.5 minutes.\n"
     ]
    }
   ],
   "source": [
    "nnmodel.defaults[\"learning_rate\"] = best[\"learning_rate\"]\n",
    "nnmodel.defaults[\"batch_size\"] = best[\"batch_size\"]\n",
    "init_param = {\"layer\": grid_best[\"layer\"], \"dropout\": grid_best[\"dropout\"]}\n",
    "best, acc = nnmodel.local_search(init_param, train_small_ds, test_small_ds, steps=5, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "536b57d4-03ca-4cf3-822f-c6b03feaf7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel.defaults[\"dropout\"] = best[\"dropout\"]\n",
    "nnmodel.defaults[\"layer\"] = best[\"layer\"]\n",
    "nnmodel.defaults[\"activation\"] = grid_best[\"activation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bb806463-f695-4e3e-b7c5-252bb70be04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05916, 'batch_size': 220, 'layer': [918, 316, 196, 104], 'activation': <class 'torch.nn.modules.linear.Identity'>, 'dropout': 0.23525}\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4.633439  [  220/744121]\n",
      "loss: 434.442266  [22220/744121]\n",
      "loss: 851.367460  [44220/744121]\n",
      "loss: 1257.674482  [66220/744121]\n",
      "loss: 1657.493434  [88220/744121]\n",
      "loss: 2050.411250  [110220/744121]\n",
      "loss: 2437.781193  [132220/744121]\n",
      "loss: 2818.604261  [154220/744121]\n",
      "loss: 3192.540667  [176220/744121]\n",
      "loss: 3559.655570  [198220/744121]\n",
      "loss: 3919.524061  [220220/744121]\n",
      "loss: 4274.038711  [242220/744121]\n",
      "loss: 4624.321822  [264220/744121]\n",
      "loss: 4969.303653  [286220/744121]\n",
      "loss: 5308.135767  [308220/744121]\n",
      "loss: 5645.444393  [330220/744121]\n",
      "loss: 5979.252656  [352220/744121]\n",
      "loss: 6309.299515  [374220/744121]\n",
      "loss: 6634.927290  [396220/744121]\n",
      "loss: 6954.938415  [418220/744121]\n",
      "loss: 7277.023932  [440220/744121]\n",
      "loss: 7593.155709  [462220/744121]\n",
      "loss: 7909.512070  [484220/744121]\n",
      "loss: 8222.135484  [506220/744121]\n",
      "loss: 8531.153265  [528220/744121]\n",
      "loss: 8836.575409  [550220/744121]\n",
      "loss: 9141.032512  [572220/744121]\n",
      "loss: 9441.815862  [594220/744121]\n",
      "loss: 9739.797811  [616220/744121]\n",
      "loss: 10033.007189  [638220/744121]\n",
      "loss: 10329.099484  [660220/744121]\n",
      "loss: 10619.293996  [682220/744121]\n",
      "loss: 10907.522915  [704220/744121]\n",
      "loss: 11198.985705  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.442944 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.900317  [  220/744121]\n",
      "loss: 284.121270  [22220/744121]\n",
      "loss: 566.502120  [44220/744121]\n",
      "loss: 847.269233  [66220/744121]\n",
      "loss: 1123.964904  [88220/744121]\n",
      "loss: 1400.866317  [110220/744121]\n",
      "loss: 1677.933325  [132220/744121]\n",
      "loss: 1951.643295  [154220/744121]\n",
      "loss: 2225.764267  [176220/744121]\n",
      "loss: 2495.209186  [198220/744121]\n",
      "loss: 2762.272203  [220220/744121]\n",
      "loss: 3029.958424  [242220/744121]\n",
      "loss: 3295.007883  [264220/744121]\n",
      "loss: 3558.531216  [286220/744121]\n",
      "loss: 3817.637391  [308220/744121]\n",
      "loss: 4076.105847  [330220/744121]\n",
      "loss: 4336.889904  [352220/744121]\n",
      "loss: 4595.261557  [374220/744121]\n",
      "loss: 4851.957444  [396220/744121]\n",
      "loss: 5107.133535  [418220/744121]\n",
      "loss: 5362.457662  [440220/744121]\n",
      "loss: 5613.828818  [462220/744121]\n",
      "loss: 5866.578957  [484220/744121]\n",
      "loss: 6115.407097  [506220/744121]\n",
      "loss: 6364.449432  [528220/744121]\n",
      "loss: 6609.180234  [550220/744121]\n",
      "loss: 6854.278593  [572220/744121]\n",
      "loss: 7097.710849  [594220/744121]\n",
      "loss: 7339.167792  [616220/744121]\n",
      "loss: 7580.040400  [638220/744121]\n",
      "loss: 7822.011537  [660220/744121]\n",
      "loss: 8061.191496  [682220/744121]\n",
      "loss: 8301.600670  [704220/744121]\n",
      "loss: 8541.218660  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.769107 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.478739  [  220/744121]\n",
      "loss: 236.260252  [22220/744121]\n",
      "loss: 473.051180  [44220/744121]\n",
      "loss: 707.143423  [66220/744121]\n",
      "loss: 940.673311  [88220/744121]\n",
      "loss: 1177.717499  [110220/744121]\n",
      "loss: 1481.440763  [132220/744121]\n",
      "loss: 1716.425593  [154220/744121]\n",
      "loss: 1948.863896  [176220/744121]\n",
      "loss: 2180.704753  [198220/744121]\n",
      "loss: 2410.918308  [220220/744121]\n",
      "loss: 2638.871221  [242220/744121]\n",
      "loss: 2864.854687  [264220/744121]\n",
      "loss: 3090.685490  [286220/744121]\n",
      "loss: 3315.814164  [308220/744121]\n",
      "loss: 3539.772818  [330220/744121]\n",
      "loss: 3765.308547  [352220/744121]\n",
      "loss: 3990.138010  [374220/744121]\n",
      "loss: 4215.629176  [396220/744121]\n",
      "loss: 4440.049632  [418220/744121]\n",
      "loss: 4662.135878  [440220/744121]\n",
      "loss: 4886.038200  [462220/744121]\n",
      "loss: 5109.170536  [484220/744121]\n",
      "loss: 5328.933265  [506220/744121]\n",
      "loss: 5548.283043  [528220/744121]\n",
      "loss: 5767.258729  [550220/744121]\n",
      "loss: 5985.837226  [572220/744121]\n",
      "loss: 6205.595560  [594220/744121]\n",
      "loss: 6424.391969  [616220/744121]\n",
      "loss: 6637.775228  [638220/744121]\n",
      "loss: 6852.181114  [660220/744121]\n",
      "loss: 7067.063101  [682220/744121]\n",
      "loss: 7284.510553  [704220/744121]\n",
      "loss: 7500.393857  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.441718 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.078930  [  220/744121]\n",
      "loss: 217.220657  [22220/744121]\n",
      "loss: 430.235338  [44220/744121]\n",
      "loss: 643.194092  [66220/744121]\n",
      "loss: 855.005914  [88220/744121]\n",
      "loss: 1070.337320  [110220/744121]\n",
      "loss: 1283.126788  [132220/744121]\n",
      "loss: 1496.887794  [154220/744121]\n",
      "loss: 1708.147807  [176220/744121]\n",
      "loss: 1919.044420  [198220/744121]\n",
      "loss: 2131.658188  [220220/744121]\n",
      "loss: 2344.176556  [242220/744121]\n",
      "loss: 2554.925629  [264220/744121]\n",
      "loss: 2764.081932  [286220/744121]\n",
      "loss: 2972.847672  [308220/744121]\n",
      "loss: 3180.434433  [330220/744121]\n",
      "loss: 3390.019786  [352220/744121]\n",
      "loss: 3596.124862  [374220/744121]\n",
      "loss: 3805.847783  [396220/744121]\n",
      "loss: 4012.669410  [418220/744121]\n",
      "loss: 4218.638399  [440220/744121]\n",
      "loss: 4423.935837  [462220/744121]\n",
      "loss: 4634.115819  [484220/744121]\n",
      "loss: 4839.326761  [506220/744121]\n",
      "loss: 5043.573154  [528220/744121]\n",
      "loss: 5246.183942  [550220/744121]\n",
      "loss: 5451.218251  [572220/744121]\n",
      "loss: 5653.743508  [594220/744121]\n",
      "loss: 5855.940346  [616220/744121]\n",
      "loss: 6058.091816  [638220/744121]\n",
      "loss: 6261.605701  [660220/744121]\n",
      "loss: 6461.134274  [682220/744121]\n",
      "loss: 6663.379090  [704220/744121]\n",
      "loss: 6867.446804  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 1.267116 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.267612  [  220/744121]\n",
      "loss: 201.669213  [22220/744121]\n",
      "loss: 405.790997  [44220/744121]\n",
      "loss: 607.463523  [66220/744121]\n",
      "loss: 806.171943  [88220/744121]\n",
      "loss: 1007.821716  [110220/744121]\n",
      "loss: 1205.572787  [132220/744121]\n",
      "loss: 1405.452123  [154220/744121]\n",
      "loss: 1606.889515  [176220/744121]\n",
      "loss: 1807.576338  [198220/744121]\n",
      "loss: 2006.210306  [220220/744121]\n",
      "loss: 2205.980288  [242220/744121]\n",
      "loss: 2404.036709  [264220/744121]\n",
      "loss: 2601.348404  [286220/744121]\n",
      "loss: 2800.038738  [308220/744121]\n",
      "loss: 2994.678989  [330220/744121]\n",
      "loss: 3192.516199  [352220/744121]\n",
      "loss: 3391.587311  [374220/744121]\n",
      "loss: 3590.609679  [396220/744121]\n",
      "loss: 3783.572881  [418220/744121]\n",
      "loss: 3978.886867  [440220/744121]\n",
      "loss: 4174.077579  [462220/744121]\n",
      "loss: 4372.336374  [484220/744121]\n",
      "loss: 4568.989933  [506220/744121]\n",
      "loss: 4763.571472  [528220/744121]\n",
      "loss: 4956.053226  [550220/744121]\n",
      "loss: 5151.534558  [572220/744121]\n",
      "loss: 5345.746877  [594220/744121]\n",
      "loss: 5540.635314  [616220/744121]\n",
      "loss: 5735.128221  [638220/744121]\n",
      "loss: 5928.125030  [660220/744121]\n",
      "loss: 6119.402248  [682220/744121]\n",
      "loss: 6314.111799  [704220/744121]\n",
      "loss: 6510.288009  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.149983 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.030566  [  220/744121]\n",
      "loss: 195.062589  [22220/744121]\n",
      "loss: 387.550963  [44220/744121]\n",
      "loss: 577.342323  [66220/744121]\n",
      "loss: 769.284932  [88220/744121]\n",
      "loss: 962.658608  [110220/744121]\n",
      "loss: 1154.797911  [132220/744121]\n",
      "loss: 1346.467635  [154220/744121]\n",
      "loss: 1541.410303  [176220/744121]\n",
      "loss: 1732.299269  [198220/744121]\n",
      "loss: 1926.295264  [220220/744121]\n",
      "loss: 2118.181143  [242220/744121]\n",
      "loss: 2309.270744  [264220/744121]\n",
      "loss: 2497.835426  [286220/744121]\n",
      "loss: 2686.019532  [308220/744121]\n",
      "loss: 2875.490642  [330220/744121]\n",
      "loss: 3061.575576  [352220/744121]\n",
      "loss: 3249.576904  [374220/744121]\n",
      "loss: 3442.209152  [396220/744121]\n",
      "loss: 3628.374972  [418220/744121]\n",
      "loss: 3816.538058  [440220/744121]\n",
      "loss: 4006.196578  [462220/744121]\n",
      "loss: 4198.648784  [484220/744121]\n",
      "loss: 4386.156667  [506220/744121]\n",
      "loss: 4575.506544  [528220/744121]\n",
      "loss: 4762.383354  [550220/744121]\n",
      "loss: 4949.373014  [572220/744121]\n",
      "loss: 5135.478154  [594220/744121]\n",
      "loss: 5322.721633  [616220/744121]\n",
      "loss: 5509.743046  [638220/744121]\n",
      "loss: 5697.123780  [660220/744121]\n",
      "loss: 5880.939394  [682220/744121]\n",
      "loss: 6069.030110  [704220/744121]\n",
      "loss: 6258.842978  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 1.201309 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.977287  [  220/744121]\n",
      "loss: 187.332942  [22220/744121]\n",
      "loss: 372.103453  [44220/744121]\n",
      "loss: 556.812017  [66220/744121]\n",
      "loss: 742.606936  [88220/744121]\n",
      "loss: 931.401003  [110220/744121]\n",
      "loss: 1117.264283  [132220/744121]\n",
      "loss: 1304.023042  [154220/744121]\n",
      "loss: 1490.948129  [176220/744121]\n",
      "loss: 1674.755094  [198220/744121]\n",
      "loss: 1860.451506  [220220/744121]\n",
      "loss: 2045.340110  [242220/744121]\n",
      "loss: 2230.193851  [264220/744121]\n",
      "loss: 2413.073255  [286220/744121]\n",
      "loss: 2596.703385  [308220/744121]\n",
      "loss: 2779.333757  [330220/744121]\n",
      "loss: 2961.661501  [352220/744121]\n",
      "loss: 3145.737033  [374220/744121]\n",
      "loss: 3331.846465  [396220/744121]\n",
      "loss: 3513.153353  [418220/744121]\n",
      "loss: 3697.739659  [440220/744121]\n",
      "loss: 3882.246387  [462220/744121]\n",
      "loss: 4068.600361  [484220/744121]\n",
      "loss: 4249.350480  [506220/744121]\n",
      "loss: 4433.090773  [528220/744121]\n",
      "loss: 4615.292506  [550220/744121]\n",
      "loss: 4797.469579  [572220/744121]\n",
      "loss: 4979.295137  [594220/744121]\n",
      "loss: 5161.262895  [616220/744121]\n",
      "loss: 5343.000384  [638220/744121]\n",
      "loss: 5524.288258  [660220/744121]\n",
      "loss: 5706.025766  [682220/744121]\n",
      "loss: 5887.477950  [704220/744121]\n",
      "loss: 6071.073875  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 1.033563 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.888414  [  220/744121]\n",
      "loss: 182.980496  [22220/744121]\n",
      "loss: 363.509650  [44220/744121]\n",
      "loss: 545.569701  [66220/744121]\n",
      "loss: 726.139598  [88220/744121]\n",
      "loss: 909.419572  [110220/744121]\n",
      "loss: 1088.203153  [132220/744121]\n",
      "loss: 1267.067371  [154220/744121]\n",
      "loss: 1446.380200  [176220/744121]\n",
      "loss: 1625.653430  [198220/744121]\n",
      "loss: 1805.208058  [220220/744121]\n",
      "loss: 1984.201654  [242220/744121]\n",
      "loss: 2162.819277  [264220/744121]\n",
      "loss: 2343.659253  [286220/744121]\n",
      "loss: 2523.605061  [308220/744121]\n",
      "loss: 2702.014011  [330220/744121]\n",
      "loss: 2880.719976  [352220/744121]\n",
      "loss: 3059.827186  [374220/744121]\n",
      "loss: 3242.813165  [396220/744121]\n",
      "loss: 3422.439990  [418220/744121]\n",
      "loss: 3602.325921  [440220/744121]\n",
      "loss: 3780.882774  [462220/744121]\n",
      "loss: 3963.878910  [484220/744121]\n",
      "loss: 4139.550447  [506220/744121]\n",
      "loss: 4317.380807  [528220/744121]\n",
      "loss: 4493.883793  [550220/744121]\n",
      "loss: 4672.512313  [572220/744121]\n",
      "loss: 4849.304357  [594220/744121]\n",
      "loss: 5029.016443  [616220/744121]\n",
      "loss: 5205.334509  [638220/744121]\n",
      "loss: 5385.372812  [660220/744121]\n",
      "loss: 5560.990253  [682220/744121]\n",
      "loss: 5741.811568  [704220/744121]\n",
      "loss: 5921.738220  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.956300 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.801528  [  220/744121]\n",
      "loss: 179.010831  [22220/744121]\n",
      "loss: 355.357097  [44220/744121]\n",
      "loss: 531.708006  [66220/744121]\n",
      "loss: 709.687175  [88220/744121]\n",
      "loss: 892.673084  [110220/744121]\n",
      "loss: 1069.505561  [132220/744121]\n",
      "loss: 1247.225924  [154220/744121]\n",
      "loss: 1424.413919  [176220/744121]\n",
      "loss: 1599.294340  [198220/744121]\n",
      "loss: 1776.977532  [220220/744121]\n",
      "loss: 1953.144516  [242220/744121]\n",
      "loss: 2129.383568  [264220/744121]\n",
      "loss: 2306.026761  [286220/744121]\n",
      "loss: 2480.222267  [308220/744121]\n",
      "loss: 2653.898341  [330220/744121]\n",
      "loss: 2828.936042  [352220/744121]\n",
      "loss: 3006.355312  [374220/744121]\n",
      "loss: 3184.688140  [396220/744121]\n",
      "loss: 3361.692799  [418220/744121]\n",
      "loss: 3538.480389  [440220/744121]\n",
      "loss: 3717.137098  [462220/744121]\n",
      "loss: 3893.001658  [484220/744121]\n",
      "loss: 4065.104920  [506220/744121]\n",
      "loss: 4240.165609  [528220/744121]\n",
      "loss: 4416.250503  [550220/744121]\n",
      "loss: 4590.462513  [572220/744121]\n",
      "loss: 4765.407142  [594220/744121]\n",
      "loss: 4941.848967  [616220/744121]\n",
      "loss: 5114.763078  [638220/744121]\n",
      "loss: 5290.563130  [660220/744121]\n",
      "loss: 5464.882778  [682220/744121]\n",
      "loss: 5642.869098  [704220/744121]\n",
      "loss: 5818.160244  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 1.002630 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.859191  [  220/744121]\n",
      "loss: 176.080607  [22220/744121]\n",
      "loss: 350.688266  [44220/744121]\n",
      "loss: 522.830254  [66220/744121]\n",
      "loss: 697.302359  [88220/744121]\n",
      "loss: 876.087036  [110220/744121]\n",
      "loss: 1048.602937  [132220/744121]\n",
      "loss: 1223.874390  [154220/744121]\n",
      "loss: 1397.135994  [176220/744121]\n",
      "loss: 1570.804188  [198220/744121]\n",
      "loss: 1745.495025  [220220/744121]\n",
      "loss: 1921.030602  [242220/744121]\n",
      "loss: 2091.136562  [264220/744121]\n",
      "loss: 2263.986277  [286220/744121]\n",
      "loss: 2438.046428  [308220/744121]\n",
      "loss: 2608.564377  [330220/744121]\n",
      "loss: 2780.480622  [352220/744121]\n",
      "loss: 2953.361380  [374220/744121]\n",
      "loss: 3129.344451  [396220/744121]\n",
      "loss: 3301.266054  [418220/744121]\n",
      "loss: 3474.046619  [440220/744121]\n",
      "loss: 3647.117380  [462220/744121]\n",
      "loss: 3824.509300  [484220/744121]\n",
      "loss: 3995.430181  [506220/744121]\n",
      "loss: 4169.519493  [528220/744121]\n",
      "loss: 4341.889558  [550220/744121]\n",
      "loss: 4514.452268  [572220/744121]\n",
      "loss: 4687.536753  [594220/744121]\n",
      "loss: 4861.716945  [616220/744121]\n",
      "loss: 5034.038060  [638220/744121]\n",
      "loss: 5208.053871  [660220/744121]\n",
      "loss: 5377.702746  [682220/744121]\n",
      "loss: 5547.727875  [704220/744121]\n",
      "loss: 5722.819916  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1.057878 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.882211  [  220/744121]\n",
      "loss: 176.701254  [22220/744121]\n",
      "loss: 350.369706  [44220/744121]\n",
      "loss: 522.257857  [66220/744121]\n",
      "loss: 692.057639  [88220/744121]\n",
      "loss: 866.384264  [110220/744121]\n",
      "loss: 1037.982596  [132220/744121]\n",
      "loss: 1213.086834  [154220/744121]\n",
      "loss: 1386.266420  [176220/744121]\n",
      "loss: 1556.973559  [198220/744121]\n",
      "loss: 1729.426589  [220220/744121]\n",
      "loss: 1900.923424  [242220/744121]\n",
      "loss: 2070.682824  [264220/744121]\n",
      "loss: 2243.963723  [286220/744121]\n",
      "loss: 2413.974574  [308220/744121]\n",
      "loss: 2583.938880  [330220/744121]\n",
      "loss: 2753.098370  [352220/744121]\n",
      "loss: 2923.932707  [374220/744121]\n",
      "loss: 3097.367420  [396220/744121]\n",
      "loss: 3268.389794  [418220/744121]\n",
      "loss: 3438.664734  [440220/744121]\n",
      "loss: 3608.192926  [462220/744121]\n",
      "loss: 3782.369975  [484220/744121]\n",
      "loss: 3953.829291  [506220/744121]\n",
      "loss: 4123.820955  [528220/744121]\n",
      "loss: 4294.424726  [550220/744121]\n",
      "loss: 4466.586104  [572220/744121]\n",
      "loss: 4634.342072  [594220/744121]\n",
      "loss: 4803.360366  [616220/744121]\n",
      "loss: 4971.598290  [638220/744121]\n",
      "loss: 5142.944602  [660220/744121]\n",
      "loss: 5311.791050  [682220/744121]\n",
      "loss: 5479.911678  [704220/744121]\n",
      "loss: 5651.347663  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.950632 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.717872  [  220/744121]\n",
      "loss: 171.564977  [22220/744121]\n",
      "loss: 341.003268  [44220/744121]\n",
      "loss: 510.298254  [66220/744121]\n",
      "loss: 682.173378  [88220/744121]\n",
      "loss: 854.593879  [110220/744121]\n",
      "loss: 1024.559026  [132220/744121]\n",
      "loss: 1196.881200  [154220/744121]\n",
      "loss: 1367.942481  [176220/744121]\n",
      "loss: 1536.391640  [198220/744121]\n",
      "loss: 1704.429189  [220220/744121]\n",
      "loss: 1878.202744  [242220/744121]\n",
      "loss: 2048.546557  [264220/744121]\n",
      "loss: 2218.496238  [286220/744121]\n",
      "loss: 2387.127663  [308220/744121]\n",
      "loss: 2555.294667  [330220/744121]\n",
      "loss: 2722.932045  [352220/744121]\n",
      "loss: 2892.319618  [374220/744121]\n",
      "loss: 3063.463109  [396220/744121]\n",
      "loss: 3230.701904  [418220/744121]\n",
      "loss: 3400.234011  [440220/744121]\n",
      "loss: 3570.902586  [462220/744121]\n",
      "loss: 3743.359565  [484220/744121]\n",
      "loss: 3909.583884  [506220/744121]\n",
      "loss: 4079.211946  [528220/744121]\n",
      "loss: 4246.518594  [550220/744121]\n",
      "loss: 4414.434773  [572220/744121]\n",
      "loss: 4582.037363  [594220/744121]\n",
      "loss: 4752.730094  [616220/744121]\n",
      "loss: 4919.648511  [638220/744121]\n",
      "loss: 5088.503591  [660220/744121]\n",
      "loss: 5255.725236  [682220/744121]\n",
      "loss: 5424.833930  [704220/744121]\n",
      "loss: 5595.477189  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.909389 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.894185  [  220/744121]\n",
      "loss: 170.448248  [22220/744121]\n",
      "loss: 339.063862  [44220/744121]\n",
      "loss: 504.304997  [66220/744121]\n",
      "loss: 670.759960  [88220/744121]\n",
      "loss: 840.919459  [110220/744121]\n",
      "loss: 1008.437734  [132220/744121]\n",
      "loss: 1177.714474  [154220/744121]\n",
      "loss: 1345.962641  [176220/744121]\n",
      "loss: 1512.906524  [198220/744121]\n",
      "loss: 1682.421275  [220220/744121]\n",
      "loss: 1849.168553  [242220/744121]\n",
      "loss: 2016.513294  [264220/744121]\n",
      "loss: 2182.210392  [286220/744121]\n",
      "loss: 2347.862650  [308220/744121]\n",
      "loss: 2516.256324  [330220/744121]\n",
      "loss: 2681.509471  [352220/744121]\n",
      "loss: 2849.171597  [374220/744121]\n",
      "loss: 3018.369547  [396220/744121]\n",
      "loss: 3183.577291  [418220/744121]\n",
      "loss: 3351.900362  [440220/744121]\n",
      "loss: 3519.399083  [462220/744121]\n",
      "loss: 3690.299484  [484220/744121]\n",
      "loss: 3855.765050  [506220/744121]\n",
      "loss: 4023.314474  [528220/744121]\n",
      "loss: 4191.270886  [550220/744121]\n",
      "loss: 4358.584948  [572220/744121]\n",
      "loss: 4523.671697  [594220/744121]\n",
      "loss: 4691.528475  [616220/744121]\n",
      "loss: 4857.645580  [638220/744121]\n",
      "loss: 5023.477805  [660220/744121]\n",
      "loss: 5189.054549  [682220/744121]\n",
      "loss: 5356.572301  [704220/744121]\n",
      "loss: 5527.931115  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.909163 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.774300  [  220/744121]\n",
      "loss: 168.820327  [22220/744121]\n",
      "loss: 336.962916  [44220/744121]\n",
      "loss: 500.667288  [66220/744121]\n",
      "loss: 667.605600  [88220/744121]\n",
      "loss: 834.345165  [110220/744121]\n",
      "loss: 997.813736  [132220/744121]\n",
      "loss: 1166.231629  [154220/744121]\n",
      "loss: 1332.207906  [176220/744121]\n",
      "loss: 1498.786976  [198220/744121]\n",
      "loss: 1664.193894  [220220/744121]\n",
      "loss: 1830.807526  [242220/744121]\n",
      "loss: 1995.532203  [264220/744121]\n",
      "loss: 2160.264076  [286220/744121]\n",
      "loss: 2324.902661  [308220/744121]\n",
      "loss: 2490.326325  [330220/744121]\n",
      "loss: 2654.383197  [352220/744121]\n",
      "loss: 2819.775051  [374220/744121]\n",
      "loss: 2985.728965  [396220/744121]\n",
      "loss: 3150.236375  [418220/744121]\n",
      "loss: 3317.508438  [440220/744121]\n",
      "loss: 3485.021107  [462220/744121]\n",
      "loss: 3653.746454  [484220/744121]\n",
      "loss: 3818.602543  [506220/744121]\n",
      "loss: 3985.410493  [528220/744121]\n",
      "loss: 4152.902643  [550220/744121]\n",
      "loss: 4318.897445  [572220/744121]\n",
      "loss: 4484.218875  [594220/744121]\n",
      "loss: 4649.427506  [616220/744121]\n",
      "loss: 4814.638758  [638220/744121]\n",
      "loss: 4978.685765  [660220/744121]\n",
      "loss: 5142.415214  [682220/744121]\n",
      "loss: 5307.565658  [704220/744121]\n",
      "loss: 5475.239744  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.850142 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.783014  [  220/744121]\n",
      "loss: 166.720419  [22220/744121]\n",
      "loss: 331.337483  [44220/744121]\n",
      "loss: 495.642938  [66220/744121]\n",
      "loss: 660.101126  [88220/744121]\n",
      "loss: 826.819073  [110220/744121]\n",
      "loss: 988.972987  [132220/744121]\n",
      "loss: 1156.928011  [154220/744121]\n",
      "loss: 1324.631214  [176220/744121]\n",
      "loss: 1489.507649  [198220/744121]\n",
      "loss: 1654.068168  [220220/744121]\n",
      "loss: 1818.008940  [242220/744121]\n",
      "loss: 1982.528710  [264220/744121]\n",
      "loss: 2146.800348  [286220/744121]\n",
      "loss: 2310.698481  [308220/744121]\n",
      "loss: 2473.511684  [330220/744121]\n",
      "loss: 2637.107484  [352220/744121]\n",
      "loss: 2801.392438  [374220/744121]\n",
      "loss: 2966.235809  [396220/744121]\n",
      "loss: 3130.690887  [418220/744121]\n",
      "loss: 3295.965899  [440220/744121]\n",
      "loss: 3461.184983  [462220/744121]\n",
      "loss: 3627.713535  [484220/744121]\n",
      "loss: 3791.271988  [506220/744121]\n",
      "loss: 3954.992183  [528220/744121]\n",
      "loss: 4119.003968  [550220/744121]\n",
      "loss: 4282.800377  [572220/744121]\n",
      "loss: 4445.787534  [594220/744121]\n",
      "loss: 4611.710844  [616220/744121]\n",
      "loss: 4774.801299  [638220/744121]\n",
      "loss: 4937.940910  [660220/744121]\n",
      "loss: 5101.263238  [682220/744121]\n",
      "loss: 5266.129912  [704220/744121]\n",
      "loss: 5431.482902  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.892122 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.865652  [  220/744121]\n",
      "loss: 166.414207  [22220/744121]\n",
      "loss: 329.257377  [44220/744121]\n",
      "loss: 493.669677  [66220/744121]\n",
      "loss: 657.943222  [88220/744121]\n",
      "loss: 822.568400  [110220/744121]\n",
      "loss: 987.789093  [132220/744121]\n",
      "loss: 1151.441593  [154220/744121]\n",
      "loss: 1314.002704  [176220/744121]\n",
      "loss: 1478.340302  [198220/744121]\n",
      "loss: 1640.270428  [220220/744121]\n",
      "loss: 1804.644636  [242220/744121]\n",
      "loss: 1968.752748  [264220/744121]\n",
      "loss: 2131.059342  [286220/744121]\n",
      "loss: 2293.904441  [308220/744121]\n",
      "loss: 2456.767352  [330220/744121]\n",
      "loss: 2620.604568  [352220/744121]\n",
      "loss: 2784.675870  [374220/744121]\n",
      "loss: 2949.653023  [396220/744121]\n",
      "loss: 3111.035009  [418220/744121]\n",
      "loss: 3274.831713  [440220/744121]\n",
      "loss: 3438.019102  [462220/744121]\n",
      "loss: 3604.730740  [484220/744121]\n",
      "loss: 3766.118458  [506220/744121]\n",
      "loss: 3930.538822  [528220/744121]\n",
      "loss: 4091.918624  [550220/744121]\n",
      "loss: 4255.629851  [572220/744121]\n",
      "loss: 4419.759081  [594220/744121]\n",
      "loss: 4584.244057  [616220/744121]\n",
      "loss: 4744.705158  [638220/744121]\n",
      "loss: 4907.163173  [660220/744121]\n",
      "loss: 5066.584228  [682220/744121]\n",
      "loss: 5229.961420  [704220/744121]\n",
      "loss: 5395.208793  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.903735 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.830740  [  220/744121]\n",
      "loss: 166.829865  [22220/744121]\n",
      "loss: 331.985491  [44220/744121]\n",
      "loss: 494.942639  [66220/744121]\n",
      "loss: 657.717829  [88220/744121]\n",
      "loss: 820.948411  [110220/744121]\n",
      "loss: 982.286214  [132220/744121]\n",
      "loss: 1145.564184  [154220/744121]\n",
      "loss: 1310.319944  [176220/744121]\n",
      "loss: 1472.113987  [198220/744121]\n",
      "loss: 1634.493875  [220220/744121]\n",
      "loss: 1798.312253  [242220/744121]\n",
      "loss: 1960.669200  [264220/744121]\n",
      "loss: 2122.675853  [286220/744121]\n",
      "loss: 2282.974206  [308220/744121]\n",
      "loss: 2444.481493  [330220/744121]\n",
      "loss: 2607.097254  [352220/744121]\n",
      "loss: 2767.439681  [374220/744121]\n",
      "loss: 2930.807676  [396220/744121]\n",
      "loss: 3092.965286  [418220/744121]\n",
      "loss: 3257.015902  [440220/744121]\n",
      "loss: 3419.980409  [462220/744121]\n",
      "loss: 3585.635302  [484220/744121]\n",
      "loss: 3747.945578  [506220/744121]\n",
      "loss: 3910.450212  [528220/744121]\n",
      "loss: 4073.387140  [550220/744121]\n",
      "loss: 4235.682628  [572220/744121]\n",
      "loss: 4396.757325  [594220/744121]\n",
      "loss: 4562.137299  [616220/744121]\n",
      "loss: 4722.947155  [638220/744121]\n",
      "loss: 4887.273504  [660220/744121]\n",
      "loss: 5047.652533  [682220/744121]\n",
      "loss: 5209.057993  [704220/744121]\n",
      "loss: 5370.796181  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.840199 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.745325  [  220/744121]\n",
      "loss: 164.866477  [22220/744121]\n",
      "loss: 326.351631  [44220/744121]\n",
      "loss: 488.067950  [66220/744121]\n",
      "loss: 649.850502  [88220/744121]\n",
      "loss: 812.001853  [110220/744121]\n",
      "loss: 972.754759  [132220/744121]\n",
      "loss: 1135.906917  [154220/744121]\n",
      "loss: 1299.645293  [176220/744121]\n",
      "loss: 1461.837764  [198220/744121]\n",
      "loss: 1621.881830  [220220/744121]\n",
      "loss: 1783.948094  [242220/744121]\n",
      "loss: 1943.625150  [264220/744121]\n",
      "loss: 2104.252816  [286220/744121]\n",
      "loss: 2265.199298  [308220/744121]\n",
      "loss: 2423.512874  [330220/744121]\n",
      "loss: 2584.623008  [352220/744121]\n",
      "loss: 2747.980334  [374220/744121]\n",
      "loss: 2911.395527  [396220/744121]\n",
      "loss: 3072.541616  [418220/744121]\n",
      "loss: 3235.677263  [440220/744121]\n",
      "loss: 3397.606496  [462220/744121]\n",
      "loss: 3560.826318  [484220/744121]\n",
      "loss: 3720.697538  [506220/744121]\n",
      "loss: 3882.007762  [528220/744121]\n",
      "loss: 4040.888314  [550220/744121]\n",
      "loss: 4203.203785  [572220/744121]\n",
      "loss: 4363.112621  [594220/744121]\n",
      "loss: 4524.504316  [616220/744121]\n",
      "loss: 4685.604311  [638220/744121]\n",
      "loss: 4848.102095  [660220/744121]\n",
      "loss: 5007.388941  [682220/744121]\n",
      "loss: 5168.987090  [704220/744121]\n",
      "loss: 5333.159782  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.864556 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.799235  [  220/744121]\n",
      "loss: 163.463377  [22220/744121]\n",
      "loss: 326.193334  [44220/744121]\n",
      "loss: 486.134658  [66220/744121]\n",
      "loss: 649.396569  [88220/744121]\n",
      "loss: 809.643278  [110220/744121]\n",
      "loss: 969.373931  [132220/744121]\n",
      "loss: 1130.290392  [154220/744121]\n",
      "loss: 1291.854466  [176220/744121]\n",
      "loss: 1452.256667  [198220/744121]\n",
      "loss: 1611.754434  [220220/744121]\n",
      "loss: 1776.247132  [242220/744121]\n",
      "loss: 1936.118878  [264220/744121]\n",
      "loss: 2098.063601  [286220/744121]\n",
      "loss: 2259.905304  [308220/744121]\n",
      "loss: 2420.139727  [330220/744121]\n",
      "loss: 2580.813092  [352220/744121]\n",
      "loss: 2743.409989  [374220/744121]\n",
      "loss: 2907.977999  [396220/744121]\n",
      "loss: 3067.841471  [418220/744121]\n",
      "loss: 3229.889691  [440220/744121]\n",
      "loss: 3389.625596  [462220/744121]\n",
      "loss: 3552.762543  [484220/744121]\n",
      "loss: 3713.173358  [506220/744121]\n",
      "loss: 3875.257340  [528220/744121]\n",
      "loss: 4037.153018  [550220/744121]\n",
      "loss: 4197.407152  [572220/744121]\n",
      "loss: 4356.497048  [594220/744121]\n",
      "loss: 4516.917358  [616220/744121]\n",
      "loss: 4673.902509  [638220/744121]\n",
      "loss: 4836.463034  [660220/744121]\n",
      "loss: 4996.090836  [682220/744121]\n",
      "loss: 5156.172598  [704220/744121]\n",
      "loss: 5320.407907  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.793307 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.559292  [  220/744121]\n",
      "loss: 160.919334  [22220/744121]\n",
      "loss: 320.311554  [44220/744121]\n",
      "loss: 479.064828  [66220/744121]\n",
      "loss: 638.826567  [88220/744121]\n",
      "loss: 800.670483  [110220/744121]\n",
      "loss: 958.732666  [132220/744121]\n",
      "loss: 1119.169292  [154220/744121]\n",
      "loss: 1281.624521  [176220/744121]\n",
      "loss: 1442.522388  [198220/744121]\n",
      "loss: 1602.265104  [220220/744121]\n",
      "loss: 1762.152493  [242220/744121]\n",
      "loss: 1919.506824  [264220/744121]\n",
      "loss: 2077.961274  [286220/744121]\n",
      "loss: 2237.981984  [308220/744121]\n",
      "loss: 2395.420203  [330220/744121]\n",
      "loss: 2553.418899  [352220/744121]\n",
      "loss: 2713.777693  [374220/744121]\n",
      "loss: 2874.767255  [396220/744121]\n",
      "loss: 3034.728223  [418220/744121]\n",
      "loss: 3194.590101  [440220/744121]\n",
      "loss: 3355.155884  [462220/744121]\n",
      "loss: 3515.014109  [484220/744121]\n",
      "loss: 3674.467987  [506220/744121]\n",
      "loss: 3835.179082  [528220/744121]\n",
      "loss: 3992.433565  [550220/744121]\n",
      "loss: 4153.246039  [572220/744121]\n",
      "loss: 4313.828039  [594220/744121]\n",
      "loss: 4472.316057  [616220/744121]\n",
      "loss: 4630.567299  [638220/744121]\n",
      "loss: 4790.576063  [660220/744121]\n",
      "loss: 4949.826269  [682220/744121]\n",
      "loss: 5107.942840  [704220/744121]\n",
      "loss: 5269.420416  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.787623 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.005660  [  220/744121]\n",
      "loss: 160.507771  [22220/744121]\n",
      "loss: 321.301475  [44220/744121]\n",
      "loss: 479.478663  [66220/744121]\n",
      "loss: 638.343480  [88220/744121]\n",
      "loss: 798.921104  [110220/744121]\n",
      "loss: 956.436833  [132220/744121]\n",
      "loss: 1115.320516  [154220/744121]\n",
      "loss: 1276.929405  [176220/744121]\n",
      "loss: 1434.859144  [198220/744121]\n",
      "loss: 1594.976642  [220220/744121]\n",
      "loss: 1752.288379  [242220/744121]\n",
      "loss: 1909.367595  [264220/744121]\n",
      "loss: 2067.936929  [286220/744121]\n",
      "loss: 2225.831565  [308220/744121]\n",
      "loss: 2382.821227  [330220/744121]\n",
      "loss: 2542.287342  [352220/744121]\n",
      "loss: 2700.408397  [374220/744121]\n",
      "loss: 2860.392398  [396220/744121]\n",
      "loss: 3021.824965  [418220/744121]\n",
      "loss: 3180.826349  [440220/744121]\n",
      "loss: 3339.917886  [462220/744121]\n",
      "loss: 3502.254627  [484220/744121]\n",
      "loss: 3660.603078  [506220/744121]\n",
      "loss: 3819.628391  [528220/744121]\n",
      "loss: 3979.657482  [550220/744121]\n",
      "loss: 4137.504727  [572220/744121]\n",
      "loss: 4294.053669  [594220/744121]\n",
      "loss: 4452.480883  [616220/744121]\n",
      "loss: 4614.496454  [638220/744121]\n",
      "loss: 4773.763183  [660220/744121]\n",
      "loss: 4931.791740  [682220/744121]\n",
      "loss: 5089.745770  [704220/744121]\n",
      "loss: 5250.116203  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.801048 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.825294  [  220/744121]\n",
      "loss: 162.641236  [22220/744121]\n",
      "loss: 322.807678  [44220/744121]\n",
      "loss: 479.291303  [66220/744121]\n",
      "loss: 640.194244  [88220/744121]\n",
      "loss: 800.811875  [110220/744121]\n",
      "loss: 958.908787  [132220/744121]\n",
      "loss: 1117.342160  [154220/744121]\n",
      "loss: 1276.858838  [176220/744121]\n",
      "loss: 1435.676015  [198220/744121]\n",
      "loss: 1594.230960  [220220/744121]\n",
      "loss: 1754.539122  [242220/744121]\n",
      "loss: 1913.322546  [264220/744121]\n",
      "loss: 2071.426332  [286220/744121]\n",
      "loss: 2228.352878  [308220/744121]\n",
      "loss: 2382.777205  [330220/744121]\n",
      "loss: 2538.727021  [352220/744121]\n",
      "loss: 2696.234718  [374220/744121]\n",
      "loss: 2856.889635  [396220/744121]\n",
      "loss: 3014.286249  [418220/744121]\n",
      "loss: 3172.594156  [440220/744121]\n",
      "loss: 3330.778406  [462220/744121]\n",
      "loss: 3491.288262  [484220/744121]\n",
      "loss: 3650.141832  [506220/744121]\n",
      "loss: 3808.290988  [528220/744121]\n",
      "loss: 3965.375915  [550220/744121]\n",
      "loss: 4124.296967  [572220/744121]\n",
      "loss: 4283.566227  [594220/744121]\n",
      "loss: 4442.314668  [616220/744121]\n",
      "loss: 4601.352781  [638220/744121]\n",
      "loss: 4759.119802  [660220/744121]\n",
      "loss: 4915.671952  [682220/744121]\n",
      "loss: 5072.723484  [704220/744121]\n",
      "loss: 5232.393138  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.799825 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.559977  [  220/744121]\n",
      "loss: 161.049734  [22220/744121]\n",
      "loss: 320.622717  [44220/744121]\n",
      "loss: 477.706085  [66220/744121]\n",
      "loss: 635.136059  [88220/744121]\n",
      "loss: 794.466803  [110220/744121]\n",
      "loss: 951.663977  [132220/744121]\n",
      "loss: 1109.364992  [154220/744121]\n",
      "loss: 1266.929632  [176220/744121]\n",
      "loss: 1425.075763  [198220/744121]\n",
      "loss: 1581.756725  [220220/744121]\n",
      "loss: 1743.292381  [242220/744121]\n",
      "loss: 1899.631949  [264220/744121]\n",
      "loss: 2056.822750  [286220/744121]\n",
      "loss: 2213.512145  [308220/744121]\n",
      "loss: 2369.506283  [330220/744121]\n",
      "loss: 2525.538231  [352220/744121]\n",
      "loss: 2681.632581  [374220/744121]\n",
      "loss: 2838.804520  [396220/744121]\n",
      "loss: 2995.185231  [418220/744121]\n",
      "loss: 3155.646891  [440220/744121]\n",
      "loss: 3312.212694  [462220/744121]\n",
      "loss: 3471.481366  [484220/744121]\n",
      "loss: 3628.904021  [506220/744121]\n",
      "loss: 3786.840599  [528220/744121]\n",
      "loss: 3941.139879  [550220/744121]\n",
      "loss: 4098.637652  [572220/744121]\n",
      "loss: 4256.904996  [594220/744121]\n",
      "loss: 4415.528824  [616220/744121]\n",
      "loss: 4571.666513  [638220/744121]\n",
      "loss: 4730.957505  [660220/744121]\n",
      "loss: 4886.318767  [682220/744121]\n",
      "loss: 5045.263720  [704220/744121]\n",
      "loss: 5204.236811  [726220/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.794489 \n",
      "\n",
      "Early stopping at epoch: 22\n",
      "losses [3.380164178842065, 2.5830568063903976, 2.2696209502156726, 2.0795433154093454, 1.9706833609595935, 1.8954651844138546, 1.8387895494109245, 1.7935582865916022, 1.7625047548825734, 1.7334231626624508, 1.7121189114894524, 1.6948991741895887, 1.6747456889796672, 1.6587036357558835, 1.6458249207329476, 1.634489723186397, 1.6274474369433023, 1.6153143120347626, 1.6118642634764742, 1.5963132969557163, 1.590008414676124, 1.584796281527599, 1.5770895988885516]\n",
      "test_losses [2.4429443732623395, 1.769107478076014, 1.4417177043289975, 1.26711595514725, 1.1499833692764414, 1.20130876598687, 1.0335629554452568, 0.9562999813310031, 1.0026304051382788, 1.0578775102105633, 0.9506323423056767, 0.9093891132700033, 0.9091633872328133, 0.850142036306447, 0.8921222205408689, 0.9037353752399313, 0.8401990850218412, 0.864556349269275, 0.7933070587289744, 0.787623092223858, 0.8010476236918876, 0.7998254047591111, 0.7944889641219172]\n",
      "accs [0.3907628925042131, 0.5754105851777195, 0.6616048830662618, 0.6906831074894089, 0.7209953122251743, 0.7064824485283548, 0.7557992508660126, 0.7705750202685473, 0.7689486664964573, 0.7497531950180439, 0.7775676253735351, 0.7808177960696986, 0.7886990160204432, 0.7913460604658835, 0.7771038934608488, 0.7796005256431523, 0.7905782210454094, 0.7840100577930755, 0.8030222467669076, 0.8041262798464635, 0.8000992726767837, 0.8013935131644877, 0.8067488123931434]\n",
      "0.8067488123931434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(nnmodel.defaults)\n",
    "acc = nnmodel.run(nnmodel.defaults, train_ds, test_ds, 100, out=True, name=\"beer_local_res\")\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fc5d8bce-d298-4f7e-b22f-62cd394c76f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.791916 \n",
      "\n",
      "(0.791915774345398, 0.8142797309804871)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.80      0.83      2528\n",
      "           1       0.54      0.92      0.68     10196\n",
      "           2       0.92      0.72      0.81     15084\n",
      "           3       0.75      0.78      0.77      3100\n",
      "           4       0.83      0.87      0.85      8778\n",
      "           5       0.83      0.79      0.81      3821\n",
      "           6       0.83      0.63      0.71      4123\n",
      "           7       0.87      0.79      0.83      8397\n",
      "           8       0.39      0.50      0.44       498\n",
      "           9       0.93      0.88      0.90     28284\n",
      "          10       0.97      0.89      0.93      1869\n",
      "          11       0.92      0.84      0.88     16721\n",
      "          12       0.94      0.88      0.91     38886\n",
      "          13       0.93      0.75      0.83      1325\n",
      "          14       0.86      0.82      0.84     20859\n",
      "          15       0.51      0.59      0.55      3022\n",
      "          16       0.58      0.88      0.70      7963\n",
      "          17       0.78      0.95      0.86     16743\n",
      "          18       0.84      0.85      0.84      8002\n",
      "          19       0.63      0.88      0.73     10498\n",
      "          20       0.23      0.87      0.36      5940\n",
      "          21       0.97      0.78      0.86      3790\n",
      "          22       0.94      0.54      0.69      2109\n",
      "          23       0.92      0.71      0.80      4038\n",
      "          24       0.82      0.55      0.66      6359\n",
      "          25       0.74      0.72      0.73     12398\n",
      "          26       0.94      0.46      0.62     10438\n",
      "          27       0.72      0.94      0.82      1116\n",
      "          28       1.00      0.85      0.92       337\n",
      "          29       0.76      0.69      0.72      2303\n",
      "          30       0.99      0.94      0.96       781\n",
      "          31       0.90      0.80      0.85      3799\n",
      "          32       0.60      0.27      0.38       339\n",
      "          33       0.94      0.78      0.85      1375\n",
      "          34       0.92      0.79      0.85       772\n",
      "          35       0.93      0.82      0.87      1699\n",
      "          36       0.86      0.75      0.80      4213\n",
      "          37       0.76      0.90      0.82      7251\n",
      "          38       0.90      0.74      0.81      1438\n",
      "          39       0.79      0.90      0.84      6542\n",
      "          40       0.92      0.86      0.89      2287\n",
      "          41       0.90      0.96      0.93       841\n",
      "          42       1.00      0.60      0.75      4512\n",
      "          43       0.84      0.63      0.72      2900\n",
      "          44       0.77      0.89      0.82      6549\n",
      "          45       0.89      0.44      0.58       743\n",
      "          46       0.92      0.67      0.78      5263\n",
      "          47       0.85      0.67      0.75      7736\n",
      "          48       0.00      0.00      0.00       253\n",
      "          49       0.93      0.69      0.79      3708\n",
      "          50       0.95      0.34      0.50       996\n",
      "          51       0.88      0.42      0.56      1580\n",
      "          52       0.77      0.59      0.67      1536\n",
      "          53       0.73      0.60      0.66      5995\n",
      "          54       0.78      0.50      0.61       903\n",
      "          55       0.90      0.74      0.82      5615\n",
      "          56       1.00      0.89      0.94       190\n",
      "          57       0.84      0.56      0.67      1669\n",
      "          58       0.97      0.75      0.85      2190\n",
      "          59       0.98      0.68      0.80      1944\n",
      "          60       0.60      0.81      0.69     11248\n",
      "          61       0.84      0.85      0.84      7226\n",
      "          62       0.95      0.95      0.95       220\n",
      "          63       1.00      0.92      0.95      1981\n",
      "          64       0.00      0.00      0.00        72\n",
      "          65       0.86      0.88      0.87      9338\n",
      "          66       0.65      0.49      0.56      3363\n",
      "          67       0.77      0.95      0.85      4217\n",
      "          68       0.90      0.83      0.86      2607\n",
      "          69       0.95      0.81      0.87       524\n",
      "          70       0.98      0.43      0.60       831\n",
      "          71       0.97      0.47      0.63       736\n",
      "          72       0.57      0.10      0.17        82\n",
      "          73       0.95      0.72      0.82      2845\n",
      "          74       0.81      0.92      0.86      3591\n",
      "          75       1.00      0.56      0.71       387\n",
      "          76       0.95      0.92      0.93      4752\n",
      "          77       0.79      0.88      0.83       365\n",
      "          78       0.98      0.73      0.83      3524\n",
      "          79       0.95      0.83      0.88      4298\n",
      "          80       0.86      0.79      0.82      2651\n",
      "          81       0.69      0.76      0.73      2630\n",
      "          82       0.96      0.88      0.92      7781\n",
      "          83       0.98      0.90      0.94      5993\n",
      "          84       0.86      0.82      0.84      4870\n",
      "          85       0.97      0.93      0.95      5053\n",
      "          86       0.94      0.89      0.91      5993\n",
      "          87       0.98      0.71      0.82      1327\n",
      "          88       0.00      0.00      0.00       130\n",
      "          89       0.86      0.93      0.89     17745\n",
      "          90       0.80      0.75      0.77      3338\n",
      "          91       1.00      0.89      0.94       343\n",
      "          92       0.82      0.81      0.81     10300\n",
      "          93       0.73      0.88      0.80      3246\n",
      "          94       0.96      0.85      0.90      5792\n",
      "          95       0.95      0.66      0.78      3024\n",
      "          96       0.89      0.79      0.84       916\n",
      "          97       0.92      0.47      0.62       947\n",
      "          98       0.88      0.89      0.89      9904\n",
      "          99       0.91      0.80      0.85      2965\n",
      "         100       0.64      0.92      0.75      3081\n",
      "         101       0.78      0.75      0.77      1271\n",
      "         102       0.94      0.74      0.83      6874\n",
      "         103       0.94      0.86      0.90     10058\n",
      "\n",
      "    accuracy                           0.81    523583\n",
      "   macro avg       0.83      0.73      0.76    523583\n",
      "weighted avg       0.84      0.81      0.81    523583\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc = test(valid_dataloader, nnmodel.model, nnmodel.loss_fn)\n",
    "print(acc)\n",
    "validate(valid_dataloader, nnmodel.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37921c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "nnmodel.defaults['learning_rate'] = 0.05916\n",
    "nnmodel.defaults['layer'] = [918, 316, 196, 104]\n",
    "nnmodel.defaults['batch_size'] = 220\n",
    "nnmodel.defaults['dropout'] = 0.23525\n",
    "nnmodel.defaults['activation'] = nn.Linear\n",
    "nnmodel.acc_func = acc_func\n",
    "cv_acc = nnmodel.run_cv(nnmodel.defaults, train_small_ds, test_small_ds, epochs=100, k_folds=5)\n",
    "print(cv_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec61608",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/beer_local_search_res_cv.txt', 'w+') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170bc49-1db2-4be9-b9db-9177f9787b6a",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11da447-5530-44d0-8b81-b0126316cbcc",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48156765-d66e-464c-a432-19625f75d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [len(train_small_ds[0][0]), 250, 164, 164, 104]\n",
    "nnmodel = NNModel(layer, device, acc_func=acc_func, loss_func=nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4337d98-4a26-43e2-8c61-f33940f48f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.18205\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.59869\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.94789\n",
      "\n",
      "Parameter Combination (0.001, 320) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.90954\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.92391\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.46196\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.01358\n",
      "\n",
      "Parameter Combination (0.001, 640) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.46648\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.25185\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.74325\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.13113\n",
      "\n",
      "Parameter Combination (0.001, 1280) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.37541\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 46\n",
      "The actual fold accuracy is 1.03151\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 45\n",
      "The actual fold accuracy is 0.93753\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 38\n",
      "The actual fold accuracy is 0.82441\n",
      "\n",
      "Parameter Combination (0.01, 320) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.93115\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.02879\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.90885\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.03684\n",
      "\n",
      "Parameter Combination (0.01, 640) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.99149\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.59571\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.99601\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.80022\n",
      "\n",
      "Parameter Combination (0.01, 1280) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.79731\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 21\n",
      "The actual fold accuracy is 2.08319\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 3.34789\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 15\n",
      "The actual fold accuracy is 1.96527\n",
      "\n",
      "Parameter Combination (0.05, 320) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 2.46545\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 29\n",
      "The actual fold accuracy is 1.72801\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 28\n",
      "The actual fold accuracy is 2.04428\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.84183\n",
      "\n",
      "Parameter Combination (0.05, 640) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 1.53804\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.71587\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 45\n",
      "The actual fold accuracy is 2.14970\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.62168\n",
      "\n",
      "Parameter Combination (0.05, 1280) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 1.16241\n",
      "\n",
      "Grid search took 2.8 minutes.\n",
      "{'learning_rate': 0.05, 'batch_size': 320}\n"
     ]
    }
   ],
   "source": [
    "test_layer = [[len(train_small_ds[0][0]), 250, 164, 164, 104], [len(train_small_ds[0][0]), 25, 16, 16, 104], [len(train_small_ds[0][0]), 250, 164, 104]]\n",
    "dict_param_1 = {\"learning_rate\": [0.001, 0.01, 0.05], \"batch_size\": [320, 640, 1280]}\n",
    "best, acc = nnmodel.grid_search(dict_param_1, train_small_ds, test_small_ds, epochs=50, cv=True, k_folds=3)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72f5955c-6e31-4cd2-8aad-cc54a577caf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 3.00168\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 4.64105\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.64407\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 3.09560\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 2.08413\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 2.81337\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.68734\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.86161\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 2.67620\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 5.15811\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 3.81991\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 3.88474\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 21\n",
      "The actual fold accuracy is 2.89922\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 19\n",
      "The actual fold accuracy is 2.13220\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 22\n",
      "The actual fold accuracy is 1.24111\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.2, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.09084\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 26\n",
      "The actual fold accuracy is 1.96158\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.75846\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 2.37677\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.2, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.69894\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 4.35547\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 44\n",
      "The actual fold accuracy is 4.76518\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 37\n",
      "The actual fold accuracy is 3.43065\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.2, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 4.18377\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 16\n",
      "The actual fold accuracy is 3.22880\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 27\n",
      "The actual fold accuracy is 2.65656\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 18\n",
      "The actual fold accuracy is 0.96757\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.3, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.28431\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.49595\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.05512\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.57567\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.3, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.04225\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 32\n",
      "The actual fold accuracy is 2.99921\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 5.16879\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 4.96022\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.3, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 4.37607\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 32\n",
      "The actual fold accuracy is 2.28679\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 15\n",
      "The actual fold accuracy is 1.15558\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 27\n",
      "The actual fold accuracy is 3.34119\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.5, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.26119\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.28468\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 46\n",
      "The actual fold accuracy is 0.96062\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 43\n",
      "The actual fold accuracy is 1.06598\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.5, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.10376\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 34\n",
      "The actual fold accuracy is 3.12448\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 12\n",
      "The actual fold accuracy is 2.60378\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 17\n",
      "The actual fold accuracy is 2.93774\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.ReLU'>, 0.5, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.88867\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.02295\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.11711\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.89501\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.01169\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.03260\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.88565\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.15900\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.02575\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.96125\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.20487\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.88683\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.01765\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.98472\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.24274\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.85059\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.2, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.02602\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.98688\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.90930\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.10196\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.2, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 0.99938\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.04155\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.94887\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.02262\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.2, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.00435\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.03062\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.89704\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.10412\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.3, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.01059\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.91169\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.16668\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.95823\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.3, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.01220\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.17854\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.97448\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.90144\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.3, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.01815\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 49\n",
      "The actual fold accuracy is 0.94974\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.94519\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.11216\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.5, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.00236\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.21397\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.82151\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.01974\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.5, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.01841\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.98536\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 0.98412\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 48\n",
      "The actual fold accuracy is 1.13403\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.activation.Sigmoid'>, 0.5, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 1.03451\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 45\n",
      "The actual fold accuracy is 6.41733\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 45\n",
      "The actual fold accuracy is 4.49057\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 41\n",
      "The actual fold accuracy is 3.91924\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 4.94238\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 34\n",
      "The actual fold accuracy is 4.23027\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 38\n",
      "The actual fold accuracy is 3.66580\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 7.96653\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 5.28753\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 42\n",
      "The actual fold accuracy is 3.14627\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 10.72757\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 42\n",
      "The actual fold accuracy is 11.73663\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 8.53682\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 43\n",
      "The actual fold accuracy is 7.58030\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 6.39403\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 45\n",
      "The actual fold accuracy is 10.02976\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.2, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 8.00136\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 45\n",
      "The actual fold accuracy is 3.98588\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 33\n",
      "The actual fold accuracy is 3.22927\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 39\n",
      "The actual fold accuracy is 5.60694\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.2, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 4.27403\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 48\n",
      "The actual fold accuracy is 14.71805\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 3.59675\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 15.44758\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.2, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 11.25412\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 31\n",
      "The actual fold accuracy is 4.10611\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 38\n",
      "The actual fold accuracy is 5.79918\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 2.55213\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.3, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 4.15247\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 32\n",
      "The actual fold accuracy is 4.46017\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 28\n",
      "The actual fold accuracy is 3.64884\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 35\n",
      "The actual fold accuracy is 2.87451\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.3, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 3.66117\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 41\n",
      "The actual fold accuracy is 10.26411\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 38\n",
      "The actual fold accuracy is 8.41346\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 45\n",
      "The actual fold accuracy is 12.34329\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.3, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 10.34029\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 26\n",
      "The actual fold accuracy is 3.92807\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 25\n",
      "The actual fold accuracy is 3.47065\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 13\n",
      "The actual fold accuracy is 2.99989\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.5, [918, 250, 164, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 3.46620\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 28\n",
      "The actual fold accuracy is 1.93451\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 32\n",
      "The actual fold accuracy is 2.97505\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 19\n",
      "The actual fold accuracy is 1.66132\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.5, [918, 25, 16, 16, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 2.19029\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 24\n",
      "The actual fold accuracy is 3.55999\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 42\n",
      "The actual fold accuracy is 8.14994\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 37\n",
      "The actual fold accuracy is 8.84394\n",
      "\n",
      "Parameter Combination (<class 'torch.nn.modules.linear.Identity'>, 0.5, [918, 250, 164, 104]) with keys ['activation', 'dropout', 'layer']\n",
      " Accuracy: 6.85129\n",
      "\n",
      "Grid search took 13.4 minutes.\n",
      "{'activation': <class 'torch.nn.modules.linear.Identity'>, 'dropout': 0.2, 'layer': [918, 250, 164, 104]}\n"
     ]
    }
   ],
   "source": [
    "nnmodel.defaults[\"learning_rate\"] = best[\"learning_rate\"]\n",
    "nnmodel.defaults[\"batch_size\"] = best[\"batch_size\"]\n",
    "dict_param_2 = {\"activation\": [nn.ReLU, nn.Sigmoid, nn.Identity], \"dropout\": [0, 0.2, 0.3, 0.5], \"layer\": test_layer}\n",
    "best, acc = nnmodel.grid_search(dict_param_2, train_small_ds, test_small_ds, epochs=50, cv=True, k_folds=3)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77b0251a-8e28-4ccb-9ed8-17297eadb45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel.defaults[\"activation\"] = best[\"activation\"]\n",
    "nnmodel.defaults[\"dropout\"] = best[\"dropout\"]\n",
    "nnmodel.defaults[\"layer\"] = best[\"layer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "489cfbc6-1cf2-4300-aa9d-dd9fd6e397c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'batch_size': 320, 'layer': [918, 250, 164, 104], 'activation': <class 'torch.nn.modules.linear.Identity'>, 'dropout': 0.2}\n"
     ]
    }
   ],
   "source": [
    "print(nnmodel.defaults)\n",
    "grid_best = nnmodel.defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d2d5d-4aff-4f68-9c7e-882d7806ea30",
   "metadata": {},
   "source": [
    "Same resulting parameter as normal splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83101f7-9a94-46ff-ad73-8d8d08bf35cf",
   "metadata": {},
   "source": [
    "### Local Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a662b36f-4c74-4cac-914a-a92855627ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel = NNModel(layer, device, acc_func=acc_func, loss_func=nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e28e4e2f-7843-4d78-8f61-11ac5ee78e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 3.54474\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 3.93098\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 15\n",
      "The actual fold accuracy is 0.89006\n",
      "\n",
      "Step 0\n",
      "Best Params, Parameter Combination {'learning_rate': 0.05, 'batch_size': 320}\n",
      " Accuracy: 2.78859\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 2.24433\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 3.47200\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 13\n",
      "The actual fold accuracy is 1.40718\n",
      "\n",
      "Parameter Combination (0.04018, 257) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 2.37450\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 17\n",
      "The actual fold accuracy is 1.04107\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 18\n",
      "The actual fold accuracy is 2.59726\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 20\n",
      "The actual fold accuracy is 1.18010\n",
      "\n",
      "Parameter Combination (0.04018, 382) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 1.60614\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 4.54725\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 44\n",
      "The actual fold accuracy is 3.91180\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 9\n",
      "The actual fold accuracy is 0.95777\n",
      "\n",
      "Parameter Combination (0.06177, 257) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.13894\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 22\n",
      "The actual fold accuracy is 1.07279\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 11\n",
      "The actual fold accuracy is 1.01377\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.90090\n",
      "\n",
      "Parameter Combination (0.06177, 382) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 1.32915\n",
      "\n",
      "Grid search took 1.3 minutes.\n",
      "Step 1\n",
      "Best Params, Parameter Combination {'learning_rate': 0.06177, 'batch_size': 257}\n",
      " Accuracy: 3.13894\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 9.50850\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 42\n",
      "The actual fold accuracy is 5.43295\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 49\n",
      "The actual fold accuracy is 9.38759\n",
      "\n",
      "Parameter Combination (0.05197, 196) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 8.10968\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 2.79647\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 11\n",
      "The actual fold accuracy is 1.19980\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 5.22973\n",
      "\n",
      "Parameter Combination (0.05197, 287) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.07533\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 24\n",
      "The actual fold accuracy is 6.61810\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 28\n",
      "The actual fold accuracy is 7.64517\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 37\n",
      "The actual fold accuracy is 2.64550\n",
      "\n",
      "Parameter Combination (0.0798, 196) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 5.63626\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 46\n",
      "The actual fold accuracy is 2.31928\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 36\n",
      "The actual fold accuracy is 4.81921\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 31\n",
      "The actual fold accuracy is 2.07749\n",
      "\n",
      "Parameter Combination (0.0798, 287) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.07199\n",
      "\n",
      "Grid search took 1.9 minutes.\n",
      "Step 2\n",
      "Best Params, Parameter Combination {'learning_rate': 0.05197, 'batch_size': 196}\n",
      " Accuracy: 8.10968\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 8\n",
      "The actual fold accuracy is 0.77277\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 7\n",
      "The actual fold accuracy is 0.99126\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 16\n",
      "The actual fold accuracy is 3.67705\n",
      "\n",
      "Parameter Combination (0.04269, 145) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 1.81369\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 36\n",
      "The actual fold accuracy is 1.76706\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 16\n",
      "The actual fold accuracy is 1.15966\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 14\n",
      "The actual fold accuracy is 0.82972\n",
      "\n",
      "Parameter Combination (0.04269, 236) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 1.25214\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 33\n",
      "The actual fold accuracy is 3.72427\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 42\n",
      "The actual fold accuracy is 10.06382\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 28\n",
      "The actual fold accuracy is 3.39825\n",
      "\n",
      "Parameter Combination (0.06527, 145) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 5.72878\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 17\n",
      "The actual fold accuracy is 1.02213\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 19\n",
      "The actual fold accuracy is 0.40365\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 11\n",
      "The actual fold accuracy is 0.76359\n",
      "\n",
      "Parameter Combination (0.06527, 236) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 0.72979\n",
      "\n",
      "Grid search took 1.2 minutes.\n",
      "Step 3\n",
      "Best Params, Parameter Combination {'learning_rate': 0.05197, 'batch_size': 196}\n",
      " Accuracy: 8.10968\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 11\n",
      "The actual fold accuracy is 0.72033\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 14\n",
      "The actual fold accuracy is 2.60988\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 9\n",
      "The actual fold accuracy is 1.59141\n",
      "\n",
      "Parameter Combination (0.04161, 164) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 1.64054\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.74413\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 3.68026\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 3.36307\n",
      "\n",
      "Parameter Combination (0.04161, 249) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 2.92915\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 21\n",
      "The actual fold accuracy is 4.71266\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 35\n",
      "The actual fold accuracy is 7.21251\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 33\n",
      "The actual fold accuracy is 4.01945\n",
      "\n",
      "Parameter Combination (0.06351, 164) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 5.31487\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 48\n",
      "The actual fold accuracy is 2.37183\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 40\n",
      "The actual fold accuracy is 4.92772\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 4.89239\n",
      "\n",
      "Parameter Combination (0.06351, 249) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 4.06398\n",
      "\n",
      "Grid search took 1.7 minutes.\n",
      "Step 4\n",
      "Best Params, Parameter Combination {'learning_rate': 0.05197, 'batch_size': 196}\n",
      " Accuracy: 8.10968\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 5.03926\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 4.25730\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 3.04477\n",
      "\n",
      "Parameter Combination (0.03688, 158) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 4.11378\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 2.56562\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 4.01255\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 4.47514\n",
      "\n",
      "Parameter Combination (0.03688, 252) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 3.68444\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 30\n",
      "The actual fold accuracy is 4.57463\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 6.70293\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 45\n",
      "The actual fold accuracy is 4.70590\n",
      "\n",
      "Parameter Combination (0.05991, 158) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 5.32782\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 1.82019\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 47\n",
      "The actual fold accuracy is 6.76906\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 49\n",
      "The actual fold accuracy is 7.93881\n",
      "\n",
      "Parameter Combination (0.05991, 252) with keys ['learning_rate', 'batch_size']\n",
      " Accuracy: 5.50935\n",
      "\n",
      "Grid search took 2.1 minutes.\n",
      "Local search took 8.5 minutes.\n",
      "{'learning_rate': 0.05197, 'batch_size': 196}\n"
     ]
    }
   ],
   "source": [
    "init_param = {\"learning_rate\": grid_best[\"learning_rate\"], \"batch_size\": grid_best[\"batch_size\"]}\n",
    "best, acc = nnmodel.local_search(init_param, train_small_ds, test_small_ds, steps=5, epochs=50, cv=True, k_folds=3)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f126ee23-ada4-40e8-946a-252caa2d2c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 41\n",
      "The actual fold accuracy is 10.48273\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 34\n",
      "The actual fold accuracy is 10.91913\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 41\n",
      "The actual fold accuracy is 9.73639\n",
      "\n",
      "Step 0\n",
      "Best Params, Parameter Combination {'layer': [918, 250, 164, 104], 'dropout': 0.2}\n",
      " Accuracy: 10.37942\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 37\n",
      "The actual fold accuracy is 8.61152\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 32\n",
      "The actual fold accuracy is 6.60532\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 34\n",
      "The actual fold accuracy is 9.86726\n",
      "\n",
      "Parameter Combination ([918, 197, 115, 104], 0.1602) with keys ['layer', 'dropout']\n",
      " Accuracy: 8.36136\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 6.28590\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 28\n",
      "The actual fold accuracy is 4.33675\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 37\n",
      "The actual fold accuracy is 5.88799\n",
      "\n",
      "Parameter Combination ([918, 197, 115, 104], 0.25436) with keys ['layer', 'dropout']\n",
      " Accuracy: 5.50355\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 39\n",
      "The actual fold accuracy is 8.82954\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 42\n",
      "The actual fold accuracy is 11.33208\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 43\n",
      "The actual fold accuracy is 10.76366\n",
      "\n",
      "Parameter Combination ([918, 279, 185, 104], 0.1602) with keys ['layer', 'dropout']\n",
      " Accuracy: 10.30843\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 35\n",
      "The actual fold accuracy is 8.07922\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 29\n",
      "The actual fold accuracy is 6.20109\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 22\n",
      "The actual fold accuracy is 3.18423\n",
      "\n",
      "Parameter Combination ([918, 279, 185, 104], 0.25436) with keys ['layer', 'dropout']\n",
      " Accuracy: 5.82151\n",
      "\n",
      "Grid search took 1.5 minutes.\n",
      "Step 1\n",
      "Best Params, Parameter Combination {'layer': [918, 250, 164, 104], 'dropout': 0.2}\n",
      " Accuracy: 10.37942\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 33\n",
      "The actual fold accuracy is 7.67957\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 9.47293\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 39\n",
      "The actual fold accuracy is 10.50065\n",
      "\n",
      "Parameter Combination ([918, 224, 120, 104], 0.14495) with keys ['layer', 'dropout']\n",
      " Accuracy: 9.21772\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 22\n",
      "The actual fold accuracy is 3.06739\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 30\n",
      "The actual fold accuracy is 4.68217\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 24\n",
      "The actual fold accuracy is 5.04837\n",
      "\n",
      "Parameter Combination ([918, 224, 120, 104], 0.24958) with keys ['layer', 'dropout']\n",
      " Accuracy: 4.26598\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 40\n",
      "The actual fold accuracy is 11.75689\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 8.98671\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 49\n",
      "The actual fold accuracy is 11.26834\n",
      "\n",
      "Parameter Combination ([918, 321, 209, 104], 0.14495) with keys ['layer', 'dropout']\n",
      " Accuracy: 10.67065\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 20\n",
      "The actual fold accuracy is 3.30231\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 25\n",
      "The actual fold accuracy is 4.38828\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 41\n",
      "The actual fold accuracy is 9.61471\n",
      "\n",
      "Parameter Combination ([918, 321, 209, 104], 0.24958) with keys ['layer', 'dropout']\n",
      " Accuracy: 5.76843\n",
      "\n",
      "Grid search took 1.6 minutes.\n",
      "Step 2\n",
      "Best Params, Parameter Combination {'layer': [918, 321, 209, 104], 'dropout': 0.14495}\n",
      " Accuracy: 10.67065\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 7.02723\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 43\n",
      "The actual fold accuracy is 3.55510\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 48\n",
      "The actual fold accuracy is 14.47747\n",
      "\n",
      "Parameter Combination ([918, 283, 159, 104], 0.10853) with keys ['layer', 'dropout']\n",
      " Accuracy: 8.35327\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 34\n",
      "The actual fold accuracy is 4.40344\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 45\n",
      "The actual fold accuracy is 3.23083\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 29\n",
      "The actual fold accuracy is 7.44120\n",
      "\n",
      "Parameter Combination ([918, 283, 159, 104], 0.17275) with keys ['layer', 'dropout']\n",
      " Accuracy: 5.02516\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 34\n",
      "The actual fold accuracy is 5.73077\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 42\n",
      "The actual fold accuracy is 12.36683\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 48\n",
      "The actual fold accuracy is 13.71781\n",
      "\n",
      "Parameter Combination ([918, 405, 253, 104], 0.10853) with keys ['layer', 'dropout']\n",
      " Accuracy: 10.60514\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 9.01761\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 24\n",
      "The actual fold accuracy is 5.69933\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 38\n",
      "The actual fold accuracy is 10.28541\n",
      "\n",
      "Parameter Combination ([918, 405, 253, 104], 0.17275) with keys ['layer', 'dropout']\n",
      " Accuracy: 8.33412\n",
      "\n",
      "Grid search took 2.2 minutes.\n",
      "Step 3\n",
      "Best Params, Parameter Combination {'layer': [918, 321, 209, 104], 'dropout': 0.14495}\n",
      " Accuracy: 10.67065\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 39\n",
      "The actual fold accuracy is 10.56153\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 40\n",
      "The actual fold accuracy is 12.85342\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 3.46639\n",
      "\n",
      "Parameter Combination ([918, 271, 178, 104], 0.11181) with keys ['layer', 'dropout']\n",
      " Accuracy: 8.96045\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 38\n",
      "The actual fold accuracy is 9.15235\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 41\n",
      "The actual fold accuracy is 10.40830\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 42\n",
      "The actual fold accuracy is 9.57518\n",
      "\n",
      "Parameter Combination ([918, 271, 178, 104], 0.16093) with keys ['layer', 'dropout']\n",
      " Accuracy: 9.71194\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 14.06006\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 38\n",
      "The actual fold accuracy is 10.44879\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 12.20714\n",
      "\n",
      "Parameter Combination ([918, 406, 238, 104], 0.11181) with keys ['layer', 'dropout']\n",
      " Accuracy: 12.23866\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "The actual fold accuracy is 10.97017\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 40\n",
      "The actual fold accuracy is 11.62846\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 44\n",
      "The actual fold accuracy is 11.60428\n",
      "\n",
      "Parameter Combination ([918, 406, 238, 104], 0.16093) with keys ['layer', 'dropout']\n",
      " Accuracy: 11.40097\n",
      "\n",
      "Grid search took 2.4 minutes.\n",
      "Step 4\n",
      "Best Params, Parameter Combination {'layer': [918, 406, 238, 104], 'dropout': 0.11181}\n",
      " Accuracy: 12.23866\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 32\n",
      "The actual fold accuracy is 2.66491\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 45\n",
      "The actual fold accuracy is 4.31720\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 41\n",
      "The actual fold accuracy is 13.37710\n",
      "\n",
      "Parameter Combination ([918, 309, 175, 104], 0.07934) with keys ['layer', 'dropout']\n",
      " Accuracy: 6.78640\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 39\n",
      "The actual fold accuracy is 11.36590\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 13.69450\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 45\n",
      "The actual fold accuracy is 3.51914\n",
      "\n",
      "Parameter Combination ([918, 309, 175, 104], 0.14441) with keys ['layer', 'dropout']\n",
      " Accuracy: 9.52652\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 35\n",
      "The actual fold accuracy is 11.00915\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Early stopping at epoch: 39\n",
      "The actual fold accuracy is 10.94665\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Early stopping at epoch: 35\n",
      "The actual fold accuracy is 3.38125\n",
      "\n",
      "Parameter Combination ([918, 492, 302, 104], 0.07934) with keys ['layer', 'dropout']\n",
      " Accuracy: 8.44568\n",
      "\n",
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Early stopping at epoch: 43\n",
      "The actual fold accuracy is 14.14489\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "The actual fold accuracy is 11.33074\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "The actual fold accuracy is 5.52022\n",
      "\n",
      "Parameter Combination ([918, 492, 302, 104], 0.14441) with keys ['layer', 'dropout']\n",
      " Accuracy: 10.33195\n",
      "\n",
      "Grid search took 2.1 minutes.\n",
      "Local search took 10.1 minutes.\n"
     ]
    }
   ],
   "source": [
    "nnmodel.defaults[\"learning_rate\"] = best[\"learning_rate\"]\n",
    "nnmodel.defaults[\"batch_size\"] = best[\"batch_size\"]\n",
    "init_param = {\"layer\": grid_best[\"layer\"], \"dropout\": grid_best[\"dropout\"]}\n",
    "best, acc = nnmodel.local_search(init_par6am, train_small_ds, test_small_ds, steps=5, epochs=50, cv=True, k_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71abf263-4556-4c55-b5fe-89803f7a4604",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel.defaults[\"dropout\"] = best[\"dropout\"]\n",
    "nnmodel.defaults[\"layer\"] = best[\"layer\"]\n",
    "nnmodel.defaults[\"activation\"] = grid_best[\"activation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "534e85a2-65ba-4e34-bdc9-690f36a4f841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05197, 'batch_size': 196, 'layer': [918, 406, 238, 104], 'activation': <class 'torch.nn.modules.linear.Identity'>, 'dropout': 0.11181}\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4.655267  [  196/744121]\n",
      "loss: 428.313416  [19796/744121]\n",
      "loss: 838.697327  [39396/744121]\n",
      "loss: 1236.876504  [58996/744121]\n",
      "loss: 1626.860841  [78596/744121]\n",
      "loss: 2010.543652  [98196/744121]\n",
      "loss: 2388.569301  [117796/744121]\n",
      "loss: 2760.119589  [137396/744121]\n",
      "loss: 3122.979672  [156996/744121]\n",
      "loss: 3480.046861  [176596/744121]\n",
      "loss: 3830.370941  [196196/744121]\n",
      "loss: 4170.875550  [215796/744121]\n",
      "loss: 4505.335046  [235396/744121]\n",
      "loss: 4830.843104  [254996/744121]\n",
      "loss: 5154.126434  [274596/744121]\n",
      "loss: 5473.111553  [294196/744121]\n",
      "loss: 5783.617405  [313796/744121]\n",
      "loss: 6091.482241  [333396/744121]\n",
      "loss: 6398.070493  [352996/744121]\n",
      "loss: 6698.490442  [372596/744121]\n",
      "loss: 6992.364316  [392196/744121]\n",
      "loss: 7284.916224  [411796/744121]\n",
      "loss: 7574.535567  [431396/744121]\n",
      "loss: 7860.999190  [450996/744121]\n",
      "loss: 8143.695333  [470596/744121]\n",
      "loss: 8426.424901  [490196/744121]\n",
      "loss: 8699.223158  [509796/744121]\n",
      "loss: 8969.637413  [529396/744121]\n",
      "loss: 9238.865981  [548996/744121]\n",
      "loss: 9506.421321  [568596/744121]\n",
      "loss: 9769.487195  [588196/744121]\n",
      "loss: 10028.526718  [607796/744121]\n",
      "loss: 10286.813082  [627396/744121]\n",
      "loss: 10540.482412  [646996/744121]\n",
      "loss: 10791.604367  [666596/744121]\n",
      "loss: 11042.219930  [686196/744121]\n",
      "loss: 11292.143542  [705796/744121]\n",
      "loss: 11540.736020  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 48.40176%, Avg loss: 2.085503 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.419551  [  196/744121]\n",
      "loss: 240.418592  [19796/744121]\n",
      "loss: 479.223320  [39396/744121]\n",
      "loss: 716.676622  [58996/744121]\n",
      "loss: 952.642597  [78596/744121]\n",
      "loss: 1184.755597  [98196/744121]\n",
      "loss: 1418.602020  [117796/744121]\n",
      "loss: 1647.136367  [137396/744121]\n",
      "loss: 1875.390250  [156996/744121]\n",
      "loss: 2103.446646  [176596/744121]\n",
      "loss: 2326.572427  [196196/744121]\n",
      "loss: 2550.140768  [215796/744121]\n",
      "loss: 2769.614595  [235396/744121]\n",
      "loss: 2989.653581  [254996/744121]\n",
      "loss: 3209.334142  [274596/744121]\n",
      "loss: 3422.060091  [294196/744121]\n",
      "loss: 3633.392698  [313796/744121]\n",
      "loss: 3844.805190  [333396/744121]\n",
      "loss: 4056.535840  [352996/744121]\n",
      "loss: 4262.728597  [372596/744121]\n",
      "loss: 4468.473273  [392196/744121]\n",
      "loss: 4675.966491  [411796/744121]\n",
      "loss: 4882.625051  [431396/744121]\n",
      "loss: 5087.646413  [450996/744121]\n",
      "loss: 5288.233154  [470596/744121]\n",
      "loss: 5486.383117  [490196/744121]\n",
      "loss: 5685.251088  [509796/744121]\n",
      "loss: 5884.563838  [529396/744121]\n",
      "loss: 6081.927821  [548996/744121]\n",
      "loss: 6278.846589  [568596/744121]\n",
      "loss: 6471.431699  [588196/744121]\n",
      "loss: 6662.828407  [607796/744121]\n",
      "loss: 6857.323234  [627396/744121]\n",
      "loss: 7047.810848  [646996/744121]\n",
      "loss: 7240.776074  [666596/744121]\n",
      "loss: 7429.167835  [686196/744121]\n",
      "loss: 7617.321993  [705796/744121]\n",
      "loss: 7807.554859  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 62.73613%, Avg loss: 1.501813 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.804171  [  196/744121]\n",
      "loss: 186.801409  [19796/744121]\n",
      "loss: 372.324923  [39396/744121]\n",
      "loss: 558.656698  [58996/744121]\n",
      "loss: 744.137120  [78596/744121]\n",
      "loss: 925.329656  [98196/744121]\n",
      "loss: 1108.178595  [117796/744121]\n",
      "loss: 1290.807657  [137396/744121]\n",
      "loss: 1471.672337  [156996/744121]\n",
      "loss: 1656.421878  [176596/744121]\n",
      "loss: 1834.948951  [196196/744121]\n",
      "loss: 2013.833391  [215796/744121]\n",
      "loss: 2189.508666  [235396/744121]\n",
      "loss: 2363.555646  [254996/744121]\n",
      "loss: 2541.982350  [274596/744121]\n",
      "loss: 2720.839975  [294196/744121]\n",
      "loss: 2896.841484  [313796/744121]\n",
      "loss: 3068.632897  [333396/744121]\n",
      "loss: 3242.629226  [352996/744121]\n",
      "loss: 3415.140493  [372596/744121]\n",
      "loss: 3589.036081  [392196/744121]\n",
      "loss: 3762.923638  [411796/744121]\n",
      "loss: 3931.867649  [431396/744121]\n",
      "loss: 4105.237247  [450996/744121]\n",
      "loss: 4277.925448  [470596/744121]\n",
      "loss: 4446.913648  [490196/744121]\n",
      "loss: 4615.619634  [509796/744121]\n",
      "loss: 4786.503071  [529396/744121]\n",
      "loss: 4954.254699  [548996/744121]\n",
      "loss: 5122.708822  [568596/744121]\n",
      "loss: 5287.750534  [588196/744121]\n",
      "loss: 5451.678217  [607796/744121]\n",
      "loss: 5619.037464  [627396/744121]\n",
      "loss: 5782.554253  [646996/744121]\n",
      "loss: 5947.947216  [666596/744121]\n",
      "loss: 6111.912740  [686196/744121]\n",
      "loss: 6273.622970  [705796/744121]\n",
      "loss: 6435.223751  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 68.81689%, Avg loss: 1.287572 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.705314  [  196/744121]\n",
      "loss: 164.738637  [19796/744121]\n",
      "loss: 325.705989  [39396/744121]\n",
      "loss: 487.588331  [58996/744121]\n",
      "loss: 649.530432  [78596/744121]\n",
      "loss: 808.822583  [98196/744121]\n",
      "loss: 968.318951  [117796/744121]\n",
      "loss: 1131.330117  [137396/744121]\n",
      "loss: 1292.812340  [156996/744121]\n",
      "loss: 1452.469711  [176596/744121]\n",
      "loss: 1610.559446  [196196/744121]\n",
      "loss: 1770.040035  [215796/744121]\n",
      "loss: 1928.448213  [235396/744121]\n",
      "loss: 2084.033864  [254996/744121]\n",
      "loss: 2242.165477  [274596/744121]\n",
      "loss: 2398.562626  [294196/744121]\n",
      "loss: 2554.048741  [313796/744121]\n",
      "loss: 2708.750664  [333396/744121]\n",
      "loss: 2864.827684  [352996/744121]\n",
      "loss: 3020.674525  [372596/744121]\n",
      "loss: 3176.960286  [392196/744121]\n",
      "loss: 3331.579622  [411796/744121]\n",
      "loss: 3486.558439  [431396/744121]\n",
      "loss: 3640.513327  [450996/744121]\n",
      "loss: 3796.352131  [470596/744121]\n",
      "loss: 3947.217698  [490196/744121]\n",
      "loss: 4096.481363  [509796/744121]\n",
      "loss: 4250.502462  [529396/744121]\n",
      "loss: 4401.181461  [548996/744121]\n",
      "loss: 4552.397564  [568596/744121]\n",
      "loss: 4700.143998  [588196/744121]\n",
      "loss: 4849.338858  [607796/744121]\n",
      "loss: 5000.187544  [627396/744121]\n",
      "loss: 5147.565130  [646996/744121]\n",
      "loss: 5297.145159  [666596/744121]\n",
      "loss: 5446.852488  [686196/744121]\n",
      "loss: 5595.498545  [705796/744121]\n",
      "loss: 5744.610595  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 74.45911%, Avg loss: 1.040094 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.381328  [  196/744121]\n",
      "loss: 148.088222  [19796/744121]\n",
      "loss: 296.193826  [39396/744121]\n",
      "loss: 445.647710  [58996/744121]\n",
      "loss: 592.618813  [78596/744121]\n",
      "loss: 739.999306  [98196/744121]\n",
      "loss: 886.260321  [117796/744121]\n",
      "loss: 1036.009742  [137396/744121]\n",
      "loss: 1185.217551  [156996/744121]\n",
      "loss: 1333.398328  [176596/744121]\n",
      "loss: 1478.813237  [196196/744121]\n",
      "loss: 1626.039082  [215796/744121]\n",
      "loss: 1770.630980  [235396/744121]\n",
      "loss: 1912.722264  [254996/744121]\n",
      "loss: 2059.867764  [274596/744121]\n",
      "loss: 2205.160660  [294196/744121]\n",
      "loss: 2346.946615  [313796/744121]\n",
      "loss: 2488.362273  [333396/744121]\n",
      "loss: 2630.941319  [352996/744121]\n",
      "loss: 2776.353583  [372596/744121]\n",
      "loss: 2919.986100  [392196/744121]\n",
      "loss: 3062.889832  [411796/744121]\n",
      "loss: 3207.221388  [431396/744121]\n",
      "loss: 3350.552907  [450996/744121]\n",
      "loss: 3496.972901  [470596/744121]\n",
      "loss: 3640.361951  [490196/744121]\n",
      "loss: 3780.982929  [509796/744121]\n",
      "loss: 3924.740054  [529396/744121]\n",
      "loss: 4066.251734  [548996/744121]\n",
      "loss: 4206.328248  [568596/744121]\n",
      "loss: 4346.968283  [588196/744121]\n",
      "loss: 4486.898445  [607796/744121]\n",
      "loss: 4628.463652  [627396/744121]\n",
      "loss: 4766.134648  [646996/744121]\n",
      "loss: 4904.090094  [666596/744121]\n",
      "loss: 5044.824722  [686196/744121]\n",
      "loss: 5185.582953  [705796/744121]\n",
      "loss: 5327.605215  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 76.07285%, Avg loss: 0.946398 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.408390  [  196/744121]\n",
      "loss: 142.701241  [19796/744121]\n",
      "loss: 282.519776  [39396/744121]\n",
      "loss: 420.205643  [58996/744121]\n",
      "loss: 558.781343  [78596/744121]\n",
      "loss: 696.496604  [98196/744121]\n",
      "loss: 834.341718  [117796/744121]\n",
      "loss: 975.071324  [137396/744121]\n",
      "loss: 1112.506765  [156996/744121]\n",
      "loss: 1253.174518  [176596/744121]\n",
      "loss: 1387.613133  [196196/744121]\n",
      "loss: 1525.433367  [215796/744121]\n",
      "loss: 1663.558343  [235396/744121]\n",
      "loss: 1800.114531  [254996/744121]\n",
      "loss: 1940.355315  [274596/744121]\n",
      "loss: 2074.949280  [294196/744121]\n",
      "loss: 2211.978663  [313796/744121]\n",
      "loss: 2346.502060  [333396/744121]\n",
      "loss: 2481.869230  [352996/744121]\n",
      "loss: 2617.240742  [372596/744121]\n",
      "loss: 2756.337873  [392196/744121]\n",
      "loss: 2891.815848  [411796/744121]\n",
      "loss: 3027.440528  [431396/744121]\n",
      "loss: 3163.891183  [450996/744121]\n",
      "loss: 3301.167849  [470596/744121]\n",
      "loss: 3435.014397  [490196/744121]\n",
      "loss: 3568.603384  [509796/744121]\n",
      "loss: 3703.400071  [529396/744121]\n",
      "loss: 3839.100824  [548996/744121]\n",
      "loss: 3974.726609  [568596/744121]\n",
      "loss: 4107.257725  [588196/744121]\n",
      "loss: 4240.110396  [607796/744121]\n",
      "loss: 4376.426674  [627396/744121]\n",
      "loss: 4508.251272  [646996/744121]\n",
      "loss: 4642.347640  [666596/744121]\n",
      "loss: 4774.741902  [686196/744121]\n",
      "loss: 4907.891686  [705796/744121]\n",
      "loss: 5040.787047  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 77.27097%, Avg loss: 0.906100 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.354001  [  196/744121]\n",
      "loss: 135.235420  [19796/744121]\n",
      "loss: 268.880092  [39396/744121]\n",
      "loss: 401.830701  [58996/744121]\n",
      "loss: 534.629229  [78596/744121]\n",
      "loss: 666.491366  [98196/744121]\n",
      "loss: 799.688235  [117796/744121]\n",
      "loss: 933.306571  [137396/744121]\n",
      "loss: 1064.797302  [156996/744121]\n",
      "loss: 1197.818240  [176596/744121]\n",
      "loss: 1331.038619  [196196/744121]\n",
      "loss: 1461.277075  [215796/744121]\n",
      "loss: 1592.106636  [235396/744121]\n",
      "loss: 1722.943751  [254996/744121]\n",
      "loss: 1855.228305  [274596/744121]\n",
      "loss: 1984.502246  [294196/744121]\n",
      "loss: 2115.874317  [313796/744121]\n",
      "loss: 2245.572779  [333396/744121]\n",
      "loss: 2376.482882  [352996/744121]\n",
      "loss: 2507.247979  [372596/744121]\n",
      "loss: 2640.806522  [392196/744121]\n",
      "loss: 2771.868219  [411796/744121]\n",
      "loss: 2901.104469  [431396/744121]\n",
      "loss: 3031.859127  [450996/744121]\n",
      "loss: 3162.802093  [470596/744121]\n",
      "loss: 3291.333119  [490196/744121]\n",
      "loss: 3419.005376  [509796/744121]\n",
      "loss: 3550.711518  [529396/744121]\n",
      "loss: 3678.790253  [548996/744121]\n",
      "loss: 3809.544964  [568596/744121]\n",
      "loss: 3937.378522  [588196/744121]\n",
      "loss: 4067.684983  [607796/744121]\n",
      "loss: 4197.934425  [627396/744121]\n",
      "loss: 4324.984698  [646996/744121]\n",
      "loss: 4451.197379  [666596/744121]\n",
      "loss: 4578.584388  [686196/744121]\n",
      "loss: 4706.597055  [705796/744121]\n",
      "loss: 4836.747648  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 78.67538%, Avg loss: 0.861354 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.386007  [  196/744121]\n",
      "loss: 128.634367  [19796/744121]\n",
      "loss: 253.622245  [39396/744121]\n",
      "loss: 380.837094  [58996/744121]\n",
      "loss: 510.639177  [78596/744121]\n",
      "loss: 638.745100  [98196/744121]\n",
      "loss: 767.137516  [117796/744121]\n",
      "loss: 897.159564  [137396/744121]\n",
      "loss: 1024.198997  [156996/744121]\n",
      "loss: 1154.625980  [176596/744121]\n",
      "loss: 1279.797329  [196196/744121]\n",
      "loss: 1407.332885  [215796/744121]\n",
      "loss: 1536.929498  [235396/744121]\n",
      "loss: 1660.472497  [254996/744121]\n",
      "loss: 1789.618384  [274596/744121]\n",
      "loss: 1915.984634  [294196/744121]\n",
      "loss: 2043.144101  [313796/744121]\n",
      "loss: 2168.244830  [333396/744121]\n",
      "loss: 2293.445520  [352996/744121]\n",
      "loss: 2420.299360  [372596/744121]\n",
      "loss: 2548.598423  [392196/744121]\n",
      "loss: 2674.883819  [411796/744121]\n",
      "loss: 2801.960139  [431396/744121]\n",
      "loss: 2929.149380  [450996/744121]\n",
      "loss: 3055.945232  [470596/744121]\n",
      "loss: 3181.528348  [490196/744121]\n",
      "loss: 3305.420290  [509796/744121]\n",
      "loss: 3432.697237  [529396/744121]\n",
      "loss: 3557.340934  [548996/744121]\n",
      "loss: 3683.333922  [568596/744121]\n",
      "loss: 3807.669752  [588196/744121]\n",
      "loss: 3933.649423  [607796/744121]\n",
      "loss: 4060.832462  [627396/744121]\n",
      "loss: 4186.605671  [646996/744121]\n",
      "loss: 4312.066403  [666596/744121]\n",
      "loss: 4436.605511  [686196/744121]\n",
      "loss: 4560.073084  [705796/744121]\n",
      "loss: 4684.867331  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 78.52379%, Avg loss: 0.846018 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.270034  [  196/744121]\n",
      "loss: 125.925979  [19796/744121]\n",
      "loss: 251.087428  [39396/744121]\n",
      "loss: 373.533553  [58996/744121]\n",
      "loss: 498.834007  [78596/744121]\n",
      "loss: 624.258565  [98196/744121]\n",
      "loss: 749.019162  [117796/744121]\n",
      "loss: 873.175729  [137396/744121]\n",
      "loss: 998.586809  [156996/744121]\n",
      "loss: 1124.290755  [176596/744121]\n",
      "loss: 1249.336929  [196196/744121]\n",
      "loss: 1372.578863  [215796/744121]\n",
      "loss: 1496.362249  [235396/744121]\n",
      "loss: 1619.123822  [254996/744121]\n",
      "loss: 1742.747881  [274596/744121]\n",
      "loss: 1867.037315  [294196/744121]\n",
      "loss: 1992.213807  [313796/744121]\n",
      "loss: 2114.118855  [333396/744121]\n",
      "loss: 2237.169953  [352996/744121]\n",
      "loss: 2360.094971  [372596/744121]\n",
      "loss: 2483.565876  [392196/744121]\n",
      "loss: 2605.910726  [411796/744121]\n",
      "loss: 2728.698645  [431396/744121]\n",
      "loss: 2851.738449  [450996/744121]\n",
      "loss: 2978.262984  [470596/744121]\n",
      "loss: 3101.608591  [490196/744121]\n",
      "loss: 3221.448089  [509796/744121]\n",
      "loss: 3344.873940  [529396/744121]\n",
      "loss: 3466.213023  [548996/744121]\n",
      "loss: 3587.200834  [568596/744121]\n",
      "loss: 3707.600249  [588196/744121]\n",
      "loss: 3829.398568  [607796/744121]\n",
      "loss: 3952.330648  [627396/744121]\n",
      "loss: 4073.496336  [646996/744121]\n",
      "loss: 4193.922673  [666596/744121]\n",
      "loss: 4316.298333  [686196/744121]\n",
      "loss: 4437.845589  [705796/744121]\n",
      "loss: 4561.349879  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.30953%, Avg loss: 0.797088 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.316966  [  196/744121]\n",
      "loss: 122.567221  [19796/744121]\n",
      "loss: 243.598015  [39396/744121]\n",
      "loss: 361.469448  [58996/744121]\n",
      "loss: 482.939754  [78596/744121]\n",
      "loss: 604.857773  [98196/744121]\n",
      "loss: 725.068579  [117796/744121]\n",
      "loss: 847.495736  [137396/744121]\n",
      "loss: 970.175698  [156996/744121]\n",
      "loss: 1094.070073  [176596/744121]\n",
      "loss: 1214.458500  [196196/744121]\n",
      "loss: 1335.186401  [215796/744121]\n",
      "loss: 1457.747439  [235396/744121]\n",
      "loss: 1578.582055  [254996/744121]\n",
      "loss: 1697.556973  [274596/744121]\n",
      "loss: 1816.449095  [294196/744121]\n",
      "loss: 1937.468446  [313796/744121]\n",
      "loss: 2055.714307  [333396/744121]\n",
      "loss: 2174.305627  [352996/744121]\n",
      "loss: 2296.322527  [372596/744121]\n",
      "loss: 2417.325147  [392196/744121]\n",
      "loss: 2536.633228  [411796/744121]\n",
      "loss: 2658.318705  [431396/744121]\n",
      "loss: 2777.785536  [450996/744121]\n",
      "loss: 2898.774010  [470596/744121]\n",
      "loss: 3019.228389  [490196/744121]\n",
      "loss: 3136.933108  [509796/744121]\n",
      "loss: 3257.750123  [529396/744121]\n",
      "loss: 3378.320425  [548996/744121]\n",
      "loss: 3498.449831  [568596/744121]\n",
      "loss: 3616.203135  [588196/744121]\n",
      "loss: 3738.424192  [607796/744121]\n",
      "loss: 3859.700836  [627396/744121]\n",
      "loss: 3979.129793  [646996/744121]\n",
      "loss: 4096.566064  [666596/744121]\n",
      "loss: 4216.366232  [686196/744121]\n",
      "loss: 4334.648182  [705796/744121]\n",
      "loss: 4454.358173  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 80.56907%, Avg loss: 0.769006 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.236971  [  196/744121]\n",
      "loss: 119.119981  [19796/744121]\n",
      "loss: 238.678633  [39396/744121]\n",
      "loss: 355.351153  [58996/744121]\n",
      "loss: 475.129942  [78596/744121]\n",
      "loss: 592.056544  [98196/744121]\n",
      "loss: 708.590628  [117796/744121]\n",
      "loss: 828.349745  [137396/744121]\n",
      "loss: 947.929758  [156996/744121]\n",
      "loss: 1066.756114  [176596/744121]\n",
      "loss: 1185.558701  [196196/744121]\n",
      "loss: 1304.465313  [215796/744121]\n",
      "loss: 1424.956736  [235396/744121]\n",
      "loss: 1540.762034  [254996/744121]\n",
      "loss: 1659.404460  [274596/744121]\n",
      "loss: 1774.848195  [294196/744121]\n",
      "loss: 1893.039927  [313796/744121]\n",
      "loss: 2010.535207  [333396/744121]\n",
      "loss: 2127.208935  [352996/744121]\n",
      "loss: 2245.727180  [372596/744121]\n",
      "loss: 2364.579147  [392196/744121]\n",
      "loss: 2481.267748  [411796/744121]\n",
      "loss: 2599.895023  [431396/744121]\n",
      "loss: 2716.637080  [450996/744121]\n",
      "loss: 2837.994359  [470596/744121]\n",
      "loss: 2956.376081  [490196/744121]\n",
      "loss: 3072.316127  [509796/744121]\n",
      "loss: 3192.103464  [529396/744121]\n",
      "loss: 3308.503856  [548996/744121]\n",
      "loss: 3427.696982  [568596/744121]\n",
      "loss: 3542.968606  [588196/744121]\n",
      "loss: 3659.963766  [607796/744121]\n",
      "loss: 3776.669890  [627396/744121]\n",
      "loss: 3893.025151  [646996/744121]\n",
      "loss: 4010.868406  [666596/744121]\n",
      "loss: 4128.581122  [686196/744121]\n",
      "loss: 4244.099222  [705796/744121]\n",
      "loss: 4361.552056  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 81.22610%, Avg loss: 0.762665 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.362634  [  196/744121]\n",
      "loss: 117.768191  [19796/744121]\n",
      "loss: 233.558276  [39396/744121]\n",
      "loss: 351.159497  [58996/744121]\n",
      "loss: 467.891505  [78596/744121]\n",
      "loss: 585.306278  [98196/744121]\n",
      "loss: 701.068123  [117796/744121]\n",
      "loss: 819.120134  [137396/744121]\n",
      "loss: 939.226499  [156996/744121]\n",
      "loss: 1056.737024  [176596/744121]\n",
      "loss: 1173.817238  [196196/744121]\n",
      "loss: 1291.627108  [215796/744121]\n",
      "loss: 1411.257502  [235396/744121]\n",
      "loss: 1527.346961  [254996/744121]\n",
      "loss: 1643.558599  [274596/744121]\n",
      "loss: 1758.491687  [294196/744121]\n",
      "loss: 1875.112144  [313796/744121]\n",
      "loss: 1990.719442  [333396/744121]\n",
      "loss: 2106.392555  [352996/744121]\n",
      "loss: 2222.748518  [372596/744121]\n",
      "loss: 2338.378061  [392196/744121]\n",
      "loss: 2454.327039  [411796/744121]\n",
      "loss: 2570.818382  [431396/744121]\n",
      "loss: 2686.324795  [450996/744121]\n",
      "loss: 2803.864593  [470596/744121]\n",
      "loss: 2921.989369  [490196/744121]\n",
      "loss: 3035.596989  [509796/744121]\n",
      "loss: 3153.865408  [529396/744121]\n",
      "loss: 3270.067115  [548996/744121]\n",
      "loss: 3386.360705  [568596/744121]\n",
      "loss: 3499.560564  [588196/744121]\n",
      "loss: 3615.941733  [607796/744121]\n",
      "loss: 3732.771148  [627396/744121]\n",
      "loss: 3847.301382  [646996/744121]\n",
      "loss: 3964.148706  [666596/744121]\n",
      "loss: 4078.498856  [686196/744121]\n",
      "loss: 4192.295486  [705796/744121]\n",
      "loss: 4307.509927  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 81.04892%, Avg loss: 0.735357 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.158294  [  196/744121]\n",
      "loss: 118.134988  [19796/744121]\n",
      "loss: 231.975142  [39396/744121]\n",
      "loss: 345.296175  [58996/744121]\n",
      "loss: 460.015120  [78596/744121]\n",
      "loss: 575.779742  [98196/744121]\n",
      "loss: 690.819526  [117796/744121]\n",
      "loss: 806.040629  [137396/744121]\n",
      "loss: 920.083471  [156996/744121]\n",
      "loss: 1037.825788  [176596/744121]\n",
      "loss: 1152.637346  [196196/744121]\n",
      "loss: 1267.806335  [215796/744121]\n",
      "loss: 1383.862718  [235396/744121]\n",
      "loss: 1496.990529  [254996/744121]\n",
      "loss: 1613.532999  [274596/744121]\n",
      "loss: 1728.633571  [294196/744121]\n",
      "loss: 1843.232860  [313796/744121]\n",
      "loss: 1956.039575  [333396/744121]\n",
      "loss: 2068.899519  [352996/744121]\n",
      "loss: 2180.775812  [372596/744121]\n",
      "loss: 2295.850017  [392196/744121]\n",
      "loss: 2409.240364  [411796/744121]\n",
      "loss: 2524.634912  [431396/744121]\n",
      "loss: 2641.500286  [450996/744121]\n",
      "loss: 2758.107990  [470596/744121]\n",
      "loss: 2871.915259  [490196/744121]\n",
      "loss: 2985.044499  [509796/744121]\n",
      "loss: 3101.410867  [529396/744121]\n",
      "loss: 3215.516729  [548996/744121]\n",
      "loss: 3328.844210  [568596/744121]\n",
      "loss: 3441.088915  [588196/744121]\n",
      "loss: 3555.894120  [607796/744121]\n",
      "loss: 3669.639674  [627396/744121]\n",
      "loss: 3782.089305  [646996/744121]\n",
      "loss: 3896.386104  [666596/744121]\n",
      "loss: 4009.888702  [686196/744121]\n",
      "loss: 4122.177462  [705796/744121]\n",
      "loss: 4235.703907  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 81.67210%, Avg loss: 0.716717 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.060697  [  196/744121]\n",
      "loss: 113.908764  [19796/744121]\n",
      "loss: 225.972215  [39396/744121]\n",
      "loss: 339.931657  [58996/744121]\n",
      "loss: 455.281941  [78596/744121]\n",
      "loss: 567.995351  [98196/744121]\n",
      "loss: 680.807562  [117796/744121]\n",
      "loss: 794.809549  [137396/744121]\n",
      "loss: 907.707495  [156996/744121]\n",
      "loss: 1023.830946  [176596/744121]\n",
      "loss: 1135.900839  [196196/744121]\n",
      "loss: 1249.501571  [215796/744121]\n",
      "loss: 1363.661669  [235396/744121]\n",
      "loss: 1476.581363  [254996/744121]\n",
      "loss: 1589.004552  [274596/744121]\n",
      "loss: 1699.807180  [294196/744121]\n",
      "loss: 1814.886867  [313796/744121]\n",
      "loss: 1925.070154  [333396/744121]\n",
      "loss: 2036.940971  [352996/744121]\n",
      "loss: 2149.632433  [372596/744121]\n",
      "loss: 2262.879938  [392196/744121]\n",
      "loss: 2374.965008  [411796/744121]\n",
      "loss: 2487.497257  [431396/744121]\n",
      "loss: 2600.436762  [450996/744121]\n",
      "loss: 2715.206751  [470596/744121]\n",
      "loss: 2827.832678  [490196/744121]\n",
      "loss: 2938.791638  [509796/744121]\n",
      "loss: 3052.019857  [529396/744121]\n",
      "loss: 3164.423978  [548996/744121]\n",
      "loss: 3274.897128  [568596/744121]\n",
      "loss: 3387.075731  [588196/744121]\n",
      "loss: 3499.114521  [607796/744121]\n",
      "loss: 3612.813608  [627396/744121]\n",
      "loss: 3723.296512  [646996/744121]\n",
      "loss: 3832.161591  [666596/744121]\n",
      "loss: 3942.702883  [686196/744121]\n",
      "loss: 4055.186450  [705796/744121]\n",
      "loss: 4168.336931  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 81.24221%, Avg loss: 0.726862 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.110060  [  196/744121]\n",
      "loss: 113.580599  [19796/744121]\n",
      "loss: 227.440816  [39396/744121]\n",
      "loss: 338.836008  [58996/744121]\n",
      "loss: 451.716110  [78596/744121]\n",
      "loss: 564.645607  [98196/744121]\n",
      "loss: 676.608852  [117796/744121]\n",
      "loss: 788.233227  [137396/744121]\n",
      "loss: 902.297964  [156996/744121]\n",
      "loss: 1017.735530  [176596/744121]\n",
      "loss: 1128.268927  [196196/744121]\n",
      "loss: 1238.626184  [215796/744121]\n",
      "loss: 1351.181480  [235396/744121]\n",
      "loss: 1461.400297  [254996/744121]\n",
      "loss: 1574.337900  [274596/744121]\n",
      "loss: 1683.340600  [294196/744121]\n",
      "loss: 1795.733022  [313796/744121]\n",
      "loss: 1905.769400  [333396/744121]\n",
      "loss: 2015.805149  [352996/744121]\n",
      "loss: 2126.450080  [372596/744121]\n",
      "loss: 2240.075592  [392196/744121]\n",
      "loss: 2351.961982  [411796/744121]\n",
      "loss: 2463.683750  [431396/744121]\n",
      "loss: 2573.433532  [450996/744121]\n",
      "loss: 2687.281677  [470596/744121]\n",
      "loss: 2798.057617  [490196/744121]\n",
      "loss: 2906.983845  [509796/744121]\n",
      "loss: 3020.297159  [529396/744121]\n",
      "loss: 3130.604189  [548996/744121]\n",
      "loss: 3242.564027  [568596/744121]\n",
      "loss: 3353.073951  [588196/744121]\n",
      "loss: 3465.024494  [607796/744121]\n",
      "loss: 3578.257472  [627396/744121]\n",
      "loss: 3689.087952  [646996/744121]\n",
      "loss: 3799.689553  [666596/744121]\n",
      "loss: 3911.148104  [686196/744121]\n",
      "loss: 4022.035043  [705796/744121]\n",
      "loss: 4132.856572  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 81.71443%, Avg loss: 0.719005 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.148708  [  196/744121]\n",
      "loss: 111.626445  [19796/744121]\n",
      "loss: 222.780747  [39396/744121]\n",
      "loss: 333.577523  [58996/744121]\n",
      "loss: 442.852105  [78596/744121]\n",
      "loss: 552.804045  [98196/744121]\n",
      "loss: 664.879142  [117796/744121]\n",
      "loss: 775.678773  [137396/744121]\n",
      "loss: 887.261818  [156996/744121]\n",
      "loss: 999.437400  [176596/744121]\n",
      "loss: 1110.217660  [196196/744121]\n",
      "loss: 1219.717727  [215796/744121]\n",
      "loss: 1331.557634  [235396/744121]\n",
      "loss: 1440.274991  [254996/744121]\n",
      "loss: 1551.707962  [274596/744121]\n",
      "loss: 1661.352822  [294196/744121]\n",
      "loss: 1772.765797  [313796/744121]\n",
      "loss: 1881.117123  [333396/744121]\n",
      "loss: 1991.131523  [352996/744121]\n",
      "loss: 2102.409887  [372596/744121]\n",
      "loss: 2214.182611  [392196/744121]\n",
      "loss: 2324.733328  [411796/744121]\n",
      "loss: 2435.859919  [431396/744121]\n",
      "loss: 2545.793154  [450996/744121]\n",
      "loss: 2658.676082  [470596/744121]\n",
      "loss: 2769.973129  [490196/744121]\n",
      "loss: 2879.211766  [509796/744121]\n",
      "loss: 2991.883035  [529396/744121]\n",
      "loss: 3103.338859  [548996/744121]\n",
      "loss: 3213.824927  [568596/744121]\n",
      "loss: 3322.603545  [588196/744121]\n",
      "loss: 3433.147414  [607796/744121]\n",
      "loss: 3543.338003  [627396/744121]\n",
      "loss: 3652.387633  [646996/744121]\n",
      "loss: 3761.222610  [666596/744121]\n",
      "loss: 3868.638046  [686196/744121]\n",
      "loss: 3976.837165  [705796/744121]\n",
      "loss: 4088.904393  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 81.46435%, Avg loss: 0.708454 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.158680  [  196/744121]\n",
      "loss: 111.052102  [19796/744121]\n",
      "loss: 223.062002  [39396/744121]\n",
      "loss: 331.262818  [58996/744121]\n",
      "loss: 441.378407  [78596/744121]\n",
      "loss: 551.078844  [98196/744121]\n",
      "loss: 660.704646  [117796/744121]\n",
      "loss: 769.153175  [137396/744121]\n",
      "loss: 880.323161  [156996/744121]\n",
      "loss: 991.688449  [176596/744121]\n",
      "loss: 1101.864765  [196196/744121]\n",
      "loss: 1211.329875  [215796/744121]\n",
      "loss: 1323.300860  [235396/744121]\n",
      "loss: 1432.008306  [254996/744121]\n",
      "loss: 1541.129757  [274596/744121]\n",
      "loss: 1647.359296  [294196/744121]\n",
      "loss: 1757.214846  [313796/744121]\n",
      "loss: 1865.952581  [333396/744121]\n",
      "loss: 1974.119364  [352996/744121]\n",
      "loss: 2085.552099  [372596/744121]\n",
      "loss: 2195.737288  [392196/744121]\n",
      "loss: 2304.192442  [411796/744121]\n",
      "loss: 2414.347288  [431396/744121]\n",
      "loss: 2523.541724  [450996/744121]\n",
      "loss: 2634.536177  [470596/744121]\n",
      "loss: 2743.214303  [490196/744121]\n",
      "loss: 2853.569018  [509796/744121]\n",
      "loss: 2964.473075  [529396/744121]\n",
      "loss: 3075.763724  [548996/744121]\n",
      "loss: 3185.521486  [568596/744121]\n",
      "loss: 3295.373062  [588196/744121]\n",
      "loss: 3406.228251  [607796/744121]\n",
      "loss: 3516.945375  [627396/744121]\n",
      "loss: 3624.649006  [646996/744121]\n",
      "loss: 3732.209754  [666596/744121]\n",
      "loss: 3842.174722  [686196/744121]\n",
      "loss: 3949.658753  [705796/744121]\n",
      "loss: 4062.119998  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 82.04603%, Avg loss: 0.683078 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.135218  [  196/744121]\n",
      "loss: 110.458828  [19796/744121]\n",
      "loss: 219.748701  [39396/744121]\n",
      "loss: 327.604836  [58996/744121]\n",
      "loss: 437.047554  [78596/744121]\n",
      "loss: 545.763580  [98196/744121]\n",
      "loss: 655.751448  [117796/744121]\n",
      "loss: 765.546512  [137396/744121]\n",
      "loss: 873.797377  [156996/744121]\n",
      "loss: 983.986447  [176596/744121]\n",
      "loss: 1090.476464  [196196/744121]\n",
      "loss: 1196.681332  [215796/744121]\n",
      "loss: 1307.197830  [235396/744121]\n",
      "loss: 1413.686431  [254996/744121]\n",
      "loss: 1521.612942  [274596/744121]\n",
      "loss: 1629.512565  [294196/744121]\n",
      "loss: 1739.084927  [313796/744121]\n",
      "loss: 1846.121489  [333396/744121]\n",
      "loss: 1954.071647  [352996/744121]\n",
      "loss: 2062.910571  [372596/744121]\n",
      "loss: 2171.135532  [392196/744121]\n",
      "loss: 2279.905218  [411796/744121]\n",
      "loss: 2388.232375  [431396/744121]\n",
      "loss: 2496.413881  [450996/744121]\n",
      "loss: 2606.533024  [470596/744121]\n",
      "loss: 2716.048911  [490196/744121]\n",
      "loss: 2825.048019  [509796/744121]\n",
      "loss: 2933.591174  [529396/744121]\n",
      "loss: 3042.338523  [548996/744121]\n",
      "loss: 3151.004149  [568596/744121]\n",
      "loss: 3259.168343  [588196/744121]\n",
      "loss: 3367.922642  [607796/744121]\n",
      "loss: 3478.139341  [627396/744121]\n",
      "loss: 3586.308569  [646996/744121]\n",
      "loss: 3694.005199  [666596/744121]\n",
      "loss: 3802.274775  [686196/744121]\n",
      "loss: 3910.592566  [705796/744121]\n",
      "loss: 4018.822228  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 81.98559%, Avg loss: 0.701021 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.041553  [  196/744121]\n",
      "loss: 109.265015  [19796/744121]\n",
      "loss: 218.142613  [39396/744121]\n",
      "loss: 323.860103  [58996/744121]\n",
      "loss: 433.402002  [78596/744121]\n",
      "loss: 542.612538  [98196/744121]\n",
      "loss: 649.844887  [117796/744121]\n",
      "loss: 758.056330  [137396/744121]\n",
      "loss: 868.378105  [156996/744121]\n",
      "loss: 977.986081  [176596/744121]\n",
      "loss: 1085.519846  [196196/744121]\n",
      "loss: 1193.272361  [215796/744121]\n",
      "loss: 1301.612196  [235396/744121]\n",
      "loss: 1408.233181  [254996/744121]\n",
      "loss: 1516.743003  [274596/744121]\n",
      "loss: 1623.283622  [294196/744121]\n",
      "loss: 1730.554671  [313796/744121]\n",
      "loss: 1837.084435  [333396/744121]\n",
      "loss: 1942.736446  [352996/744121]\n",
      "loss: 2049.825502  [372596/744121]\n",
      "loss: 2158.643668  [392196/744121]\n",
      "loss: 2266.823508  [411796/744121]\n",
      "loss: 2376.548160  [431396/744121]\n",
      "loss: 2483.464387  [450996/744121]\n",
      "loss: 2593.516508  [470596/744121]\n",
      "loss: 2701.672515  [490196/744121]\n",
      "loss: 2808.791072  [509796/744121]\n",
      "loss: 2917.361959  [529396/744121]\n",
      "loss: 3024.471081  [548996/744121]\n",
      "loss: 3133.097214  [568596/744121]\n",
      "loss: 3240.284804  [588196/744121]\n",
      "loss: 3346.857269  [607796/744121]\n",
      "loss: 3455.522383  [627396/744121]\n",
      "loss: 3563.212373  [646996/744121]\n",
      "loss: 3669.819763  [666596/744121]\n",
      "loss: 3777.444327  [686196/744121]\n",
      "loss: 3884.775525  [705796/744121]\n",
      "loss: 3993.132993  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 82.15860%, Avg loss: 0.695997 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.209514  [  196/744121]\n",
      "loss: 109.237694  [19796/744121]\n",
      "loss: 217.240165  [39396/744121]\n",
      "loss: 325.520946  [58996/744121]\n",
      "loss: 434.168635  [78596/744121]\n",
      "loss: 541.942104  [98196/744121]\n",
      "loss: 648.611334  [117796/744121]\n",
      "loss: 756.500455  [137396/744121]\n",
      "loss: 864.105234  [156996/744121]\n",
      "loss: 972.814681  [176596/744121]\n",
      "loss: 1079.774712  [196196/744121]\n",
      "loss: 1186.725193  [215796/744121]\n",
      "loss: 1295.139042  [235396/744121]\n",
      "loss: 1401.078148  [254996/744121]\n",
      "loss: 1507.710693  [274596/744121]\n",
      "loss: 1614.629920  [294196/744121]\n",
      "loss: 1722.699363  [313796/744121]\n",
      "loss: 1828.694435  [333396/744121]\n",
      "loss: 1934.348725  [352996/744121]\n",
      "loss: 2040.350938  [372596/744121]\n",
      "loss: 2148.396086  [392196/744121]\n",
      "loss: 2255.078737  [411796/744121]\n",
      "loss: 2363.728833  [431396/744121]\n",
      "loss: 2470.878258  [450996/744121]\n",
      "loss: 2579.151417  [470596/744121]\n",
      "loss: 2686.366446  [490196/744121]\n",
      "loss: 2792.304056  [509796/744121]\n",
      "loss: 2899.639369  [529396/744121]\n",
      "loss: 3005.501331  [548996/744121]\n",
      "loss: 3112.257462  [568596/744121]\n",
      "loss: 3218.894299  [588196/744121]\n",
      "loss: 3325.087909  [607796/744121]\n",
      "loss: 3431.595726  [627396/744121]\n",
      "loss: 3538.080087  [646996/744121]\n",
      "loss: 3642.039170  [666596/744121]\n",
      "loss: 3748.435106  [686196/744121]\n",
      "loss: 3856.031334  [705796/744121]\n",
      "loss: 3964.414951  [725396/744121]\n",
      "Test Error: \n",
      " Accuracy: 81.72460%, Avg loss: 0.693506 \n",
      "\n",
      "Early stopping at epoch: 19\n",
      "losses [3.1010403226230028, 2.1044945731927824, 1.7370339439578957, 1.5508358907096789, 1.4387287058766214, 1.361546418448326, 1.3067110923989873, 1.265408613178207, 1.2319327052216609, 1.2037041404387308, 1.1782780369394918, 1.1636693879965891, 1.1449211943862247, 1.1265159459028677, 1.1169798846680583, 1.105354658138887, 1.0970394251250517, 1.086040567937323, 1.0792761624000686, 1.0711976130102256]\n",
      "test_losses [2.0855033336898328, 1.5018127392199467, 1.287572073043125, 1.0400935094391686, 0.946398118350664, 0.9061001345551863, 0.8613541376312476, 0.8460176634700644, 0.7970881916271381, 0.7690063731273503, 0.7626648058506136, 0.7353571114800779, 0.7167166475508664, 0.7268624750563202, 0.7190053221524787, 0.7084541917579473, 0.6830776084479887, 0.7010212261641641, 0.6959967277486435, 0.6935060136660897]\n",
      "accs [48.401757881130706, 62.736127908663846, 68.81688522509837, 74.45910764389737, 76.0728517499357, 77.27097020035738, 78.67537746776996, 78.52378853646728, 80.30952522465067, 80.56907026024044, 81.22610408518563, 81.04891672041856, 81.67209938204428, 81.24220834961561, 81.71442833606534, 81.46434669518378, 82.04602511435394, 81.985586624837, 82.15859834679792, 81.72459831630181]\n",
      "81.72459831630181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(nnmodel.defaults)\n",
    "acc = nnmodel.run(nnmodel.defaults, train_ds, test_ds, 100, out=True, name=\"beer_local_cv_res\")\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e44464ac-92cb-40da-8251-14c6d5d72231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.690558 \n",
      "\n",
      "(0.6905580759048462, 0.8260817822388964)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      2528\n",
      "           1       0.68      0.88      0.77     10196\n",
      "           2       0.78      0.85      0.82     15084\n",
      "           3       0.91      0.68      0.78      3100\n",
      "           4       0.83      0.87      0.85      8778\n",
      "           5       0.76      0.81      0.79      3821\n",
      "           6       0.76      0.77      0.77      4123\n",
      "           7       0.98      0.71      0.82      8397\n",
      "           8       0.85      0.35      0.49       498\n",
      "           9       0.67      0.95      0.79     28284\n",
      "          10       0.94      0.90      0.92      1869\n",
      "          11       0.92      0.87      0.89     16721\n",
      "          12       0.86      0.93      0.89     38886\n",
      "          13       0.91      0.77      0.84      1325\n",
      "          14       0.86      0.85      0.85     20859\n",
      "          15       0.57      0.69      0.63      3022\n",
      "          16       0.78      0.84      0.81      7963\n",
      "          17       0.94      0.93      0.93     16743\n",
      "          18       0.90      0.83      0.87      8002\n",
      "          19       0.50      0.92      0.64     10498\n",
      "          20       0.57      0.74      0.65      5940\n",
      "          21       0.96      0.80      0.88      3790\n",
      "          22       0.92      0.57      0.70      2109\n",
      "          23       0.91      0.69      0.79      4038\n",
      "          24       0.75      0.68      0.71      6359\n",
      "          25       0.66      0.82      0.73     12398\n",
      "          26       0.94      0.49      0.64     10438\n",
      "          27       0.94      0.93      0.93      1116\n",
      "          28       0.73      0.85      0.79       337\n",
      "          29       0.70      0.77      0.73      2303\n",
      "          30       1.00      0.92      0.96       781\n",
      "          31       0.93      0.79      0.86      3799\n",
      "          32       0.69      0.21      0.32       339\n",
      "          33       0.98      0.75      0.85      1375\n",
      "          34       0.98      0.78      0.87       772\n",
      "          35       0.91      0.85      0.88      1699\n",
      "          36       0.69      0.78      0.73      4213\n",
      "          37       0.86      0.88      0.87      7251\n",
      "          38       0.76      0.79      0.78      1438\n",
      "          39       0.88      0.91      0.90      6542\n",
      "          40       0.98      0.86      0.91      2287\n",
      "          41       0.99      0.91      0.95       841\n",
      "          42       0.99      0.62      0.76      4512\n",
      "          43       0.87      0.65      0.74      2900\n",
      "          44       0.83      0.89      0.86      6549\n",
      "          45       0.71      0.57      0.64       743\n",
      "          46       0.97      0.62      0.76      5263\n",
      "          47       0.68      0.82      0.74      7736\n",
      "          48       0.95      0.16      0.28       253\n",
      "          49       0.91      0.76      0.83      3708\n",
      "          50       0.81      0.48      0.60       996\n",
      "          51       0.67      0.65      0.66      1580\n",
      "          52       0.81      0.57      0.67      1536\n",
      "          53       0.69      0.70      0.69      5995\n",
      "          54       0.55      0.78      0.65       903\n",
      "          55       0.87      0.78      0.82      5615\n",
      "          56       1.00      0.89      0.94       190\n",
      "          57       0.82      0.63      0.71      1669\n",
      "          58       0.91      0.83      0.87      2190\n",
      "          59       0.70      0.85      0.77      1944\n",
      "          60       0.77      0.75      0.76     11248\n",
      "          61       0.84      0.85      0.85      7226\n",
      "          62       1.00      0.94      0.97       220\n",
      "          63       0.99      0.91      0.95      1981\n",
      "          64       0.00      0.00      0.00        72\n",
      "          65       0.95      0.86      0.90      9338\n",
      "          66       0.83      0.44      0.57      3363\n",
      "          67       0.94      0.91      0.92      4217\n",
      "          68       0.91      0.85      0.87      2607\n",
      "          69       0.69      0.88      0.77       524\n",
      "          70       0.94      0.52      0.67       831\n",
      "          71       0.93      0.60      0.73       736\n",
      "          72       0.79      0.32      0.45        82\n",
      "          73       0.96      0.68      0.80      2845\n",
      "          74       0.94      0.88      0.91      3591\n",
      "          75       0.90      0.65      0.76       387\n",
      "          76       0.94      0.93      0.94      4752\n",
      "          77       0.87      0.87      0.87       365\n",
      "          78       0.98      0.72      0.83      3524\n",
      "          79       0.91      0.87      0.89      4298\n",
      "          80       0.77      0.83      0.80      2651\n",
      "          81       0.90      0.68      0.78      2630\n",
      "          82       0.82      0.90      0.86      7781\n",
      "          83       0.98      0.88      0.93      5993\n",
      "          84       0.84      0.85      0.85      4870\n",
      "          85       0.97      0.94      0.95      5053\n",
      "          86       0.97      0.85      0.91      5993\n",
      "          87       0.97      0.74      0.84      1327\n",
      "          88       0.00      0.00      0.00       130\n",
      "          89       0.94      0.89      0.91     17745\n",
      "          90       0.82      0.76      0.79      3338\n",
      "          91       0.96      0.89      0.92       343\n",
      "          92       0.95      0.77      0.85     10300\n",
      "          93       0.92      0.85      0.89      3246\n",
      "          94       0.93      0.87      0.90      5792\n",
      "          95       0.89      0.74      0.81      3024\n",
      "          96       0.90      0.78      0.83       916\n",
      "          97       0.93      0.51      0.66       947\n",
      "          98       0.94      0.86      0.90      9904\n",
      "          99       0.94      0.82      0.87      2965\n",
      "         100       0.98      0.86      0.92      3081\n",
      "         101       0.97      0.52      0.68      1271\n",
      "         102       0.94      0.74      0.83      6874\n",
      "         103       0.97      0.84      0.90     10058\n",
      "\n",
      "    accuracy                           0.83    523583\n",
      "   macro avg       0.85      0.75      0.78    523583\n",
      "weighted avg       0.85      0.83      0.83    523583\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czeh/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc = test(valid_dataloader, nnmodel.model, nnmodel.loss_fn)\n",
    "print(acc)\n",
    "validate(valid_dataloader, nnmodel.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a80e73",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8662d3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest took 6.2 minutes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "rf = RandomForestClassifier(n_estimators=20, max_features=100, random_state=42)  \n",
    "\n",
    "rf.fit(X_train_bag, y_train['class'])\n",
    "end = time.time()\n",
    "print(f\"Random Forest took {round((end - start)/60, 1)} minutes.\")\n",
    "y_prediction = rf.predict(X_valid_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c8b19014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9619831048754448\n",
      "F1-Score: 0.9619926208541042\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94      2528\n",
      "           1       0.90      0.95      0.93     10196\n",
      "           2       0.95      0.97      0.96     15084\n",
      "           3       0.95      0.93      0.94      3100\n",
      "           4       0.98      0.98      0.98      8778\n",
      "           5       0.96      0.96      0.96      3821\n",
      "           6       0.91      0.93      0.92      4123\n",
      "           7       0.97      0.97      0.97      8397\n",
      "           8       0.99      0.94      0.96       498\n",
      "           9       0.98      0.99      0.98     28284\n",
      "          10       0.99      0.98      0.98      1869\n",
      "          11       0.98      0.98      0.98     16721\n",
      "          12       0.98      0.99      0.98     38886\n",
      "          13       0.99      0.97      0.98      1325\n",
      "          14       0.96      0.97      0.96     20859\n",
      "          15       0.91      0.92      0.91      3022\n",
      "          16       0.96      0.96      0.96      7963\n",
      "          17       0.98      0.99      0.98     16743\n",
      "          18       0.96      0.97      0.96      8002\n",
      "          19       0.98      0.98      0.98     10498\n",
      "          20       0.95      0.96      0.96      5940\n",
      "          21       0.99      0.97      0.98      3790\n",
      "          22       0.94      0.90      0.92      2109\n",
      "          23       0.93      0.95      0.94      4038\n",
      "          24       0.93      0.92      0.93      6359\n",
      "          25       0.96      0.95      0.96     12398\n",
      "          26       0.94      0.95      0.95     10438\n",
      "          27       0.98      0.98      0.98      1116\n",
      "          28       0.97      0.97      0.97       337\n",
      "          29       0.94      0.95      0.95      2303\n",
      "          30       1.00      0.99      0.99       781\n",
      "          31       0.96      0.96      0.96      3799\n",
      "          32       0.95      0.87      0.91       339\n",
      "          33       0.95      0.91      0.93      1375\n",
      "          34       0.96      0.92      0.94       772\n",
      "          35       0.97      0.97      0.97      1699\n",
      "          36       0.92      0.89      0.91      4213\n",
      "          37       0.98      0.98      0.98      7251\n",
      "          38       0.94      0.92      0.93      1438\n",
      "          39       0.99      0.99      0.99      6542\n",
      "          40       0.97      0.95      0.96      2287\n",
      "          41       1.00      0.99      0.99       841\n",
      "          42       0.98      0.98      0.98      4512\n",
      "          43       0.89      0.91      0.90      2900\n",
      "          44       0.97      0.97      0.97      6549\n",
      "          45       0.93      0.87      0.90       743\n",
      "          46       0.98      0.95      0.96      5263\n",
      "          47       0.95      0.94      0.94      7736\n",
      "          48       0.92      0.77      0.84       253\n",
      "          49       0.97      0.94      0.96      3708\n",
      "          50       0.90      0.85      0.87       996\n",
      "          51       0.92      0.93      0.92      1580\n",
      "          52       0.90      0.88      0.89      1536\n",
      "          53       0.85      0.89      0.87      5995\n",
      "          54       0.87      0.90      0.89       903\n",
      "          55       0.96      0.96      0.96      5615\n",
      "          56       0.99      0.97      0.98       190\n",
      "          57       0.95      0.95      0.95      1669\n",
      "          58       0.97      0.96      0.97      2190\n",
      "          59       0.95      0.95      0.95      1944\n",
      "          60       0.95      0.94      0.94     11248\n",
      "          61       0.93      0.96      0.94      7226\n",
      "          62       0.99      0.98      0.98       220\n",
      "          63       0.98      0.97      0.97      1981\n",
      "          64       0.71      0.68      0.70        72\n",
      "          65       0.96      0.96      0.96      9338\n",
      "          66       0.93      0.91      0.92      3363\n",
      "          67       0.98      0.97      0.98      4217\n",
      "          68       0.98      0.95      0.96      2607\n",
      "          69       0.96      0.95      0.95       524\n",
      "          70       0.78      0.77      0.77       831\n",
      "          71       0.95      0.89      0.92       736\n",
      "          72       0.83      0.89      0.86        82\n",
      "          73       0.92      0.91      0.91      2845\n",
      "          74       0.97      0.97      0.97      3591\n",
      "          75       0.90      0.91      0.91       387\n",
      "          76       0.98      0.98      0.98      4752\n",
      "          77       0.96      0.95      0.95       365\n",
      "          78       0.96      0.95      0.96      3524\n",
      "          79       0.98      0.98      0.98      4298\n",
      "          80       0.93      0.92      0.92      2651\n",
      "          81       0.92      0.91      0.91      2630\n",
      "          82       0.96      0.96      0.96      7781\n",
      "          83       0.98      0.98      0.98      5993\n",
      "          84       0.98      0.97      0.98      4870\n",
      "          85       0.99      0.99      0.99      5053\n",
      "          86       0.98      0.98      0.98      5993\n",
      "          87       0.96      0.94      0.95      1327\n",
      "          88       0.60      0.67      0.64       130\n",
      "          89       0.98      0.98      0.98     17745\n",
      "          90       0.97      0.96      0.97      3338\n",
      "          91       0.99      0.97      0.98       343\n",
      "          92       0.96      0.97      0.96     10300\n",
      "          93       0.96      0.95      0.95      3246\n",
      "          94       0.98      0.98      0.98      5792\n",
      "          95       0.98      0.95      0.96      3024\n",
      "          96       0.97      0.92      0.94       916\n",
      "          97       0.90      0.87      0.88       947\n",
      "          98       0.98      0.98      0.98      9904\n",
      "          99       0.92      0.92      0.92      2965\n",
      "         100       0.99      0.98      0.98      3081\n",
      "         101       0.98      0.96      0.97      1271\n",
      "         102       0.98      0.97      0.97      6874\n",
      "         103       0.97      0.96      0.97     10058\n",
      "\n",
      "    accuracy                           0.96    523583\n",
      "   macro avg       0.95      0.94      0.94    523583\n",
      "weighted avg       0.96      0.96      0.96    523583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_valid, y_prediction)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "f1 = f1_score(y_valid, y_prediction, average='weighted')\n",
    "print(f'F1-Score: {f1}')\n",
    "\n",
    "print(classification_report(y_valid, y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb26501b-2fde-4fee-9268-ee0de06cd15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(rf, X_train_bag, y_train, scoring=('accuracy', 'f1_weighted'), return_train_score=False)\n",
    "\n",
    "accuracy = np.mean(cv_results['accuracy'])\n",
    "f1 = np.mean(cv_results['f1_weighted'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
